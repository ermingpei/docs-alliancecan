<languages />

=Major upgrade of our Advanced Research Computing infrastructure=

Our Advanced Research Computing infrastructure is undergoing major changes starting in winter 2024-2025, most new systems are now expected to be available to users during summer 2025. These changes will improve High Performance Computing (HPC) and Cloud services for Canadian researchers. This page will be regularly updated to keep you informed of the activities concerning the transition to the new equipment.

The infrastructure renewal will replace the nearly 80% of our current equipment that is approaching end-of-life. The new equipment will offer faster processing speeds, greater storage capacity, and improved reliability.

=New system details=

{| class="wikitable"
|-
| '''New System''' || '''Old System to be Replaced''' || '''Documentation'''
|-
| [[Arbutus]] || [[Cloud]] (as a virtual infrastructure there is no change to the cloud interface.) || [[Arbutus|see this page]]
|-
| [[Rorqual/en|Rorqual]] || [[Beluga/en|Béluga]] || [[Rorqual/en|see this page]]
|-
| [[Fir]] || |[[Cedar]] || [[Fir|see this page]]
|-
| [[Trillium]] || [[Niagara]] & [[Mist]] || [[Trillium|see this page]]
|-
| [[Nibi]] || [[Graham]] || [[Nibi|see this page]]
|}

=System capacity, reductions and outages =
During the installation and the transition to the new systems, outages and reductions will be unavoidable due to constraints on space and electrical power. 
We recommend that you consider the possibility of outages when you plan research programs, graduate examinations, etc.

For a list of completed events, please see [[Infrastructure renewal completed events]].

{| class="wikitable"
|-
| '''Start Time''' || '''End Time''' || '''Status''' || '''System''' || '''Type''' || '''Description'''
|-
| June 6, 2025, 9:00 AM (EDT) || June 10, 2025, 12:00 PM (EDT) (4 days) || Upcoming || Béluga, Narval, Juno (non-HA) || Outage || Scheduled electrical maintenance will require the shutdown of '''Béluga and Narval compute nodes''' from 9:00 a.m. (EDT) on June 6 until 12:00 p.m. (noon) on June 10, 2025.  
Cloud instances in the '''Juno Cloud (non-High Availability zone)''' will also be shut down during this period.  

Jobs scheduled to finish after 9:00 a.m. on June 6 will remain queued until the clusters are back online.

Brief interruptions for network and storage maintenance:

* '''Cloud instances on Béluga Cloud and in the Juno Cloud HA zone''' may experience brief access interruptions due to network maintenance.  
* '''Storage systems on Béluga and Narval''' will remain accessible via Globus and the login nodes of each cluster, but may experience intermittent access disruptions due to network and storage work.
|-
| Jan 22, 2025 || Ongoing || In Progress || Cedar (70%) || Reduction || Starting January 22, Cedar cluster will operate at approximately 70% capacity until Fir is commissioned during summer of 2025.
|-  
| Feb 25, 2025 || Ongoing || In Progress || Graham (25%) || Reduction ||   
'''Mar 21, 2025 UPDATE:''' The Graham compute cluster is now running with '''reduced capacity'''.  
  
'''Graham Cloud remains operational during this period.'''  
    
|-
| Jan 6, 2025 || Ongoing || In Progress || Niagara (50%), Mist (35%) || Reduction || Niagara will operate at 50% capacity and Mist at 35% to support ongoing system improvements and the integration with the new system, Trillium, expected during summer 2025. 
<i>Mist required a temporary shutdown for a few hours on January 6.<i>
|}

=Resource Allocation Competition (RAC)=
The [https://www.alliancecan.ca/en/services/advanced-research-computing/accessing-resources/resource-allocation-competition Resource Allocation Competition]  will be impacted by this transition, but the application process remains the same. <br>
2024/25 allocations will remain in effect on retiring clusters while each cluster remains in service.  The 2025/26 allocations will be implemented everywhere once all new clusters are in service.<br>
Because the old clusters will mostly be out of service before all new ones are available, if you hold both a 2024 and a 2025 RAC award you will experience a period when neither award is available to you. You will be able to compute with your default allocation (<code>def-xxxxxx</code>) on each new cluster as soon as it goes into service, but the 2025 RAC allocations will only become available when all new clusters are in service.
=User training resources=
{| class="wikitable"
|-
| '''Course Title''' || '''Course Provider''' || '''Instructor''' || '''Date''' || '''Description''' || '''Audience''' || '''Format''' || '''Registration'''
|-
| Workflow Hacks for Large Datasets in HPC || Simon Fraser University (SFU) / West DRI || Alex Razoumov || Tuesday, May 20, 2025, 10:00 AM PT || Over the years, we have delivered webinars on tools that can significantly enhance research workflows involving large datasets. In this session, we will highlight some of these valuable tools:<br>• <b>In-situ visualization:</b> enables interactive rendering of large in-memory arrays without the need to store them to disk.<br>• <b>Lossy 3D data compression:</b> reduces the size of 3D datasets by up to 100X with no visible artifacts, making it ideal for storage and archival.<br>• <b>Distributed storage:</b> helps manage vast amounts of data across multiple locations.<br>• <b>DAR (Disk ARchiver):</b> a modern, high-performance alternative to TAR that offers indexing, differential archives, and faster extraction.|| Users working with large datasets || Webinar; <br>Recordings and materials from previous related webinars are freely available at [https://training.westdri.ca https://training.westdri.ca].  || Past
|-
| [https://training.sharcnet.ca/courses/enrol/index.php?id=210 Mastering GPU Efficiency] || SHARCNET || Sergey Mashchenko || Available Anytime || This online self-paced course provides basic training for [https://training.sharcnet.ca/courses/mod/glossary/showentry.php?eid=112&displayformat=dictionary Alliance] users on using GPUs on our [https://training.sharcnet.ca/courses/mod/glossary/showentry.php?eid=86&displayformat=dictionary national systems]. Modern GPUs (such as NVIDIA A100 and H100) are massively parallel and very expensive devices. Most of GPU jobs are incapable of utilizing these GPUs efficiently, either due to the problem size being too small to saturate the GPU, or due to the intermittent (bursty) GPU utilization pattern. This course will teach you how to measure the GPU utilization of your jobs on our clusters, and show how to use the two NVIDIA technologies - MPS (Multi-Process Service) and MIG (Multi-Instance GPU) - to improve GPU utilization. || Prospective users of the upgraded systems ||1-hour self-paced online course with a certificate of completion|| [https://training.sharcnet.ca/courses/enrol/index.php?id=210 Access the course here/Alliance CCDB account is required]
|-
|Introduction to the Fir cluster || Simon Fraser University (SFU) / West DRI || Alex Razoumov || September 2025 (rescheduled) || SFU’s newest cluster, Fir, is now expected to be available during summer 2025.  In this webinar, we will give an overview of the cluster and its hardware, walk through the filesystems and their recommended usage, talk about job submission policies and overall best practices for using the cluster. || Prospective users of [[Fir]] cluster || Webinar || Registration details to be updated closer to the new date
|-
| [https://youtu.be/pxY3G3BhwyA Survival guide for the upcoming GPU upgrades] || SHARCNET || Sergey Mashchenko || Wednesday, November 20, 2024, 12:00 PM to 1:00 PM ET || In the coming months, national systems will be undergoing significant upgrades. In particular, older GPUs (P100, V100) will be replaced with the newest H100 GPUs from NVIDIA. The total GPU computing power of the upgraded systems will grow by a factor of 3.5, but the number of GPUs will decrease significantly (from 3200 to 2100). This will present a significant challenge for users, as the usual practice of using a whole GPU for each process or MPI rank will no longer be feasible in most cases. Fortunately, NVIDIA provides two powerful technologies that can be used to mitigate this situation: MPS (Multi-Process Service) and MIG (Multi-Instance GPU). The presentation will walk the audience through both technologies and discuss the ways they can be used on the clusters. The discussion will include how to determine which approach will work best for specific code, and a live demonstration will be given at the end. ||Prospective users of the upgraded systems. Users intending to use a substantial amount of H100 resources (e.g., more than one GPU at a time, and/or over 24 hours runtime) || 1-hour [https://youtu.be/pxY3G3BhwyA presentation] and [https://helpwiki.sharcnet.ca/wiki/images/1/1d/MIG_MPS.pdf slides] || Past
|}

= Frequently asked questions = 

== Will my data be copied to its new system? ==
Data migration to the new systems is the responsibility of each National Host Site who will inform you of what you need to do.

== Will my files be deleted when a system is undergoing a complete data center shutdown as part of renewal activities? ==
No, your files will not be deleted. During renewal activities, each National Host Site will migrate /project and /home data from the existing storage system to the new storage system once it is installed. These migrations typically occur during outages, but specific details may vary by National Host Site. Each National Host Site will keep users informed of any specific, user-visible effects. 
Additionally, tape systems for backups and /nearline data are not being replaced, so backups and /nearline data will remain unchanged. 
For further technical questions, please email [[technical support]]. This goes directly to our ticketing system, where a support expert can provide a detailed response.

== When will outages occur? ==
Each National Host Site will have its own schedule for outages as the installation of and transition to new equipment proceeds. As usual, specific outages will be described on [https://status.alliancecan.ca our system status web page]. We will provide more general updates on this wiki page and you will periodically receive emails with updates and outage notices.

== Whom can I contact for questions about the transition? ==
Contact our [[technical support]]. They will try their best to answer any questions they can.

== Will my jobs and applications still be able to run on the new system? ==
Generally yes, but the new CPUs and GPUs may require recompilation or reconfiguration of some applications. More details will be provided as the transition unfolds.

== Will the software from the current systems still be available? ==
Yes, our [[Standard software environments|standard software environment]] will be available on the new systems.

== Will commercial, licensed software be migrated to the new systems? ==
Yes, the plan is that the current commercial software licenses will be transitioned from an old system to the new replacement so to the extent possible users should see identical access to those special applications (Gaussian, AMS/ADF, etc.). There is a small risk that the software providers will change their licensing terms for the new system. Such issues will be addressed individually as they come up.

== Will there be staggered outages? ==
We will do our best to limit overlapping outages, but  because we are very constrained by delivery schedules and funding deadlines, there will probably be periods when several of our systems are simultaneously offline. Outages will be announced as early as possible.

== Can I purchase old hardware after equipment upgrades? ==
Most of the equipment is legally the property of the hosting institution.  When the equipment is retired, the host institution manages its disposal following that institution's guidelines. This typically involves "e-cycling"--- recycling the equipment rather than selling it. If you're looking to acquire the old hardware, it's best to contact the host institution directly, as they may have specific policies or options for selling equipment.