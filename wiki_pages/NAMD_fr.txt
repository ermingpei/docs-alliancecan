<languages />
[[Category:Software]][[Category:BiomolecularSimulation]]


[http://www.ks.uiuc.edu/Research/namd/ NAMD] est un logiciel de dynamique moléculaire orienté-objet conçu pour la simulation de gros systèmes biomoléculaires.
Les simulations sont préparées et analysées dans le logiciel de visualisation [[VMD]].


= Installation =
Le logiciel est installé par notre équipe technique et est disponible via des modules. Si vous avez besoin d'une version plus récente, que vous devez faire vous-même l'installation ou que vous avez des questions, contactez le [[Technical support/fr|soutien technique]].

= Modules d'environnement =

La plus récente version 3.0.1 est installée sur toutes nos grappes. La version précédente 2.14 est aussi disponible.

Les versions 2.13 et 2.12 sont aussi disponibles.

Pour les tâches exécutées sur plusieurs nœuds, utilisez UCX.

= Scripts =

L'information sur l'ordonnanceur Slurm se trouve à la page [[Running jobs/fr|Exécuter des tâches]].

== Tâches séquentielles et tâches multifils == 
Le script suivant est pour une simulation séquentielle sur un seul cœur. Pour utiliser plus de cœurs, augmentez la valeur de <tt>--cpus-per-task</tt>, sans dépasser le nombre de cœurs disponible sur le nœud.
{{File
  |name=serial_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#
#SBATCH --cpus-per-task=1
#SBATCH --mem 2048            # memory in Mb, increase as needed    
#SBATCH -o slurm.%N.%j.out    # STDOUT file
#SBATCH -t 0:05:00            # time (D-HH:MM), increase as needed
#SBATCH --account=def-specifyaccount

# these are simple examples, please experiment with additional flags to improve your runtimes
# in particular, adding  +setcpuaffinity  flag may improve performance

# commands for NAMD version 3.0.1
module load StdEnv/2023  gcc/12.3 namd-multicore/3.0.1
namd3 +p$SLURM_CPUS_PER_TASK  +idlepoll apoa1.namd

# commands for NAMD version 2.14
module load StdEnv/2020 namd-multicore/2.14
namd2 +p$SLURM_CPUS_PER_TASK  +idlepoll apoa1.namd
}}

== Tâches parallèles sur CPU ==

=== Tâches MPI ===
'''NOTE : N'utilisez pas MPI mais plutôt UCX.'''  

=== Tâches verbs ===

NOTE:  Avec NAMD 2.14, utilisez UCX sur les autres grappes. Les directives ci-dessous s'appliquent uniquement aux versions NAMD 2.13 et 2.12.


Ces directives seront mises à jour une fois cette configuration testée sur les nouvelles grappes.
Dans cet exemple, nous utilisons 64 processus sur 2 nœuds, à raison de 32 processus par nœud; les 32 cœurs sont donc pleinement utilisés. Nous supposons que des nœuds entiers sont utilisés et <code>ntasks-per-node</code> devrait être 32 sur Graham. Pour une performance optimale, utilisez des nœuds entiers.

'''NOTES''':
*Les versions Verbs ne fonctionnent pas sur Cedar en raison de la réseautique qui est différente; utilisez plutôt la version MPI.
*Les versions verbs ne fonctionnent pas sur Béluga en raison de l'incompatibilité des pilotes du noyau InfiniBand; utilisez plutôt la version UCX.
{{File
  |name=verbs_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=32
#SBATCH --mem=0            # memory per node, 0 means all memory
#SBATCH -o slurm.%N.%j.out    # STDOUT
#SBATCH -t 0:05:00            # time (D-HH:MM)
#SBATCH --account=def-specifyaccount

NODEFILE=nodefile.dat
slurm_hl2hl.py --format CHARM > $NODEFILE
P=$SLURM_NTASKS

module load namd-verbs/2.12
CHARMRUN=`which charmrun`
NAMD2=`which namd2`
$CHARMRUN ++p $P ++nodelist $NODEFILE  $NAMD2  +idlepoll apoa1.namd
}}

===Tâches UCX===
L'exemple suivant utilise 80 processus à raison de 40 processus par nœud sur 2 nœuds et utilise ainsi pleinement les 80 cœurs. Le script  suppose que des nœuds entiers sont utilisés; ainsi, sur Béluga, <code>ntasks-per-node</code> devrait être 40. Les tâches NAMD qui utilisent des nœuds entiers offrent la meilleure performance.


'''NOTE :''' Les versions UCX devraient fonctionner sur toutes les grappes.
{{File
  |name=ucx_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=40
#SBATCH --mem=0            # memory per node, 0 means all memory
#SBATCH -o slurm.%N.%j.out    # STDOUT
#SBATCH -t 0:05:00            # time (D-HH:MM)
#SBATCH --account=def-specifyaccount

# these are simple examples, please experiment with additional flags to improve your runtimes
# in particular, adding  +setcpuaffinity  flag may improve performance

# commands for NAMD version 3.0.1
module load StdEnv/2023 gcc/13.3 namd-ucx/3.0.1
srun --mpi=pmi2 namd3 apoa1.namd

# commands for NAMD version 2.14
module load StdEnv/2020 namd-ucx/2.14
srun --mpi=pmi2 namd2 apoa1.namd
}}

=== Tâches OFI===

'''NOTE''': Les versions OFI fonctionnent '''UNIQUEMENT''' sur Cedar en raison de sa réseautique qui est différente. Des problèmes sont survenus avec OFI et il est préférable d'utiliser UCX.
{{File
  |name=ucx_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#SBATCH --account=def-specifyaccount
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=32
#SBATCH -t 0:05:00            # time (D-HH:MM)
#SBATCH --mem=0            # memory per node, 0 means all memory
#SBATCH -o slurm.%N.%j.out    # STDOUT

<!--T:55-->
module load StdEnv/2020 namd-ofi/2.14
srun --mpi=pmi2 namd2 stmv.namd 
}}

=== Tâches utilisant 1 GPU ===
Le prochain exemple utilise huit cœurs CPU et un GPU P100 sur un seul nœud.

'''Important''' : NAMD 3 offre un nouveau paramètre d’entrée, ce qui dirige plus de calculs vers le GPU. Cela peut améliorer considérablement la performance.

Pour l'utiliser, ajoutez la ligne suivante à votre fichier en entrée&nbsp;:

 GPUresident on;

{{File
  |name=multicore_gpu_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash

#SBATCH --cpus-per-task=8 
#SBATCH --mem=2048            
#SBATCH --time=0:15:00
#SBATCH --gpus-per-node=p100:1
#SBATCH --account=def-specifyaccount

# these are simple examples, please experiment with additional flags to improve your runtimes
# in particular, adding  +setcpuaffinity  flag may improve performance

# commands for NAMD version 3.0.1
module load StdEnv/2023  gcc/12.3  cuda/12.2 namd-multicore/3.0.1
namd3 +p$SLURM_CPUS_PER_TASK  +idlepoll stmv.namd

# commands for NAMD version 2.14
module load StdEnv/2020 cuda/11.0 namd-multicore/2.14
namd2 +p$SLURM_CPUS_PER_TASK  +idlepoll apoa1.namd
}}


== Tâches utilisant plusieurs GPU ==
=== Tâches UCX avec GPU ===
Cet exemple est pour Béluga et suppose que des nœuds entiers sont utilisés, ce qui offre une meilleure performance pour les tâches NAMD. L'exemple utilise 8 processus sur 2 nœuds, chaque processus utilisant 10 fils et 1 GPU. Ceci utilise pleinement les nœuds GPU de Béluga qui ont 40 cœurs et 4 GPU par nœud. Remarquez que 1 cœur par tâche est réservé à un fil de communication; il est donc normal que NAMD ne rapporte que 72 cœurs utilisés.  

Pour utiliser ce script sur une autre grappe, voyez les caractéristiques des nœuds disponibles sur la grappe et ajustez les options --cpus-per-task et --gpus-per-node en conséquence.

'''NOTE''' : Les versions UCX peuvent être utilisées sur toutes les grappes.
{{File
  |name=ucx_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash

#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=10 # number of threads per task (process)
#SBATCH --gpus-per-node=v100:4
#SBATCH --mem=0            # memory per node, 0 means all memory
#SBATCH --time=0:15:00
#SBATCH --account=def-specifyaccount

module load StdEnv/2020  intel/2020.1.217  cuda/11.0 namd-ucx-smp/2.14
NUM_PES=$(expr $SLURM_CPUS_PER_TASK - 1 )
srun --cpus-per-task=$SLURM_CPUS_PER_TASK --mpi=pmi2 namd2 ++ppn $NUM_PES apoa1.namd
}}

=== Tâches OFI avec GPU ===

'''NOTE''': Les versions OFI fonctionnent '''UNIQUEMENT''' sur Cedar en raison de sa réseautique qui est différente. Des problèmes sont survenus avec OFI et il est préférable d'utiliser UCX.   
{{File
  |name=ucx_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#SBATCH --account=def-specifyaccount
#SBATCH --ntasks 8            # number of tasks
#SBATCH --nodes=2
#SBATCH --cpus-per-task=6
#SBATCH --gpus-per-node=p100:4
#SBATCH -t 0:05:00            # time (D-HH:MM)
#SBATCH --mem=0            # memory per node, 0 means all memory

module load StdEnv/2020 cuda/11.0 namd-ofi-smp/2.14
NUM_PES=$(expr $SLURM_CPUS_PER_TASK - 1 )
srun --cpus-per-task=$SLURM_CPUS_PER_TASK --mpi=pmi2 namd2 ++ppn $NUM_PES stmv.namd
}}

=== Tâches verbs avec GPU ===

NOTE:  Avec NAMD 2.14, utilisez les GPU UCX sur toutes les grappes. Les directives ci-dessous s'appliquent à NAMD 2.13 et 2.12.

Cet exemple utilise 64 processus sur 2 nœuds, chaque nœud exécutant 32 processus, ce qui utilise pleinement les 32 cœurs. Chaque nœud utilise 2 GPU, donc la tâche utilise un total de 4 GPU. Le script  suppose que des nœuds entiers sont utilisés; sur Graham, <code>ntasks-per-node</code> serait 32. Les tâches NAMD qui utilisent des nœuds entiers offrent la meilleure performance.

'''NOTE''' : Les versions verbs ne fonctionnent pas sur Cedar à cause de différences de réseautique.  
{{File
  |name=verbsgpu_namd_job.sh
  |lang="sh"
  |contents=
#!/bin/bash
#
#SBATCH --ntasks 64            # number of tasks
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=32
#SBATCH --mem 0            # memory per node, 0 means all memory
#SBATCH --gpus-per-node=p100:2
#SBATCH -o slurm.%N.%j.out    # STDOUT
#SBATCH -t 0:05:00            # time (D-HH:MM)
#SBATCH --account=def-specifyaccount

slurm_hl2hl.py --format CHARM > nodefile.dat
NODEFILE=nodefile.dat
OMP_NUM_THREADS=32
P=$SLURM_NTASKS

module load cuda/8.0.44
module load namd-verbs-smp/2.12
CHARMRUN=`which charmrun`
NAMD2=`which namd2`
$CHARMRUN ++p $P ++ppn $OMP_NUM_THREADS ++nodelist $NODEFILE  $NAMD2  +idlepoll apoa1.namd
}}

==Performance et étalonnage <i>benchmarking</i>==

Le guide <i>[https://mdbench.ace-net.ca/mdbench/ Molecular Dynamics Performance Guide]</i> a été créé par une équipe [https://www.ace-net.ca/ d'ACENET]. Le guide décrit les conditions optimales pour exécuter aussi des tâches sur nos grappes avec AMBER, GROMACS et OpenMM.

Voici un exemple d'étalonnage. La performance de NAMD varie selon les systèmes simulés, en rapport particulièrement avec la quantité d'atomes. Il serait donc très utile d'effectuer le type d'étalonnage montré ici dans les cas où la simulation d'un système particulier serait de longue durée. Les données ainsi collectées peuvent aussi servir à documenter vos demandes aux concours d'allocation de ressources.

Pour obtenir des résultats pertinents, nous vous suggérons de varier le nombre d'étapes pour que la simulation du système se fasse sur quelques minutes et que la collecte des données de durée se fasse à des intervalles d'au moins quelques secondes. Vous pourriez remarquer des variations dans vos résultats de durée si le temps d'exécution est trop court.  

Les données ci-dessous proviennent d'un étalonnage standard apoa1 effectué sur la grappe Graham qui possède des nœuds CPU de 32 cœurs et des nœuds GPU de 32 cœurs et 2 GPU. Pour faire le même exercice avec une autre grappe, vous devrez tenir compte de la structure de ses nœuds.

Dans le premier tableau, nous utilisons NAMD 2.12 chargé du module verbs. L'efficacité est calculée avec (durée avec 1 cœur) / (N * (durée avec N cœurs) ).

{| class="wikitable sortable"
|-
! # cœurs !! Durée réelle par étape !! Efficacité
|-
| 1 ||  0.8313||100%
|-
| 2 ||  0.4151||100%
|-
| 4 ||  0.1945|| 107%
|-
| 8 ||   0.0987|| 105%
|-
| 16 ||   0.0501|| 104%
|-
| 32   ||    0.0257|| 101%
|-
| 64 ||  0.0133|| 98%
|-
| 128 || 0.0074|| 88%
|-
| 256 || 0.0036|| 90%
|-
| 512 || 0.0021|| 77%
|-
|}

Dans ce cas, nous constatons qu'il est acceptable d'utiliser 256 cœurs pour simuler le système. Si vous demandez plus de cœurs qu'il n'en faut, vos tâches seront plus longtemps en attente et votre résultat d'ensemble en serait affecté.

Dans le prochain cas, l'étalonnage est fait avec l'utilisation de GPU. Le module multicœur de NAMD est utilisé pour les simulations pouvant être effectuées avec un (1) nœud alors que le module verbs-smp sert dans les cas de tâches nécessitant plusieurs nœuds.

{| class="wikitable sortable"
|-
! # cœurs  !! #GPU !! Durée réelle par étape !! Notes
|-
| 4 || 1   ||  0.0165 || 1 nœud, multicœur
|-
| 8 || 1   || 0.0088 || 1 nœud, multicœur
|-
| 16 || 1   || 0.0071 || 1 nœud, multicœur
|-
| 32   || 2   ||  0.0045  || 1 nœud, multicœur
|-
| 64 || 4 || 0.0058  || 2 nœuds, verbs-smp
|-
| 128 || 8 ||  0.0051 || 2 nœuds, verbs-smp
|-
|}

Les données du tableau indiquent clairement qu'il est absolument inutile d'utiliser plus d'un nœud puisque la performance décroît avec 2 nœuds ou plus. Avec 1 nœud, il est préférable d'utiliser 1 GPU/16 cœurs puisque l'efficacité est maximale; l'utilisation de 2 GPU/32 cœurs est acceptable si vos résultats doivent être produits rapidement. Puisque les nœuds GPU de Graham l'ordre de priorité est le même pour toutes les tâches 1 GPU/16 cœurs, il n'y a aucun avantage à utiliser 4 ou 8 cœurs.

On peut se demander si la simulation peut ou non utiliser un GPU. Les résultats de l'étalonnage indiquent que l'utilisation d'un nœud GPU (2 GPU/32 cœurs) sur Graham traite la tâche plus rapidement que sur 4 nœuds qui ne sont pas des GPU. Puisque le coût d'un nœud GPU sur Graham est près de deux fois plus que celui d'un nœud qui n'est pas un GPU, il est plus économique d'utiliser les GPU. C'est ce que vous devriez faire tant que c'est possible, mais puisque les nœuds CPU sont en plus grand nombre, vous devriez considérer aussi ne pas utiliser de GPU si le temps d'attente est trop long.

= NAMD 3 =

NAMD 3 est disponible dans un module. Dans le cas de certaines configurations de système, la performance pourrait être meilleure comparée à celle de NAMD 2.14.

Pour l'essayer immédiatement, vous pouvez télécharger le binaire à partir du site web NAMD website et le modifier pour nos systèmes avec, par exemple (indiquez la version au besoin)

 tar xvfz NAMD_3.0alpha11_Linux-x86_64-multicore-CUDA-SingleNode.tar.gz 
 cd NAMD_3.0alpha11_Linux-x86_64-multicore-CUDA
 setrpaths.sh  --path .

Par la suite, l'exécutable namd3 dans ce répertoire sera lié aux librairies appropriées.  Vous pouvez alors soumettre une tâche qui utilise cet exécutable.

Pour obtenir une meilleure performance avec NAMD 3 sur un GPU, nous vous recommandons fortement d'ajouter au fichier de configuration le mot-clé suivant, pourvu que la configuration en entrée que vous utilisez le permette.

 GPUresident on;

Pour plus d'information sur ce paramètre et ses changements, voir [https://www.ks.uiuc.edu/Research/namd/alpha/3.0alpha/ cette page].

= Références =
* À télécharger : http://www.ks.uiuc.edu/Development/Download/download.cgi?PackageName=NAMD (on vous demandera de vous inscrire)
*[http://www.ks.uiuc.edu/Research/namd/2.14/ug/ NAMD User's guide for version 2.14]
*[https://www.ks.uiuc.edu/Research/namd/3.0.1/ug/ NAMD User's guide for version 3.0.1]
*[http://www.ks.uiuc.edu/Research/namd/3.0.1/notes.html NAMD version 3.0.1 release notes]
*[http://www.ks.uiuc.edu/Research/namd/2.14/notes.html NAMD version 2.14 release notes]
* Tutoriels : http://www.ks.uiuc.edu/Training/Tutorials/