<languages />
[[Category:AI and Machine Learning]]
Pour tirer le maximum de vos applications d'apprentissage machine, il faut connaître certains aspects particuliers de nos grappes. Ces machines sont beaucoup plus complexes que l'ordinateur local avec lequel vous faites du prototypage. Entre autres, une grappe possède des systèmes de fichiers distribués qui vont d'un type de stockage à un autre de façon transparente. Bien que l'accès à un fichier dans <code>/project</code> <b>peut donner l'impression de se faire de la même manière</b> que s'il était situé dans le nœud courant, sous le capot les effets sur la performance sont bien différents. Il est donc important de prendre connaissance de la section [[AI and Machine Learning/fr#Gérer_vos_ensembles_de_données|Gérer vos ensembles de données]] ci-dessous.

Cette page décrit les bonnes pratiques dans l'utilisation des grappes ainsi que des références à de l'information utile.

== Tutoriels ==

SHARCNET offre un tutoriel de formation autonome; cliquez sur [https://training.sharcnet.ca/courses/enrol/index.php?id=180 Introduction to Machine Learning].

Si votre programme est prêt à être exécuté sur nos grappes, voyez [[Tutoriel Apprentissage machine | notre tutoriel]].

Voyez aussi ce [https://prashp.gitlab.io/post/compute-canada-tut/ tutoriel préparé par un utilisateur], qui décrit les étapes pour configurer votre environnement et celui de l'Alliance avec Python.

== Python ==

[[Python/fr|Python ]] est un logiciel populaire en apprentissage machine. Prenez connaissance de [[Python/fr|notre page wiki]] pour des renseignements importants sur les versions, les environnements virtuels, les nœuds de connexion et de calcul, le <code>multiprocessing</code>, Anaconda, Jupyter, etc.

=== Éviter Anaconda ===

Nous vous recommandons d'utiliser <code>virtualenv</code> pour éviter les problèmes suivants causés par Anaconda et évoqués sur [[Anaconda|cette page]].

<b>Dans la plupart des cas, il est facile de passer à <code>virtualenv</code>. Vous n'avez qu'à installer les mêmes paquets, à l'exception de CUDA, CuDNN et d'autres bibliothèques de bas niveau qui sont déjà sur nos grappes.</b>

== Information sur les paquets logiciels disponibles ==

Pour des renseignements sur l'installation et les problèmes fréquents, voyez la page wiki pour chacun des paquets suivants&nbsp;:

* [[TensorFlow/fr|TensorFlow]]
* [[PyTorch/fr|PyTorch]]
* [[Keras/fr|Keras]]
* [[Torch/fr|Torch]]
* [[SpaCy/fr|SpaCy]]
* [[XGBoost/fr|XGBoost]]
* [[Large_Scale_Machine_Learning_(Big_Data)/fr#Scikit-Learn|Scikit-Learn]]
* [[Large_Scale_Machine_Learning_(Big_Data)/fr#Snap_ML|SnapML]]

== Gérer vos ensembles de données ==

=== Stockage et gestion de fichiers ===

Les besoins pour la recherche sont diversifiés; nous offrons donc plusieurs solutions qui vont du stockage local temporaire haute vitesse au stockage à long terme sur différents supports. Pour plus d'information, voyez [[Storage and file management/fr|Stockage et gestion de fichiers]].

===Choisir le type de stockage selon la taille de votre ensemble de données===

* Si votre ensemble de données est d'environ 10Go ou moins, il entre probablement dans la mémoire, dépendant de la quantité de mémoire de votre tâche. Vos tâches d'apprentissage machine ne devraient pas lire de données sur disque.
* Si votre ensemble de données est d'environ 100Go ou moins, il entre dans l'espace de stockage local du nœud de calcul; transférez-le dans cet espace au début de la tâche puisqu'il est beaucoup plus rapide et fiable que les espaces partagés que sont /home, /project et /scratch. Pour chaque tâche, un répertoire temporaire est disponible à $SLURM_TMPDIR; voyez l'exemple de [[Tutoriel Apprentissage machine|notre tutoriel]]. Il faut toutefois savoir qu'une tâche d'un autre utilisateur peut occuper pleinement l'espace de stockage du nœud et ne vous laisser aucune place (nous cherchons une solution à ce problème); par contre, si c'est votre jour de chance, vous pourriez avoir un téraoctet juste pour vous.
* Si votre ensemble de données est plus grand, vous pourriez devoir le laisser dans un espace partagé. Vous pouvez stocker des données de façon permanente dans votre espace /project; l'espace /scratch est parfois plus rapide, mais n'est pas conçu pour du stockage permanent. Tous les espaces de stockage partagés (/home, /project et /scratch) servent à lire et à stocker des données à faible fréquence (par exemple, 1 gros bloc par 10 secondes plutôt que 10 petits blocs par seconde).

=== Ensembles de données composés de plusieurs petits fichiers ===

En apprentissage machine, il est fréquent d'avoir des ensembles de données composés de centaines et même de milliers de fichiers, par exemple dans le cas des ensembles de données d'images. Chacun des fichiers peut être de petite taille, souvent en deçà de quelques centaines de kilo-octets et dans ces cas, certains problèmes peuvent survenir&nbsp;:

* le système de fichiers impose un [[Storage and file management/fr#Quotas_et_politiques|quota]] qui restreint le nombre de fichiers,
* l'application pourrait être considérablement ralentie par le transfert des fichiers de <code>/project</code> ou <code>/scratch</code> vers un nœud de calcul.

Avec un système de fichiers distribué, les données devraient être rassemblées dans un seul fichier d'archive; voyez [[Handling large collections of files/fr|Travailler avec un grand nombre de fichiers]].

== Calculs de longue durée ==

Si vos calculs exigent beaucoup de temps, il est recommandé d'utiliser des points de contrôle (<i>checkpoints</i>); par exemple, plutôt que trois jours d'entraînement, vous pourriez avoir trois blocs de 24 heures chacun. De cette manière, votre travail ne serait pas perdu en cas de panne et vous pourriez bénéficier d'une meilleure priorisation de vos tâches puisqu'il y a plus de nœuds qui sont réservés pour les tâches courtes.
Votre bibliothèque préférée supporte probablement les <i>checkpoints</i>; voyez le cas type présenté dans [[Tutoriel Apprentissage machine|notre tutoriel]]. Si votre programme ne le permet pas, consultez la [[Points de contrôle|solution générique]].

Voir les autres exemples dans 

[[PyTorch/fr#Créer_des_points_de_contrôle|Points de contrôle PyTorch]]

[[TensorFlow/fr#Créer_des_points_de_contrôle|Points de contrôle TensorFlow]]

== Exécution de plusieurs tâches similaires ==

Dans un des cas suivants :

* recherche d'hyperparamètres,
* entraînement de plusieurs variantes d'une même méthode,
* exécution de plusieurs processus d'optimisation de même durée,

vous devriez grouper plusieurs tâches pour n'en former qu'une avec un outil comme [[META-Farm/fr|META]], [[GLOST/fr|GLOST]] ou [[GNU Parallel/fr|GNU Parallel]].

== Suivi de l'expérimentation et optimisation des hyperparamètres ==

[[Weights & Biases (wandb)/fr|Weights & Biases (wandb)]]  et [[Comet.ml/fr|Comet.ml]] peuvent vous aider à optimiser votre allocation de calcul en

* facilitant le suivi et l'analyse des processus d'apprentissage,
* permettant une optimisation bayésienne d'hyperparamètres.

Comet et Wandb ne sont pas disponibles présentement sur Graham.

==Apprentissage machine à grande échelle (mégadonnées)==

Les paquets d'apprentissage profond modernes comme PyTorch et TensorFlow offrent des utilitaires pour les travaux natifs d’apprentissage à grande échelle et les tutoriels sont nombreux. Un sujet peu abordé par contre est la scalabilité des méthodes classiques d’apprentissage machine (et non d’apprentissage profond) pour le travail avec de grands ensembles de données; à ce sujet, voir la page wiki [[Large_Scale_Machine_Learning_(Big_Data)/fr|Apprentissage machine à grande échelle (mégadonnées)]].

== Dépannage ==

===Déterminisme dans les réseaux de neurones récurrents avec CUDA===

Quand la bibliothèque cuDNN est présente dans CUDA Toolkit versions 10.2 et plus, il est possible de voir un comportement non déterministe dans les réseaux de neurones récurrents (RNN) et les appels à l’API d’auto-attention multitêtes.
Pour éviter ce problème, vous pouvez configurer la variable d’environnement CUBLAS_WORKSPACE_CONFIG avec une seule taille pour la mémoire tampon, par exemple <code>:16:8</code> ou <code>:4096:2</code>. Ainsi, cuBLAS fixe la mémoire GPU à 8 tampons de 16Ko chacun ou à 2 tampons de 4Mo chacun.