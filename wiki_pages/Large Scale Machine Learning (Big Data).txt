<languages />
[[Category:AI and Machine Learning]]
<translate>

<!--T:1-->
In the field of Deep Learning, the widespread use of mini-batching strategies along with first-order iterative solvers makes most common training tasks naturally scalable to large quantities of data. Whether you are looking at training Deep Neural Networks on a few thousand examples, or hundreds of millions of them, the flow of your code will look pretty much the same: load a few examples from a target source (from disk, from memory, from a remote source...) and iterate through them, computing gradients and using them to update the parameters of the model as you go. Conversely, in many Traditional Machine Learning packages --notably <code>scikit-learn</code>-- scaling your code to train on very large datasets is often not trivial. Many algorithms that fit common models such as Generalized Linear Models (GLMs) and Support Vector Machines (SVMs) for example, may have default implementations that require the entire training set to be loaded in memory and often do not leverage any manner of thread or process parallelism. Some of these implementations also rely on memory-intensive solvers, which may require several times the size of your training set's worth of memory to work properly.

<!--T:2-->
This page covers options to scale out traditional machine learning methods to very large datasets. Whether your training workload is too massive to fit even in a large memory node, or just big enough to take a really long time to process serially, the sections that follow may provide some insights to help you train models on Big Data.

=Scikit-learn= <!--T:3-->

<!--T:4-->
[https://scikit-learn.org/stable/index.html Scikit-learn] is a Python module for machine learning that is built on top of SciPy and distributed under the 3-Clause BSD license. This popular package features an intuitive API that makes building fairly complex machine learning pipelines very straightforward. However, many of its implementations of common methods such as GLMs and SVMs assume that the entire training set can be loaded in memory, which might be a showstopper when dealing with massive datasets. Furthermore, some of these algorithms opt for memory-intensive solvers by default. In some cases, you can avoid these limitations using the ideas that follow.

==Stochastic gradient solvers== <!--T:5-->

<!--T:6-->
If your training set is small enough that it can be loaded entirely in memory, but you are experiencing Out-Of-Memory (OOM) errors during training, the culprit is likely a memory-intensive solver. Many common machine learning methods in <code>scikit-learn</code> have variations of [https://en.wikipedia.org/wiki/Stochastic_gradient_descent stochastic gradient descent (SGD)] available as an option and replacing the default solver by an SGD-based one is often a straightforward solution to OOM errors.

<!--T:7-->
The following example compares a [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html Ridge Regression] performed using the default solver with an SGD-based one. You can monitor memory usage by running the command <code>htop</code> on the terminal while the Python code runs.

<!--T:8-->
{{File
  |name=ridge-default.py
  |lang="python"
  |contents=
from sklearn.datasets import make_regression
from sklearn.linear_model import Ridge

<!--T:9-->
X,y = make_regression(n_samples=100000, n_features=10000, n_informative=50)

<!--T:10-->
model = Ridge()

<!--T:11-->
model.fit(X,y)
}}

<!--T:12-->
{{File
  |name=ridge-saga.py
  |lang="python"
  |contents=
from sklearn.datasets import make_regression
from sklearn.linear_model import Ridge

<!--T:13-->
X,y = make_regression(n_samples=100000, n_features=10000, n_informative=50)

<!--T:14-->
model = Ridge(solver='saga')

<!--T:15-->
model.fit(X,y)
}}

<!--T:16-->
Another option that reduces memory usage even more, is to use [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html SGDRegressor] instead of Ridge. This class implements many types of generalized linear models for regression, using a vanilla stochastic gradient descent as a solver. One caveat of using SGDRegressor is that it only works if the output is unidimensional (a scalar).

<!--T:17-->
{{File
  |name=ridge-sgd_regressor.py
  |lang="python"
  |contents=
from sklearn.datasets import make_regression
from sklearn.linear_model import SGDRegressor

<!--T:18-->
X,y = make_regression(n_samples=100000, n_features=10000, n_informative=50)

<!--T:19-->
model = SGDRegressor(penalty='l2') # set penalty='l2' to perform a ridge regression

<!--T:20-->
model.fit(X,y)
}}

==Batch learning== <!--T:21-->

<!--T:22-->
In cases where your dataset is too large to fit in memory --or just large enough that it does not leave enough memory free for training-- it is possible to leave your data on disk and load it in batches during training, similar to how deep learning packages work. Scikit-learn refers to this as [https://scikit-learn.org/stable/computing/scaling_strategies.html <i>out-of-core learning</i>] and it is a viable option whenever an estimator has the <code>partial_fit</code> [https://scikit-learn.org/stable/computing/scaling_strategies.html?highlight=partial_fit#incremental-learning  method available]. In the examples below, we perform out-of-core learning by iterating over datasets stored on disk.

<!--T:23-->
In this first example, we use [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html SGDClassifier] to fit a linear SVM classifier with batches of data coming from a pair of <b>numpy</b> arrays. These arrays are stored on disk as [https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html#npy-format npy files] and we will keep them there by [https://numpy.org/doc/stable/reference/generated/numpy.memmap.html memory-mapping] these files. Since <code>SGDClassifier</code> has the <code>partial_fit</code> method, we can iterate through our large memory-mapped files loading only a small batch of rows from the arrays in memory at a time. Each call to <code>partial_fit</code> will then run one epoch of stochastic gradient descent over a batch of data.

<!--T:24-->
{{File
  |name=svm-sgd-npy.py
  |lang="python"
  |contents=
import numpy as np
from sklearn.linear_model import SGDClassifier

<!--T:25-->
def batch_loader(X, y, batch_size):
    return ((X[idx:idx + batch_size],y[idx:idx + batch_size]) for idx in range(0, len(X), batch_size)) # function returns a Generator

<!--T:26-->
inputs = np.memmap('./x_array.npy',dtype='float64',shape=(100000,10000))
targets = np.memmap('./y_array.npy',dtype='int8',shape=(100000,))

<!--T:27-->
model = SGDClassifier(loss='hinge') # Using loss='hinge' is equivalent to fitting a Linear SVM

<!--T:28-->
for batch in batch_loader(inputs, targets, batch_size=512):
    X,y = batch
    model.partial_fit(X,y)

<!--T:29-->
}}

<!--T:30-->
Another common method of storing data for Machine Learning is using CSV files. In this example, we train a LASSO regression model reading data in batches from a CSV file using the [https://pandas.pydata.org pandas] package.

<!--T:31-->
{{File
  |name=lasso-sgd-csv.py
  |lang="python"
  |contents=
import pandas as pd
from sklearn.linear_model import SGDRegressor

<!--T:32-->
model = SGDRegressor(penalty='l1')

<!--T:33-->
for batch in pd.read_csv("./data.csv", chunksize=512, iterator=True):
    X = batch.drop('target', axis=1)
    y = batch['target']
    model.partial_fit(X,y)
}}

=Snap ML= <!--T:34-->
[https://snapml.readthedocs.io/en/latest/ Snap ML] is a closed-source machine learning library developed by IBM that currently supports a number of classical machine learning models and scales gracefully to datasets with billions of examples and/or features. It offers distributed training, GPU acceleration and supports sparse data structures. It features an API very similar to scikit-learn and can be used as a replacement for that library when dealing with massive datasets.

== Installation == <!--T:69--> 

===Latest available wheels=== <!--T:70-->
To see the latest version of Snap ML that we have built:
{{Command|avail_wheels "snapml"}}
For more information, see [[Python#Available_wheels |Available wheels]].

===Installing the wheel=== <!--T:71-->

<!--T:72-->
The preferred option is to install it using the Python [https://pythonwheels.com/ wheel] as follows: 
:1. Load a Python [[Utiliser_des_modules/en#Sub-command_load|module]], thus <code>module load python</code>
:2. Create and start a [[Python#Creating_and_using_a_virtual_environment|virtual environment]].
:3. Install SnapML in the virtual environment with <code>pip install</code>. 

<!--T:73-->
:{{Command|prompt=(venv) [name@server ~]|pip install --no-index snapml }}

==Multithreading== <!--T:35-->

<!--T:36-->
All estimators in Snap ML support thread parallelism, which can be controlled via the <code>n_jobs</code> parameter. Setting this parameter to the number of cores available in your job will typically deliver a good speedup relative to the scikit-learn implementation of the same estimator. The following is a performance comparison of <code>Ridge</code> between scikit-learn and Snap ML.

<!--T:37-->
{{File
  |name=ridge-snap-vs-sklearn.py
  |lang="python"
  |contents=
from sklearn.datasets import make_regression
from sklearn.linear_model import Ridge
from snapml import LinearRegression
import time

<!--T:38-->
X,y = make_regression(n_samples=100000, n_features=10000, n_informative=50)

<!--T:39-->
model_sk = Ridge(solver='saga')

<!--T:40-->
print("Running Ridge with sklearn...")
tik = time.perf_counter()
model_sk.fit(X,y)
tok = time.perf_counter()

<!--T:41-->
print(f"sklearn took {tok-tik:0.2f} seconds to fit.")

<!--T:42-->
model_snap = LinearRegression(penalty='l2',n_jobs=4)

<!--T:43-->
print("Running Ridge with SnapML...")
tik = time.perf_counter()
model_snap.fit(X,y)
tok = time.perf_counter()

<!--T:44-->
print(f"SnapML took {tok-tik:0.2f} seconds to fit.")
}}

==Training on GPU== <!--T:45-->

<!--T:46-->
All estimators in <code>Snap ML</code> support GPU acceleration, with one or multiple GPUs. For single GPU training, simply set the parameter <code>use_gpu=True</code>. For multiple GPU training, in addition to setting <code>use_gpu</code>, pass a list containing the GPU IDs available to your job to <code>device_ids</code>. For example, inside a job that requested 2 GPUs, set <code>device_ids=[0,1]</code> to use both GPUs for training. The following example extends the performance comparison from the previous section to include training on GPU with Snap ML, this time training an SVM classifier with a non-linear kernel.

<!--T:47-->
{{File
  |name=ridge-snap-vs-sklearn2.py
  |lang="python"
  |contents=
from sklearn.datasets import make_classification
from sklearn.svm import SVC 
from snapml import SupportVectorMachine
import time

<!--T:48-->
X,y = make_classification(n_samples=100000, n_features=10000, n_classes=3, n_informative=50)

<!--T:49-->
model_sk = SVC(kernel='rbf') #sklearn's SVM fit-time scales at least quadratically with the number of samples... this will take a loooong time.

<!--T:50-->
print("Running SVM Classifier with sklearn...")
tik = time.perf_counter()
model_sk.fit(X,y)
tok = time.perf_counter()

<!--T:51-->
print(f"sklearn took {tok-tik:0.2f} seconds to fit.")

<!--T:52-->
model_snap = SupportVectorMachine(kernel='rbf',n_jobs=4)

<!--T:53-->
print("Running SVM Classifier with SnapML without GPU...")
tik = time.perf_counter()
model_snap.fit(X,y)
tok = time.perf_counter()

<!--T:54-->
print(f"SnapML took {tok-tik:0.2f} seconds to fit without GPU.")

<!--T:55-->
model_snap_gpu = SupportVectorMachine(kernel='rbf',n_jobs=4, use_gpu=True)

<!--T:56-->
print("Running SVM Classifier with SnapML with GPU...")
tik = time.perf_counter()
model_snap_gpu.fit(X,y)
tok = time.perf_counter()

<!--T:57-->
print(f"SnapML took {tok-tik:0.2f} seconds to fit with GPU.")
}}

==Out-of-memory training== <!--T:58-->

<!--T:59-->
All estimators in Snap ML use first-order iterative solvers, similar to SGD, by default. It is thus possible to perform training in batches and avoid loading entire datasets in memory. Unlike scikit-learn however, Snap ML accepts memory-mapped numpy arrays as inputs directly.

<!--T:60-->
{{File
  |name=snap-npy.py
  |lang="python"
  |contents=
import numpy as np
from snapml import LogisticRegression

<!--T:61-->
X = np.memmap('./x_array.npy',dtype='float64',shape=(100000,10000))
y = np.memmap('./y_array.npy',dtype='int8',shape=(100000,))

<!--T:62-->
model = LogisticRegression(n_jobs=4)

<!--T:63-->
model.fit(X,y)

<!--T:64-->
}}

==MPI== <!--T:65-->

<!--T:66-->
Snap ML features distributed implementations of many estimators. To run in distributed mode, call your Python script using <code>mpirun</code> or <code>srun</code>.

=Spark ML= <!--T:67-->

<!--T:68-->
[https://spark.apache.org/docs/latest/ml-guide.html Spark ML] is a machine learning library built on top of [[Apache_Spark/en|Apache Spark]]. It enables users to scale out many machine learning methods to massive amounts of data, over multiple nodes, without worrying about distributing datasets or explicitly writing distributed/parallel code. The library also includes many useful tools for distributed linear algebra and statistics. Please see our tutorial on [[Apache_Spark/en#Usage|submitting Spark jobs]] before trying out the examples on the official [https://spark.apache.org/docs/latest/ml-guide.html Spark ML documentation].

</translate>