# Niagara

This page is a translated version of the page [Niagara](https://docs.alliancecan.ca/mediawiki/index.php?title=Niagara&oldid=170621) and the translation is 100% complete.

Other languages: [English](https://docs.alliancecan.ca/mediawiki/index.php?title=Niagara&oldid=170621), fran√ßais

Available since April 2018

**Connection Node:** niagara.alliancecan.ca

**Globus Endpoint:** computecanada.ca#niagara

**Copy Nodes (rsync, scp, ...):** nia-dm2, nia-dm2, see [Moving Data](link-to-moving-data-page-if-available)

**System Status:** https://docs.scinet.utoronto.ca

**Portal:** https://my.scinet.utoronto.ca

Niagara is a homogeneous cluster, owned by the University of Toronto and operated by SciNet.  Capable of accommodating parallel tasks of 1040 cores and more, it is designed to efficiently manage the intensive throughput generated by a variety of data-intensive scientific applications. Its network and storage functions offer excellent performance and high capacity. Niagara also demonstrates appreciable energy efficiency.

In general, the environment is similar to that of Cedar or Graham, with a few minor differences. See the specifics in [Niagara: Getting Started Guide](#niagara-getting-started-guide).

GPUs are available on the Mist cluster; see the preliminary documentation on the [SciNet website](link-to-scinet-gpu-docs).

This cluster is part of the resources allocated as part of the [Resource Allocation Competition](link-to-resource-allocation-competition-page).


**Video:** [Presentation of Niagara at the SciNet User Group Meeting of February 14, 2018](link-to-video)

**Video:** [Hardware Installation](link-to-video)


## Contents

1. [Technical Specifications](#technical-specifications)
2. [Storage](#storage)
3. [High-Performance Networking](#high-performance-networking)
4. [Node Characteristics](#node-characteristics)
5. [Scheduling](#scheduling)
6. [Software](#software)
7. [Access](#access)
    7.1 [Getting Started Guide](#getting-started-guide)


## Technical Specifications

* 2024 nodes with either 40 Intel Skylake 2.4GHz cores or 40 Intel Cascadelake 2.5GHz cores, for a total of 80,640 cores
* 202GB (188 GiB) of RAM per node
* InfiniBand EDR (Enhanced Data Rate) network with Dragonfly+ topology
* 12.5PB /scratch space, 3.5PB project space (IBM Spectrum Scale parallel file system, formerly known as GPFS)
* 256TB burst buffer, Excelero + IBM Spectrum Scale
* No local disk
* No GPUs
* Theoretical peak performance (Rpeak) of 6.25 Pflops
* Measured maximum performance (Rmax) of 3.6 Pflops
* Power consumption 920kW


## Storage

**/home space**

* Total volume: 200TB
* Parallel file system (IBM Spectrum Scale)
* Backed up on tapes
* Persistent

**/scratch space**

* 12.5PB (~100GB/s write; ~120GB/s read)
* Parallel file system (IBM Spectrum Scale)
* Inactive data is purged

**Burst Buffer**

* 232TB (~90GB/s write; ~154GB/s read)
* Extra high-performance parallel file system (Excelero + IBM Spectrum Scale)
* Inactive data is purged

**/project space**

* 3.5PB (~100GB/s write; ~120GB/s read)
* High-performance parallel file system (IBM Spectrum Scale)
* Backed up on tapes
* Allocated via the [Resource Allocation Competition](link-to-resource-allocation-competition-page)
* Persistent

**/archive space**

* Total volume: 20PB
* High-performance storage (IBM HPSS)
* HSM backed up on tapes
* Allocated via the [Resource Allocation Competition](link-to-resource-allocation-competition-page)
* Persistent


## High-Performance Networking

The Niagara cluster offers an InfiniBand EDR (Enhanced Data Rate) network with a five-wing Dragonfly+ topology. Each of the wings, with a maximum of 432 nodes each (for 17280 cores), has 1-1 connectors. Communication between wings is done via adaptive routing, thus avoiding congestion and offering a 2:1 blocking ratio between nodes in different wings.


## Node Characteristics

* CPU: 2 sockets with 20 Intel Skylake cores (2.4GHz, AVX512), for a total of 40 cores per node
* Computing power: 3.07 TFlops (theoretical peak)
* Network connection: 100GB/s EDR Dragonfly+
* RAM: 202GB (188 GiB), slightly more than 4GiB per core
* Local disk: none
* GPUs/accelerators: none
* Operating system: Linux, CentOS 7


## Scheduling

Task scheduling is done with Slurm, and the basic commands are the same as with Cedar and Graham. However, the following differences should be noted:

* Scheduling is done only by node; tasks must always request multiples of 40 cores per task.
* It is not necessary and not recommended to request a specific amount of memory; the amount of memory is equal for each node (202GB/188GiB minus the memory used by the operating system).
* Usage details will be communicated as soon as available.


## Software

Software is installed from modules.

Usual software on Alliance clusters as well as Niagara-specific software is available.

Unlike Cedar and Graham clusters, no module is loaded by default on Niagara, this to avoid version conflicts. To load the Cedar or Graham software stack, use the CCEnv module as discussed in [Niagara Quickstart](#niagara-getting-started-guide).


## Access

Access to Niagara is not automatically available to all account holders with the Alliance, but if you have an account, access can be activated.

If you have an active account but do not yet have access to Niagara (for example, because you are a new user and belong to a group whose principal investigator does not have resources allocated by [competition](link-to-resource-allocation-competition-page)), go to the [page of available services](link-to-available-services-page) in CCDB and click "Join". Access will usually be granted within two business days.

If you need assistance, please [contact us](link-to-contact-us-page).


## Getting Started Guide

Please carefully read the [Niagara: Getting Started Guide](link-to-getting-started-guide).


Retrieved from "[https://docs.alliancecan.ca/mediawiki/index.php?title=Niagara/fr&oldid=170622](https://docs.alliancecan.ca/mediawiki/index.php?title=Niagara/fr&oldid=170622)"
