<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>PyTorch - Alliance Doc</title>
<script>(function(){var className="client-js";var cookie=document.cookie.match(/(?:^|; )ccwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"aDpE0I34nW-BQYizeTZISwAAA1I","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"PyTorch","wgTitle":"PyTorch","wgCurRevisionId":175635,"wgRevisionId":175635,"wgArticleId":4457,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":[
"Pages with syntax highlighting errors","Software","AI and Machine Learning"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"PyTorch","wgRelevantArticleId":4457,"wgIsProbablyEditable":false,"wgRelevantPageIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgULSAcceptLanguageList":[],"wgMFDisplayWikibaseDescriptions":{"search":false,"watchlist":false,"tagline":false},"wgCiteReferencePreviewsActive":true,"wgTranslatePageTranslation":"source","wgULSPosition":"personal","wgULSisCompactLinksEnabled":true,"wgVector2022LanguageInHeader":false,"wgULSisLanguageSelectorEmpty":false};RLSTATE={"site.styles":"ready","user.styles":"ready","user":"ready","user.options":"loading","ext.translate.tag.languages":"ready","ext.pygments":"ready","skins.vector.styles.legacy":"ready","ext.translate.edit.documentation.styles":"ready","ext.translate":"ready","codex-search-styles":"ready","ext.uls.pt":"ready"};RLPAGEMODULES=[
"ext.pygments.view","ext.tabs","site","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.languageSelector","ext.translate.pagetranslation.uls","ext.uls.compactlinks","ext.uls.geoclient","ext.uls.interface","ext.moderation.notify","ext.moderation.notify.desktop"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/mediawiki/load.php?lang=en&amp;modules=codex-search-styles%7Cext.pygments%2Ctranslate%7Cext.translate.edit.documentation.styles%7Cext.translate.tag.languages%7Cext.uls.pt%7Cskins.vector.styles.legacy&amp;only=styles&amp;skin=vector">
<script async="" src="/mediawiki/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/mediawiki/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector">
<meta name="generator" content="MediaWiki 1.43.0">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta name="viewport" content="width=1120">
<link rel="icon" href="/mediawiki/resources/assets/Alliance_favicon.png">
<link rel="search" type="application/opensearchdescription+xml" href="/mediawiki/rest.php/v1/search" title="Alliance Doc (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://docs.alliancecan.ca/mediawiki/api.php?action=rsd">
<link rel="alternate" type="application/atom+xml" title="Alliance Doc Atom feed" href="/mediawiki/index.php?title=Special:RecentChanges&amp;feed=atom">
<style type="text/css" id="tabs-dynamic-styles">/*<![CDATA[*/
/* Dynamically generated tabs styles */
.tabs-input-1:checked ~ .tabs-container .tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-content-2,
.tabs-input-0:checked ~ .tabs-container .tabs-content-1 {display:inline-block;}
.tabs-input-1:checked ~ .tabs-container .tabs-inline.tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-inline.tabs-content-2,
.tabs-input-0:checked ~ .tabs-container .tabs-inline.tabs-content-1 {display:inline;}
.tabs-input-1:checked ~ .tabs-container .tabs-block.tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-block.tabs-content-2,
.tabs-input-0:checked ~ .tabs-container .tabs-block.tabs-content-1 {display:block;}
/* The same styles, but with .checked instead of :checked, for browsers that rely on the JavaScript fallback */
.tabs-input-1.checked ~ .tabs-container .tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-content-2,
.tabs-input-0.checked ~ .tabs-container .tabs-content-1 {display:inline-block;}
.tabs-input-1.checked ~ .tabs-container .tabs-inline.tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-inline.tabs-content-2,
.tabs-input-0.checked ~ .tabs-container .tabs-inline.tabs-content-1 {display:inline;}
.tabs-input-1.checked ~ .tabs-container .tabs-block.tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-block.tabs-content-2,
.tabs-input-0.checked ~ .tabs-container .tabs-block.tabs-content-1 {display:block;}
.tabs-dropdown .tabs-content,.tabs-dropdown .tabs-container,.tabs-dropdown li,.tabs-dropdown ul,.tabs-dropdown ol {background-color: white /* Malicious data in tabs-dropdown-bgcolor */}
/*]]>*/</style>
</head>
<body class="skin-vector-legacy mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-PyTorch rootpage-PyTorch skin-vector action-view"><div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice"></div>
	<div class="mw-indicators">
	<div id="mw-indicator-languageselector" class="mw-indicator"><span id="languageselector-box-1" class="languageselector " style=""><form name="languageselector-form-1" id="languageselector-form-1" method="get" action="/mediawiki/index.php" style="display:inline;"><input type="hidden" value="PyTorch" name="title"><select name="setlang" id="languageselector-select-1" style=""><option value="aae">Arbërisht</option><option value="ab">аԥсшәа</option><option value="abs">bahasa ambon</option><option value="ace">Acèh</option><option value="acf">Kwéyòl Sent Lisi</option><option value="acm">عراقي</option><option value="ady">адыгабзэ</option><option value="ady-cyrl">адыгабзэ</option><option value="aeb">تونسي / Tûnsî</option><option value="aeb-arab">تونسي</option><option value="aeb-latn">Tûnsî</option><option value="af">Afrikaans</option><option value="aln">Gegë</option><option value="alt">алтай тил</option><option value="am">አማርኛ</option><option value="ami">Pangcah</option><option value="an">aragonés</option><option value="ang">Ænglisc</option><option value="ann">Obolo</option><option value="anp">अंगिका</option><option value="apc">شامي</option><option value="ar">العربية</option><option value="arc">ܐܪܡܝܐ</option><option value="arn">mapudungun</option><option value="arq">جازايرية</option><option value="ary">الدارجة</option><option value="arz">مصرى</option><option value="as">অসমীয়া</option><option value="ase">American sign language</option><option value="ast">asturianu</option><option value="atj">Atikamekw</option><option value="av">авар</option><option value="avk">Kotava</option><option value="awa">अवधी</option><option value="ay">Aymar aru</option><option value="az">azərbaycanca</option><option value="azb">تۆرکجه</option><option value="ba">башҡортса</option><option value="ban">Basa Bali</option><option value="ban-bali">ᬩᬲᬩᬮᬶ</option><option value="bar">Boarisch</option><option value="bbc">Batak Toba</option><option value="bbc-latn">Batak Toba</option><option value="bcc">جهلسری بلوچی</option><option value="bci">wawle</option><option value="bcl">Bikol Central</option><option value="bdr">Bajau Sama</option><option value="be">беларуская</option><option value="be-tarask">беларуская (тарашкевіца)</option><option value="bew">Betawi</option><option value="bg">български</option><option value="bgc">हरियाणवी</option><option value="bgn">روچ کپتین بلوچی</option><option value="bh">भोजपुरी</option><option value="bho">भोजपुरी</option><option value="bi">Bislama</option><option value="bjn">Banjar</option><option value="blk">ပအိုဝ်ႏဘာႏသာႏ</option><option value="bm">bamanankan</option><option value="bn">বাংলা</option><option value="bo">བོད་ཡིག</option><option value="bpy">বিষ্ণুপ্রিয়া মণিপুরী</option><option value="bqi">بختیاری</option><option value="br">brezhoneg</option><option value="brh">Bráhuí</option><option value="bs">bosanski</option><option value="btm">Batak Mandailing</option><option value="bto">Iriga Bicolano</option><option value="bug">Basa Ugi</option><option value="bxr">буряад</option><option value="ca">català</option><option value="cbk-zam">Chavacano de Zamboanga</option><option value="ccp">𑄌𑄋𑄴𑄟𑄳𑄦</option><option value="cdo">閩東語 / Mìng-dĕ̤ng-ngṳ̄</option><option value="ce">нохчийн</option><option value="ceb">Cebuano</option><option value="ch">Chamoru</option><option value="chn">chinuk wawa</option><option value="chr">ᏣᎳᎩ</option><option value="chy">Tsetsêhestâhese</option><option value="ckb">کوردی</option><option value="co">corsu</option><option value="cps">Capiceño</option><option value="cpx">莆仙語 / Pó-sing-gṳ̂</option><option value="cpx-hans">莆仙语（简体）</option><option value="cpx-hant">莆仙語（繁體）</option><option value="cr">Nēhiyawēwin / ᓀᐦᐃᔭᐍᐏᐣ</option><option value="crh">qırımtatarca</option><option value="crh-cyrl">къырымтатарджа (Кирилл)</option><option value="crh-latn">qırımtatarca (Latin)</option><option value="crh-ro">tatarşa</option><option value="cs">čeština</option><option value="csb">kaszëbsczi</option><option value="cu">словѣньскъ / ⰔⰎⰑⰂⰡⰐⰠⰔⰍⰟ</option><option value="cv">чӑвашла</option><option value="cy">Cymraeg</option><option value="da">dansk</option><option value="dag">dagbanli</option><option value="de">Deutsch</option><option value="de-at">Österreichisches Deutsch</option><option value="de-ch">Schweizer Hochdeutsch</option><option value="de-formal">Deutsch (Sie-Form)</option><option value="dga">Dagaare</option><option value="din">Thuɔŋjäŋ</option><option value="diq">Zazaki</option><option value="dsb">dolnoserbski</option><option value="dtp">Kadazandusun</option><option value="dty">डोटेली</option><option value="dua">Duálá</option><option value="dv">ދިވެހިބަސް</option><option value="dz">ཇོང་ཁ</option><option value="ee">eʋegbe</option><option value="efi">Efịk</option><option value="egl">emiliàn e rumagnòl</option><option value="el">Ελληνικά</option><option value="eml">emiliàn e rumagnòl</option><option value="en" selected="">English</option><option value="en-ca">Canadian English</option><option value="en-gb">British English</option><option value="eo">Esperanto</option><option value="es">español</option><option value="es-formal">español (formal)</option><option value="et">eesti</option><option value="eu">euskara</option><option value="ext">estremeñu</option><option value="fa">فارسی</option><option value="fat">mfantse</option><option value="ff">Fulfulde</option><option value="fi">suomi</option><option value="fit">meänkieli</option><option value="fj">Na Vosa Vakaviti</option><option value="fo">føroyskt</option><option value="fon">fɔ̀ngbè</option><option value="fr">français</option><option value="frc">français cadien</option><option value="frp">arpetan</option><option value="frr">Nordfriisk</option><option value="fur">furlan</option><option value="fy">Frysk</option><option value="ga">Gaeilge</option><option value="gaa">Ga</option><option value="gag">Gagauz</option><option value="gan">贛語</option><option value="gan-hans">赣语（简体）</option><option value="gan-hant">贛語（繁體）</option><option value="gcf">kréyòl Gwadloup</option><option value="gcr">kriyòl gwiyannen</option><option value="gd">Gàidhlig</option><option value="gl">galego</option><option value="gld">на̄ни</option><option value="glk">گیلکی</option><option value="gn">Avañe'ẽ</option><option value="gom">गोंयची कोंकणी / Gõychi Konknni</option><option value="gom-deva">गोंयची कोंकणी</option><option value="gom-latn">Gõychi Konknni</option><option value="gor">Bahasa Hulontalo</option><option value="got">𐌲𐌿𐍄𐌹𐍃𐌺</option><option value="gpe">Ghanaian Pidgin</option><option value="grc">Ἀρχαία ἑλληνικὴ</option><option value="gsw">Alemannisch</option><option value="gu">ગુજરાતી</option><option value="guc">wayuunaiki</option><option value="gur">farefare</option><option value="guw">gungbe</option><option value="gv">Gaelg</option><option value="ha">Hausa</option><option value="hak">客家語 / Hak-kâ-ngî</option><option value="haw">Hawaiʻi</option><option value="he">עברית</option><option value="hi">हिन्दी</option><option value="hif">Fiji Hindi</option><option value="hif-latn">Fiji Hindi</option><option value="hil">Ilonggo</option><option value="hno">ہندکو</option><option value="hr">hrvatski</option><option value="hrx">Hunsrik</option><option value="hsb">hornjoserbsce</option><option value="hsn">湘語</option><option value="ht">Kreyòl ayisyen</option><option value="hu">magyar</option><option value="hu-formal">magyar (formal)</option><option value="hy">հայերեն</option><option value="hyw">Արեւմտահայերէն</option><option value="ia">interlingua</option><option value="iba">Jaku Iban</option><option value="ibb">ibibio</option><option value="id">Bahasa Indonesia</option><option value="ie">Interlingue</option><option value="ig">Igbo</option><option value="igl">Igala</option><option value="ii">ꆇꉙ</option><option value="ik">Iñupiatun</option><option value="ike-cans">ᐃᓄᒃᑎᑐᑦ</option><option value="ike-latn">inuktitut</option><option value="ilo">Ilokano</option><option value="inh">гӀалгӀай</option><option value="io">Ido</option><option value="is">íslenska</option><option value="isv-cyrl">меджусловјанскы</option><option value="isv-latn">medžuslovjansky</option><option value="it">italiano</option><option value="iu">ᐃᓄᒃᑎᑐᑦ / inuktitut</option><option value="ja">日本語</option><option value="jam">Patois</option><option value="jbo">la .lojban.</option><option value="jut">jysk</option><option value="jv">Jawa</option><option value="ka">ქართული</option><option value="kaa">Qaraqalpaqsha</option><option value="kab">Taqbaylit</option><option value="kai">Karai-karai</option><option value="kbd">адыгэбзэ</option><option value="kbd-cyrl">адыгэбзэ</option><option value="kbp">Kabɩyɛ</option><option value="kcg">Tyap</option><option value="kea">kabuverdianu</option><option value="kg">Kongo</option><option value="kge">Kumoring</option><option value="khw">کھوار</option><option value="ki">Gĩkũyũ</option><option value="kiu">Kırmancki</option><option value="kjh">хакас</option><option value="kjp">ဖၠုံလိက်</option><option value="kk">қазақша</option><option value="kk-arab">قازاقشا (تٴوتە)</option><option value="kk-cn">قازاقشا (جۇنگو)</option><option value="kk-cyrl">қазақша (кирил)</option><option value="kk-kz">қазақша (Қазақстан)</option><option value="kk-latn">qazaqşa (latın)</option><option value="kk-tr">qazaqşa (Türkïya)</option><option value="kl">kalaallisut</option><option value="km">ភាសាខ្មែរ</option><option value="kn">ಕನ್ನಡ</option><option value="knc">Yerwa Kanuri</option><option value="ko">한국어</option><option value="ko-kp">조선말</option><option value="koi">перем коми</option><option value="kr">kanuri</option><option value="krc">къарачай-малкъар</option><option value="kri">Krio</option><option value="krj">Kinaray-a</option><option value="krl">karjal</option><option value="ks">कॉशुर / کٲشُر</option><option value="ks-arab">کٲشُر</option><option value="ks-deva">कॉशुर</option><option value="ksh">Ripoarisch</option><option value="ksw">စှီၤ</option><option value="ku">kurdî</option><option value="ku-arab">کوردی (عەرەبی)</option><option value="ku-latn">kurdî (latînî)</option><option value="kum">къумукъ</option><option value="kus">Kʋsaal</option><option value="kv">коми</option><option value="kw">kernowek</option><option value="ky">кыргызча</option><option value="la">Latina</option><option value="lad">Ladino</option><option value="lb">Lëtzebuergesch</option><option value="lbe">лакку</option><option value="lez">лезги</option><option value="lfn">Lingua Franca Nova</option><option value="lg">Luganda</option><option value="li">Limburgs</option><option value="lij">Ligure</option><option value="liv">Līvõ kēļ</option><option value="lki">لەکی</option><option value="lld">Ladin</option><option value="lmo">lombard</option><option value="ln">lingála</option><option value="lo">ລາວ</option><option value="loz">Silozi</option><option value="lrc">لۊری شومالی</option><option value="lt">lietuvių</option><option value="ltg">latgaļu</option><option value="lua">ciluba</option><option value="lus">Mizo ţawng</option><option value="luz">لئری دوٙمینی</option><option value="lv">latviešu</option><option value="lzh">文言</option><option value="lzz">Lazuri</option><option value="mad">Madhurâ</option><option value="mag">मगही</option><option value="mai">मैथिली</option><option value="map-bms">Basa Banyumasan</option><option value="mdf">мокшень</option><option value="mg">Malagasy</option><option value="mhr">олык марий</option><option value="mi">Māori</option><option value="min">Minangkabau</option><option value="mk">македонски</option><option value="ml">മലയാളം</option><option value="mn">монгол</option><option value="mnc">manju gisun</option><option value="mnc-latn">manju gisun</option><option value="mnc-mong">ᠮᠠᠨᠵᡠ ᡤᡳᠰᡠᠨ</option><option value="mni">ꯃꯤꯇꯩ ꯂꯣꯟ</option><option value="mnw">ဘာသာမန်</option><option value="mo">молдовеняскэ</option><option value="mos">moore</option><option value="mr">मराठी</option><option value="mrh">Mara</option><option value="mrj">кырык мары</option><option value="ms">Bahasa Melayu</option><option value="ms-arab">بهاس ملايو</option><option value="mt">Malti</option><option value="mui">Baso Palembang</option><option value="mwl">Mirandés</option><option value="my">မြန်မာဘာသာ</option><option value="myv">эрзянь</option><option value="mzn">مازِرونی</option><option value="na">Dorerin Naoero</option><option value="nah">Nāhuatl</option><option value="nan">閩南語 / Bân-lâm-gú</option><option value="nan-hant">閩南語（傳統漢字）</option><option value="nan-latn-pehoeji">Bân-lâm-gú (Pe̍h-ōe-jī)</option><option value="nan-latn-tailo">Bân-lâm-gú (Tâi-lô)</option><option value="nap">Napulitano</option><option value="nb">norsk bokmål</option><option value="nds">Plattdüütsch</option><option value="nds-nl">Nedersaksies</option><option value="ne">नेपाली</option><option value="new">नेपाल भाषा</option><option value="nia">Li Niha</option><option value="nit">కొలామి</option><option value="niu">Niuē</option><option value="nl">Nederlands</option><option value="nl-informal">Nederlands (informeel)</option><option value="nmz">nawdm</option><option value="nn">norsk nynorsk</option><option value="nod">ᨣᩤᩴᨾᩮᩬᩥᨦ</option><option value="nog">ногайша</option><option value="nov">Novial</option><option value="nqo">ߒߞߏ</option><option value="nr">isiNdebele seSewula</option><option value="nrm">Nouormand</option><option value="nso">Sesotho sa Leboa</option><option value="nup">Nupe</option><option value="nv">Diné bizaad</option><option value="ny">Chi-Chewa</option><option value="nyn">runyankore</option><option value="nyo">Orunyoro</option><option value="nys">Nyunga</option><option value="oc">occitan</option><option value="ojb">Ojibwemowin</option><option value="olo">livvinkarjala</option><option value="om">Oromoo</option><option value="or">ଓଡ଼ିଆ</option><option value="os">ирон</option><option value="pa">ਪੰਜਾਬੀ</option><option value="pag">Pangasinan</option><option value="pam">Kapampangan</option><option value="pap">Papiamentu</option><option value="pcd">Picard</option><option value="pcm">Naijá</option><option value="pdc">Deitsch</option><option value="pdt">Plautdietsch</option><option value="pfl">Pälzisch</option><option value="pi">पालि</option><option value="pih">Norfuk / Pitkern</option><option value="pl">polski</option><option value="pms">Piemontèis</option><option value="pnb">پنجابی</option><option value="pnt">Ποντιακά</option><option value="prg">prūsiskan</option><option value="ps">پښتو</option><option value="pt">português</option><option value="pt-br">português do Brasil</option><option value="pwn">pinayuanan</option><option value="qqq">Message documentation</option><option value="qu">Runa Simi</option><option value="qug">Runa shimi</option><option value="rgn">Rumagnôl</option><option value="rif">Tarifit</option><option value="rki">ရခိုင်</option><option value="rm">rumantsch</option><option value="rmc">romaňi čhib</option><option value="rmy">romani čhib</option><option value="rn">ikirundi</option><option value="ro">română</option><option value="roa-tara">tarandíne</option><option value="rsk">руски</option><option value="ru">русский</option><option value="rue">русиньскый</option><option value="rup">armãneashti</option><option value="ruq">Vlăheşte</option><option value="ruq-cyrl">Влахесте</option><option value="ruq-latn">Vlăheşte</option><option value="rut">мыхаӀбишды</option><option value="rw">Ikinyarwanda</option><option value="ryu">うちなーぐち</option><option value="sa">संस्कृतम्</option><option value="sah">саха тыла</option><option value="sat">ᱥᱟᱱᱛᱟᱲᱤ</option><option value="sc">sardu</option><option value="scn">sicilianu</option><option value="sco">Scots</option><option value="sd">سنڌي</option><option value="sdc">Sassaresu</option><option value="sdh">کوردی خوارگ</option><option value="se">davvisámegiella</option><option value="se-fi">davvisámegiella (Suoma bealde)</option><option value="se-no">davvisámegiella (Norgga bealde)</option><option value="se-se">davvisámegiella (Ruoŧa bealde)</option><option value="sei">Cmique Itom</option><option value="ses">Koyraboro Senni</option><option value="sg">Sängö</option><option value="sgs">žemaitėška</option><option value="sh">srpskohrvatski / српскохрватски</option><option value="sh-cyrl">српскохрватски (ћирилица)</option><option value="sh-latn">srpskohrvatski (latinica)</option><option value="shi">Taclḥit</option><option value="shn">ၽႃႇသႃႇတႆး </option><option value="shy">tacawit</option><option value="shy-latn">tacawit</option><option value="si">සිංහල</option><option value="sjd">кӣллт са̄мь кӣлл</option><option value="sje">bidumsámegiella</option><option value="sk">slovenčina</option><option value="skr">سرائیکی</option><option value="skr-arab">سرائیکی</option><option value="sl">slovenščina</option><option value="sli">Schläsch</option><option value="sm">Gagana Samoa</option><option value="sma">åarjelsaemien</option><option value="smn">anarâškielâ</option><option value="sms">nuõrttsääʹmǩiõll</option><option value="sn">chiShona</option><option value="so">Soomaaliga</option><option value="sq">shqip</option><option value="sr">српски / srpski</option><option value="sr-ec">српски (ћирилица)</option><option value="sr-el">srpski (latinica)</option><option value="srn">Sranantongo</option><option value="sro">sardu campidanesu</option><option value="ss">SiSwati</option><option value="st">Sesotho</option><option value="stq">Seeltersk</option><option value="sty">себертатар</option><option value="su">Sunda</option><option value="sv">svenska</option><option value="sw">Kiswahili</option><option value="syl">ꠍꠤꠟꠐꠤ</option><option value="szl">ślůnski</option><option value="szy">Sakizaya</option><option value="ta">தமிழ்</option><option value="tay">Tayal</option><option value="tcy">ತುಳು</option><option value="tdd">ᥖᥭᥰ ᥖᥬᥲ ᥑᥨᥒᥰ</option><option value="te">తెలుగు</option><option value="tet">tetun</option><option value="tg">тоҷикӣ</option><option value="tg-cyrl">тоҷикӣ</option><option value="tg-latn">tojikī</option><option value="th">ไทย</option><option value="ti">ትግርኛ</option><option value="tig">ትግሬ</option><option value="tk">Türkmençe</option><option value="tl">Tagalog</option><option value="tly">tolışi</option><option value="tn">Setswana</option><option value="to">lea faka-Tonga</option><option value="tok">toki pona</option><option value="tpi">Tok Pisin</option><option value="tr">Türkçe</option><option value="tru">Ṫuroyo</option><option value="trv">Seediq</option><option value="ts">Xitsonga</option><option value="tt">татарча / tatarça</option><option value="tt-cyrl">татарча</option><option value="tt-latn">tatarça</option><option value="ttj">Orutooro</option><option value="tum">chiTumbuka</option><option value="tw">Twi</option><option value="ty">reo tahiti</option><option value="tyv">тыва дыл</option><option value="tzm">ⵜⴰⵎⴰⵣⵉⵖⵜ</option><option value="udm">удмурт</option><option value="ug">ئۇيغۇرچە / Uyghurche</option><option value="ug-arab">ئۇيغۇرچە</option><option value="ug-latn">Uyghurche</option><option value="uk">українська</option><option value="ur">اردو</option><option value="uz">oʻzbekcha / ўзбекча</option><option value="ve">Tshivenda</option><option value="vec">vèneto</option><option value="vep">vepsän kel’</option><option value="vi">Tiếng Việt</option><option value="vls">West-Vlams</option><option value="vmf">Mainfränkisch</option><option value="vmw">emakhuwa</option><option value="vo">Volapük</option><option value="vot">Vaďďa</option><option value="vro">võro</option><option value="wa">walon</option><option value="wal">wolaytta</option><option value="war">Winaray</option><option value="wls">Fakaʻuvea</option><option value="wo">Wolof</option><option value="wuu">吴语</option><option value="wuu-hans">吴语（简体）</option><option value="wuu-hant">吳語（正體）</option><option value="xal">хальмг</option><option value="xh">isiXhosa</option><option value="xmf">მარგალური</option><option value="xsy">saisiyat</option><option value="yi">ייִדיש</option><option value="yo">Yorùbá</option><option value="yrl">Nhẽẽgatú</option><option value="yue">粵語</option><option value="yue-hans">粵语（简体）</option><option value="yue-hant">粵語（繁體）</option><option value="za">Vahcuengh</option><option value="zea">Zeêuws</option><option value="zgh">ⵜⴰⵎⴰⵣⵉⵖⵜ ⵜⴰⵏⴰⵡⴰⵢⵜ</option><option value="zh">中文</option><option value="zh-cn">中文（中国大陆）</option><option value="zh-hans">中文（简体）</option><option value="zh-hant">中文（繁體）</option><option value="zh-hk">中文（香港）</option><option value="zh-mo">中文（澳門）</option><option value="zh-my">中文（马来西亚）</option><option value="zh-sg">中文（新加坡）</option><option value="zh-tw">中文（臺灣）</option><option value="zu">isiZulu</option></select><input id="languageselector-commit-1" style="" type="submit" value="set"></form></span></div>
	</div>
	<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">PyTorch</span></h1>
	<div id="bodyContent" class="vector-body">
		<div id="siteSub" class="noprint">From Alliance Doc</div>
		<div id="contentSub"><div id="mw-content-subtitle"></div></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" class="mw-body-content"><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="mw-pt-languages noprint navigation-not-searchable" lang="en" dir="ltr"><div class="mw-pt-languages-label">Other languages:</div><ul class="mw-pt-languages-list"><li><span class="mw-pt-languages-ui mw-pt-languages-selected mw-pt-progress mw-pt-progress--complete" lang="en" dir="ltr">English</span></li>
<li><a href="/wiki/PyTorch/fr" class="mw-pt-progress mw-pt-progress--complete" title="PyTorch (100% translated)" lang="fr" dir="ltr">français</a></li></ul></div>
<p><a rel="nofollow" class="external text" href="http://pytorch.org/">PyTorch</a> is a Python package that provides two high-level features:
</p>
<ul><li>Tensor computation (like NumPy) with strong GPU acceleration</li>
<li>Deep neural networks built on a tape-based autograd system</li></ul>
<p>If you are porting a PyTorch program to one of our clusters, you should follow <a href="/wiki/Tutoriel_Apprentissage_machine/en" title="Tutoriel Apprentissage machine/en">our tutorial on the subject</a>.
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Disambiguation"><span class="tocnumber">1</span> <span class="toctext">Disambiguation</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Installation"><span class="tocnumber">2</span> <span class="toctext">Installation</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Latest_available_wheels"><span class="tocnumber">2.1</span> <span class="toctext">Latest available wheels</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Installing_our_wheel"><span class="tocnumber">2.2</span> <span class="toctext">Installing our wheel</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="#GPU_and_CPU"><span class="tocnumber">2.2.1</span> <span class="toctext">GPU and CPU</span></a></li>
<li class="toclevel-3 tocsection-6"><a href="#Extra"><span class="tocnumber">2.2.2</span> <span class="toctext">Extra</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-7"><a href="#Job_submission"><span class="tocnumber">3</span> <span class="toctext">Job submission</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#High_performance_with_PyTorch"><span class="tocnumber">4</span> <span class="toctext">High performance with PyTorch</span></a>
<ul>
<li class="toclevel-2 tocsection-9"><a href="#TF32:_Performance_vs_numerical_accuracy"><span class="tocnumber">4.1</span> <span class="toctext">TF32: Performance vs numerical accuracy</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#PyTorch_with_multiple_CPUs"><span class="tocnumber">4.2</span> <span class="toctext">PyTorch with multiple CPUs</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#PyTorch_with_a_single_GPU"><span class="tocnumber">4.3</span> <span class="toctext">PyTorch with a single GPU</span></a>
<ul>
<li class="toclevel-3 tocsection-12"><a href="#Data_parallelism_with_a_single_GPU"><span class="tocnumber">4.3.1</span> <span class="toctext">Data parallelism with a single GPU</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-13"><a href="#PyTorch_with_multiple_GPUs"><span class="tocnumber">4.4</span> <span class="toctext">PyTorch with multiple GPUs</span></a>
<ul>
<li class="toclevel-3 tocsection-14"><a href="#Issue_with_DistributedDataParallel_and_PyTorch_1.10"><span class="tocnumber">4.4.1</span> <span class="toctext">Issue with DistributedDataParallel and PyTorch 1.10</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="#Data_parallelism_with_multiple_GPUs"><span class="tocnumber">4.4.2</span> <span class="toctext">Data parallelism with multiple GPUs</span></a>
<ul>
<li class="toclevel-4 tocsection-16"><a href="#Using_DistributedDataParallel"><span class="tocnumber">4.4.2.1</span> <span class="toctext">Using DistributedDataParallel</span></a></li>
<li class="toclevel-4 tocsection-17"><a href="#Using_PyTorch_Lightning"><span class="tocnumber">4.4.2.2</span> <span class="toctext">Using PyTorch Lightning</span></a></li>
<li class="toclevel-4 tocsection-18"><a href="#Using_Horovod"><span class="tocnumber">4.4.2.3</span> <span class="toctext">Using Horovod</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-19"><a href="#Model_parallelism_with_multiple_GPUs"><span class="tocnumber">4.4.3</span> <span class="toctext">Model parallelism with multiple GPUs</span></a></li>
<li class="toclevel-3 tocsection-20"><a href="#Combining_model_and_data_parallelism"><span class="tocnumber">4.4.4</span> <span class="toctext">Combining model and data parallelism</span></a>
<ul>
<li class="toclevel-4 tocsection-21"><a href="#Using_Torch_RPC_and_DDP"><span class="tocnumber">4.4.4.1</span> <span class="toctext">Using Torch RPC and DDP</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-22"><a href="#DeepSpeed"><span class="tocnumber">4.4.5</span> <span class="toctext">DeepSpeed</span></a>
<ul>
<li class="toclevel-4 tocsection-23"><a href="#ZeRO_on_GPU"><span class="tocnumber">4.4.5.1</span> <span class="toctext">ZeRO on GPU</span></a></li>
<li class="toclevel-4 tocsection-24"><a href="#ZeRO_with_offload_to_CPU"><span class="tocnumber">4.4.5.2</span> <span class="toctext">ZeRO with offload to CPU</span></a></li>
<li class="toclevel-4 tocsection-25"><a href="#ZeRO_with_offload_to_NVMe"><span class="tocnumber">4.4.5.3</span> <span class="toctext">ZeRO with offload to NVMe</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-26"><a href="#Creating_model_checkpoints"><span class="tocnumber">5</span> <span class="toctext">Creating model checkpoints</span></a>
<ul>
<li class="toclevel-2 tocsection-27"><a href="#With_PyTorch_Lightning"><span class="tocnumber">5.1</span> <span class="toctext">With PyTorch Lightning</span></a></li>
<li class="toclevel-2 tocsection-28"><a href="#With_custom_training_loops"><span class="tocnumber">5.2</span> <span class="toctext">With custom training loops</span></a></li>
<li class="toclevel-2 tocsection-29"><a href="#During_distributed_training"><span class="tocnumber">5.3</span> <span class="toctext">During distributed training</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-30"><a href="#Troubleshooting"><span class="tocnumber">6</span> <span class="toctext">Troubleshooting</span></a>
<ul>
<li class="toclevel-2 tocsection-31"><a href="#Memory_leak"><span class="tocnumber">6.1</span> <span class="toctext">Memory leak</span></a></li>
<li class="toclevel-2 tocsection-32"><a href="#c10::Error"><span class="tocnumber">6.2</span> <span class="toctext">c10::Error</span></a></li>
<li class="toclevel-2 tocsection-33"><a href="#CUDA_error:_no_kernel_image_is_available_for_execution_on_the_device"><span class="tocnumber">6.3</span> <span class="toctext">CUDA error: no kernel image is available for execution on the device</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-34"><a href="#LibTorch"><span class="tocnumber">7</span> <span class="toctext">LibTorch</span></a>
<ul>
<li class="toclevel-2 tocsection-35"><a href="#How_to_use_LibTorch"><span class="tocnumber">7.1</span> <span class="toctext">How to use LibTorch</span></a>
<ul>
<li class="toclevel-3 tocsection-36"><a href="#Setting_up_the_environment"><span class="tocnumber">7.1.1</span> <span class="toctext">Setting up the environment</span></a></li>
<li class="toclevel-3 tocsection-37"><a href="#Compiling_a_minimal_example"><span class="tocnumber">7.1.2</span> <span class="toctext">Compiling a minimal example</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-38"><a href="#Resources"><span class="tocnumber">8</span> <span class="toctext">Resources</span></a></li>
</ul>
</div>

<h1><span class="mw-headline" id="Disambiguation">Disambiguation</span></h1>
<p>PyTorch has a distant connection with <a href="/wiki/Torch" title="Torch">Torch</a>, but for all practical purposes you can treat them as separate projects.
</p><p>PyTorch developers also offer <a href="#LibTorch">LibTorch</a>, which allows one to implement extensions to PyTorch using C++, and to implement pure C++ machine learning applications. Models written in Python using PyTorch can be converted and used in pure C++ through <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/advanced/cpp_export.html">TorchScript</a>.
</p>
<h1><span class="mw-headline" id="Installation">Installation</span></h1>
<h2><span class="mw-headline" id="Latest_available_wheels">Latest available wheels</span></h2>
<p>To see the latest version of PyTorch that we have built:
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=avail_wheels+torch" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>avail_wheels<span class="w"> </span>torch
</pre></div>
</div>
</div>
<p>For more information, see <a href="/wiki/Python#Available_wheels" title="Python">Available wheels</a>.
</p>
<h2><span class="mw-headline" id="Installing_our_wheel">Installing our wheel</span></h2>
<p>The preferred option is to install it using the Python <a rel="nofollow" class="external text" href="https://pythonwheels.com/">wheel</a> as follows: 
</p>
<dl><dd>1. Load a Python <a href="/wiki/Utiliser_des_modules/en#Sub-command_load" title="Utiliser des modules/en">module</a>, thus <code>module load python</code></dd>
<dd>2. Create and start a <a href="/wiki/Python#Creating_and_using_a_virtual_environment" title="Python">virtual environment</a>.</dd>
<dd>3. Install PyTorch in the virtual environment with <code>pip install</code>.</dd></dl>
<h4><span class="mw-headline" id="GPU_and_CPU">GPU and CPU</span></h4>
<dl><dd><div></div></dd></dl>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=pip+install+--no-index+torch" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">(</span>venv<span class="o">)</span><span class="w"> </span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-index<span class="w"> </span>torch
</pre></div>
</div>

<table class="wikitable" width="600px">
<tbody><tr>
<td>
<p><span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Light-bulb.png" class="mw-file-description"><img src="/mediawiki/images/8/8b/Light-bulb.png" decoding="async" width="25" height="25" class="mw-file-element" /></a></span>With H100 gpus, torch 2.3 and higher is required.
</p>
</td>
</tr>
</tbody></table>
<p><br />
<b>Note:</b> There are known issues with PyTorch 1.10 on our clusters (except for Narval). If you encounter problems while using distributed training, or if you get an error containing <code>c10::Error</code>, we recommend installing PyTorch 1.9.1 using <code>pip install --no-index torch==1.9.1</code>.
</p>
<h4><span class="mw-headline" id="Extra">Extra</span></h4>
<p>In addition to <code>torch</code>, you can install <code>torchvision</code>, <code>torchtext</code> and <code>torchaudio</code>:
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=pip+install+--no-index+torch+torchvision+torchtext+torchaudio" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">(</span>venv<span class="o">)</span><span class="w"> </span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-index<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchtext<span class="w"> </span>torchaudio
</pre></div>
</div>
</div>
<h1><span class="mw-headline" id="Job_submission">Job submission</span></h1>
<p>Here is an example of a job submission script using the python wheel, with a virtual environment inside a job:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-test.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--gres%3Dgpu%3A1+++++++%23+Request+GPU+%22generic+resources%22%0A%23SBATCH+--cpus-per-task%3D6++%23+Cores+proportional+to+GPUs%3A+6+on+Cedar%2C+16+on+Graham.%0A%23SBATCH+--mem%3D32000M+++++++%23+Memory+proportional+to+GPUs%3A+32000+Cedar%2C+64000+Graham.%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python%2F%3Cselect+version%3E+%23+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+--no-index%0A%0Apython+pytorch-test.py" />
<input type="hidden" name="filename" value="pytorch-test.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --gres=gpu:1       # Request GPU &quot;generic resources&quot;</span>
<span class="c1">#SBATCH --cpus-per-task=6  # Cores proportional to GPUs: 6 on Cedar, 16 on Graham.</span>
<span class="c1">#SBATCH --mem=32000M       # Memory proportional to GPUs: 32000 Cedar, 64000 Graham.</span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python/&lt;<span class="k">select</span><span class="w"> </span>version&gt;<span class="w"> </span><span class="c1"># Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>--no-index

python<span class="w"> </span>pytorch-test.py
</pre></div>
</div>
<p><br />
</p><p>The Python script <code>pytorch-test.py</code> has the form
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-test.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+torch%0Ax+%3D+torch.Tensor%285%2C+3%29%0Aprint%28x%29%0Ay+%3D+torch.rand%285%2C+3%29%0Aprint%28y%29%0A%23+let+us+run+the+following+only+if+CUDA+is+available%0Aif+torch.cuda.is_available%28%29%3A%0A++++x+%3D+x.cuda%28%29%0A++++y+%3D+y.cuda%28%29%0A++++print%28x+%2B+y%29" />
<input type="hidden" name="filename" value="pytorch-test.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="c1"># let us run the following only if CUDA is available</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p><br />
</p><p>You can then submit a PyTorch job with:
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=sbatch+pytorch-test.sh" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>sbatch<span class="w"> </span>pytorch-test.sh
</pre></div>
</div>
</div>
<h1><span class="mw-headline" id="High_performance_with_PyTorch">High performance with PyTorch</span></h1>
<h2><span class="mw-headline" id="TF32:_Performance_vs_numerical_accuracy">TF32: Performance vs numerical accuracy</span></h2>
<p>On version 1.7.0 PyTorch has introduced support for <a rel="nofollow" class="external text" href="https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/">Nvidia's TensorFloat-32 (TF32) Mode</a>, which in turn is available only on Ampere and later Nvidia GPU architectures. This mode of executing tensor operations has been shown to yield up to 20x speed-ups compared to equivalent single precision (FP32) operations and is enabled by default in PyTorch versions 1.7.x up to 1.11.x. However, such gains in performance come at the cost of potentially decreased accuracy in the results of operations, which may become problematic in cases such as when dealing with ill-conditioned matrices, or when performing long sequences of tensor operations as is common in deep learning models. Following calls from its user community, TF32 is now <b>disabled by default for matrix multiplications</b>, but still <b>enabled by default for convolutions</b> starting with PyTorch version 1.12.0.
</p><p>As of October 2022, our only cluster equipped with Ampere GPUs is <a href="/wiki/Narval" title="Narval">Narval</a>. When using PyTorch on Narval, users should be cognizant of the following:
</p>
<ol><li>You may notice a significant slowdown when running the exact same GPU-enabled code with <code>torch &lt; 1.12.0</code> and <code>torch &gt;= 1.12.0</code>.</li>
<li>You may get different results when running the exact same GPU-enabled code with <code>torch &lt; 1.12.0</code> and <code>torch &gt;= 1.12.0</code>.</li></ol>
<p>To enable or disable TF32 on <code>torch &gt;= 1.12.0</code> set the following flags to <code>True</code> or <code>False</code> accordingly:
</p>
<pre>torch.backends.cuda.matmul.allow_tf32 = False # Enable/disable TF32 for matrix multiplications
torch.backends.cudnn.allow_tf32 = False # Enable/disable TF32 for convolutions
</pre>
<p>For more information, see <a rel="nofollow" class="external text" href="https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere">PyTorch's official documentation</a>
</p>
<h2><span class="mw-headline" id="PyTorch_with_multiple_CPUs">PyTorch with multiple CPUs</span></h2>
<p>PyTorch natively supports parallelizing work across multiple CPUs in two ways: intra-op parallelism and inter-op parallelism.
</p>
<ul><li><b>intra-op</b> refers to PyTorch's parallel implementations of operators commonly used in Deep Learning, such as matrix multiplication and convolution, using <a rel="nofollow" class="external text" href="https://www.openmp.org">OpenMP</a> directly or through low-level libraries like <a rel="nofollow" class="external text" href="https://en.wikipedia.org/wiki/Math_Kernel_Library">MKL</a> and <a rel="nofollow" class="external text" href="https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top/api-based-programming/intel-oneapi-deep-neural-network-library-onednn.html">OneDNN</a>. Whenever you run PyTorch code that performs such operations, they will automatically leverage multi-threading over as many CPU cores as are available to your job.</li>
<li><b>inter-op</b> parallelism on the other hand refers to PyTorch's ability to execute different parts of your code concurrently. This modality of parallelism typically requires that you explicitly design your program such that different parts can run in parallel. Examples include code that leverages PyTorch's Just-In-Time compiler <code>torch.jit</code> to run asynchronous tasks in a <a rel="nofollow" class="external text" href="https://pytorch.org/docs/stable/jit.html#built-in-functions-and-modules">TorchScript</a> program.</li></ul>
<p>With small scale models, we strongly recommend using <b>multiple CPUs instead of using a GPU</b>. While training will almost certainly run faster on a GPU (except in cases where the model is very small), if your model and your dataset are not large enough, the speed up relative to CPU will likely not be very significant and your job will end up using only a small portion of the GPU's compute capabilities. This might not be an issue on your own workstation, but in a shared environment like our HPC clusters, this means you are unnecessarily blocking a resource that another user may need to run actual large scale computations! Furthermore, you would be unnecessarily using up your group's allocation and affecting the priority of your colleagues' jobs.
</p><p>The code example below contains many opportunities for intra-op parallelism. By simply requesting more CPUs and without any code changes, we can observe the effect of PyTorch's native support for parallelism on performance:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-multi-cpu.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--tasks-per-node%3D1+%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+to+see+the+effect+on+performance%0A%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A05%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0Aecho+%22starting+training...%22%0A%0Atime+python+cifar10-cpu.py" />
<input type="hidden" name="filename" value="pytorch-multi-cpu.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --tasks-per-node=1 </span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... to see the effect on performance</span>

<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:05:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>

<span class="nb">time</span><span class="w"> </span>python<span class="w"> </span>cifar10-cpu.py
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> cifar10-cpu.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+numpy+as+np%0Aimport+time%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0Aimport+os%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+cpu+performance+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A++++torch.set_num_threads%28int%28os.environ%5B%27SLURM_CPUS_PER_TASK%27%5D%29%29%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++%23%23%23+This+next+line+will+attempt+to+download+the+CIFAR10+dataset+from+the+internet+if+you+don%27t+already+have+it+stored+in+.%2Fdata+%0A++++%23%23%23+Run+this+line+on+a+login+node+with+%22download%3DTrue%22+prior+to+submitting+your+job%2C+or+manually+download+the+data+from+%0A++++%23%23%23+https%3A%2F%2Fwww.cs.toronto.edu%2F%7Ekriz%2Fcifar-10-python.tar.gz+and+place+it+under+.%2Fdata%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="cifar10-cpu.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, cpu performance test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_CPUS_PER_TASK&#39;</span><span class="p">]))</span>
    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="c1">### This next line will attempt to download the CIFAR10 dataset from the internet if you don&#39;t already have it stored in ./data </span>
    <span class="c1">### Run this line on a login node with &quot;download=True&quot; prior to submitting your job, or manually download the data from </span>
    <span class="c1">### https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and place it under ./data</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h2><span class="mw-headline" id="PyTorch_with_a_single_GPU">PyTorch with a single GPU</span></h2>
<p>There is a common misconception that you should definitely use a GPU for model training if one is available. While this may <i>almost always</i>  hold true (training very small models is often faster on one or more CPUs) on your own local workstation equipped with a GPU, it is not the case on our HPC clusters.
</p><p>Simply put, <b>you should not ask for a GPU</b> if your code is not capable of making a reasonable use of its compute capacity.
</p><p>GPUs draw their performance advantage in Deep Learning tasks mainly from two sources: 
</p>
<ol><li>Their ability to parallelize the execution of certain key numerical operations, such as <a rel="nofollow" class="external text" href="https://en.wikipedia.org/wiki/Multiply–accumulate_operation">multiply-accumulate</a>, over many thousands of compute cores compared to the single-digit count of cores available in most common CPUs.</li>
<li>A much higher memory bandwidth than CPUs, which allows GPUs to efficiently use their massive number of cores to process much larger amounts of data per compute cycle.</li></ol>
<p>Like in the multi-cpu case, PyTorch contains parallel implementations of operators commonly used in Deep Learning, such as matrix multiplication and convolution, using GPU-specific libraries like <a rel="nofollow" class="external text" href="https://developer.nvidia.com/cudnn">CUDNN</a> or <a rel="nofollow" class="external text" href="https://github.com/ROCmSoftwarePlatform/MIOpen">MIOpen</a>, depending on the hardware platform. This means that for a learning task to be worth running on a GPU, it must be composed of elements that scale out with massive parallelism in terms of the number of operations that can be performed in parallel, the amount of data they require, or, ideally, both. Concretely this means, for example, large models (with large numbers of units and layers), large inputs, or, ideally, both.
</p><p>In the example below, we adapt the multi-cpu code from the previous section to run on one GPU and examine its performance. We can observe that two parameters play an important role: <code>batch_size</code> and <code>num_workers</code>. The first influences performance by increasing the size of our inputs at each iteration, thus putting more of the GPU's capacity to use. The second influences performance by streamlining the movement of our inputs from the Host's (or the CPU's) memory to the GPU's memory, thus reducing the amount of time the GPU sits idle waiting for data to process.
</p><p>Two takeaways emerge from this:
</p>
<ol><li>Increase your <code>batch_size</code> to as much as you can fit in the GPU's memory to optimize your compute performance.</li>
<li>Use a <code>DataLoader</code> with as many workers as you have <code>cpus-per-task</code> to streamline feeding data to the GPU.</li></ol>
<p>Of course, <code>batch_size</code> is also an important parameter with respect to a model's performance on a given task (accuracy, error, etc.) and different schools of thought have different views on the impact of using large batches. This page will not go into this subject, but if you have reason to believe that a small (relative to space in GPU memory) batch size is best for your application, skip to <a class="mw-selflink-fragment" href="#Data_Parallelism_with_a_single_GPU">Data Parallelism with a single GPU</a> to see how to maximize GPU utilization with small inputs.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-single-gpu.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--gres%3Dgpu%3A1+%23+request+a+GPU%0A%23SBATCH+--tasks-per-node%3D1+%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A05%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0Aecho+%22starting+training...%22%0Atime+python+cifar10-gpu.py+--batch_size%3D512+--num_workers%3D0" />
<input type="hidden" name="filename" value="pytorch-single-gpu.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --gres=gpu:1 # request a GPU</span>
<span class="c1">#SBATCH --tasks-per-node=1 </span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:05:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>
<span class="nb">time</span><span class="w"> </span>python<span class="w"> </span>cifar10-gpu.py<span class="w"> </span>--batch_size<span class="o">=</span><span class="m">512</span><span class="w"> </span>--num_workers<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> cifar10-gpu.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+numpy+as+np%0Aimport+time%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+single+gpu+performance+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29.cuda%28%29+%23+Load+model+on+the+GPU%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29+%23+Load+the+loss+function+on+the+GPU%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A+++++++%0A+++++++inputs+%3D+inputs.cuda%28%29+%0A+++++++targets+%3D+targets.cuda%28%29%0A%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="cifar10-gpu.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, single gpu performance test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="c1"># Load model on the GPU</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="c1"># Load the loss function on the GPU</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
       
       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> 
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span class="mw-headline" id="Data_parallelism_with_a_single_GPU">Data parallelism with a single GPU</span></h3>
<p>In cases where a model is fairly small, such that it does not take up a large portion of GPU memory and it cannot use a reasonable amount of its compute capacity, it is <b>not advisable to use a GPU</b>. Use <a class="mw-selflink-fragment" href="#PyTorch_with_Multiple_CPUs">one or more CPUs</a> instead. However, in a scenario where you have such a model, but have a very large dataset and wish to perform training with a small batch size, taking advantage of Data parallelism on a GPU becomes a viable option. 
</p><p>Data Parallelism, in this context, refers to methods to perform training over multiple replicas of a model in parallel, where each replica receives a different chunk of training data at each iteration. Gradients are then aggregated at the end of an iteration and the parameters of all replicas are updated in a synchronous or asynchronous fashion, depending on the method. Using this approach may provide a significant speed-up by iterating through all examples in a large dataset approximately <i>N</i> times faster, where <i>N</i> is the number of model replicas. An <b>important caveat</b> of this approach, is that in order to get a trained model that is equivalent to the same model trained without Data Parallelism, the user must scale either the learning rate or the desired batch size in function of the number of replicas. See <a rel="nofollow" class="external text" href="https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/13">this discussion</a> for more information.  
</p><p>PyTorch has implementations of Data Parallelism methods, with the <code>DistributedDataParallel</code> class being the one <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel">recommended by PyTorch maintainers</a> for best performance. Designed to work with <a rel="nofollow" class="external text" href="https://docs.computecanada.ca/wiki/PyTorch#PyTorch_with_Multiple_GPUs">multiple GPUs</a>, it can be also be used with a single GPU.
</p><p>In the example that follows, we adapt the single GPU code from the previous section to use Data Parallelism. This task is fairly small - with a batch size of 512 images, our model takes up about 1GB of GPU memory space, and it uses only about 6% of its compute capacity during training. This is a model that <b>should not</b> be trained on our clusters. However, using Data Parallelism, we can fit up to 14 or 15 replicas of this model on a V100 GPU with 16GB memory and increase our resource usage, while getting a nice speed-up. We use Nvidia's <a rel="nofollow" class="external text" href="https://docs.nvidia.com/deploy/mps/index.html">Multi-Process Service (MPS)</a>, along with <a rel="nofollow" class="external text" href="https://docs.computecanada.ca/wiki/MPI">MPI</a> to efficiently place multiple model replicas on one GPU:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-gpu-mps.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--gres%3Dgpu%3A1+%23+request+a+GPU%0A%23SBATCH+--tasks-per-node%3D8+%23+This+is+the+number+of+model+replicas+we+will+place+on+the+GPU.+Change+this+to+10%2C12%2C14%2C...+to+see+the+effect+on+performance++%0A%23SBATCH+--cpus-per-task%3D1+%23+increase+this+parameter+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A05%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0A%23+Activate+Nvidia+MPS%3A%0Aexport+CUDA_MPS_PIPE_DIRECTORY%3D%2Ftmp%2Fnvidia-mps%0Aexport+CUDA_MPS_LOG_DIRECTORY%3D%2Ftmp%2Fnvidia-log%0Anvidia-cuda-mps-control+-d%0A%0Aecho+%22starting+training...%22%0Atime+srun+--cpus-per-task%3D%24SLURM_CPUS_PER_TASK+python+cifar10-gpu-mps.py+--batch_size%3D512+--num_workers%3D0" />
<input type="hidden" name="filename" value="pytorch-gpu-mps.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --gres=gpu:1 # request a GPU</span>
<span class="c1">#SBATCH --tasks-per-node=8 # This is the number of model replicas we will place on the GPU. Change this to 10,12,14,... to see the effect on performance  </span>
<span class="c1">#SBATCH --cpus-per-task=1 # increase this parameter and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:05:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="c1"># Activate Nvidia MPS:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_MPS_PIPE_DIRECTORY</span><span class="o">=</span>/tmp/nvidia-mps
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_MPS_LOG_DIRECTORY</span><span class="o">=</span>/tmp/nvidia-log
nvidia-cuda-mps-control<span class="w"> </span>-d

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>
<span class="nb">time</span><span class="w"> </span>srun<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span><span class="w"> </span>python<span class="w"> </span>cifar10-gpu-mps.py<span class="w"> </span>--batch_size<span class="o">=</span><span class="m">512</span><span class="w"> </span>--num_workers<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> cifar10-gpu-mps.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0Aimport+time%0Aimport+datetime%0Aimport+numpy+as+np%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+distributed+data+parallel+maps+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--init_method%27%2C+default%3D%27tcp%3A%2F%2F127.0.0.1%3A3456%27%2C+type%3Dstr%2C+help%3D%27%27%29%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++rank+%3D+os.environ.get%28%22SLURM_LOCALID%22%29%0A%0A++++current_device+%3D+0%0A++++torch.cuda.set_device%28current_device%29%0A%0A++++%22%22%22+this+block+initializes+a+process+group+and+initiate+communications%0A++++++++++++++++between+all+processes+that+will+run+a+model+replica+%22%22%22%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Initializing+Process+Group...%27.format%28rank%29%29%0A%0A++++dist.init_process_group%28backend%3D%22mpi%22%2C+init_method%3Dargs.init_method%29+%23+Use+backend%3D%22mpi%22+or+%22gloo%22.+NCCL+does+not+work+on+a+single+GPU+due+to+a+hard-coded+multi-GPU+topology+check.%0A++++print%28%22process+group+ready%21%22%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Making+model..%27.format%28rank%29%29%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++net.cuda%28%29%0A++++net+%3D+torch.nn.parallel.DistributedDataParallel%28net%2C+device_ids%3D%5Bcurrent_device%5D%29+%23+Wrap+the+model+with+DistributedDataParallel%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Preparing+data..%27.format%28rank%29%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27%7E%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A+++++++%0A+++++++inputs+%3D+inputs.cuda%28%29+%0A+++++++targets+%3D+targets.cuda%28%29%0A%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="cifar10-gpu-mps.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, distributed data parallel maps test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--init_method&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;tcp://127.0.0.1:3456&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_LOCALID&quot;</span><span class="p">)</span>

    <span class="n">current_device</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; this block initializes a process group and initiate communications</span>
<span class="sd">                between all processes that will run a model replica &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Initializing Process Group...&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;mpi&quot;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">)</span> <span class="c1"># Use backend=&quot;mpi&quot; or &quot;gloo&quot;. NCCL does not work on a single GPU due to a hard-coded multi-GPU topology check.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;process group ready!&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Making model..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">net</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">current_device</span><span class="p">])</span> <span class="c1"># Wrap the model with DistributedDataParallel</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Preparing data..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;~/data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
       
       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> 
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h2><span class="mw-headline" id="PyTorch_with_multiple_GPUs">PyTorch with multiple GPUs</span></h2>
<h3><span class="mw-headline" id="Issue_with_DistributedDataParallel_and_PyTorch_1.10">Issue with DistributedDataParallel and PyTorch 1.10</span></h3>
<p>There is a known issue with our PyTorch 1.10 wheel <code>torch-1.10.0+computecanada</code>. Multi-GPU code that uses <a class="mw-selflink-fragment" href="#Using_DistributedDataParallel">DistributedDataParallel</a> running with this PyTorch version may fail unpredictably if the backend is set to <code>'nccl'</code> or <code>'gloo'</code>. We recommend using our latest PyTorch build instead of version 1.10 on all GP clusters.
</p>
<h3><span class="mw-headline" id="Data_parallelism_with_multiple_GPUs">Data parallelism with multiple GPUs</span></h3>
<p>Data Parallelism, in this context, refers to methods to perform training over multiple replicas of a model in parallel, where each replica receives a different chunk of training data at each iteration. Gradients are then aggregated at the end of an iteration and the parameters of all replicas are updated in a synchronous or asynchronous fashion, depending on the method. Using this approach may provide a significant speed-up by iterating through all examples in a large dataset approximately N times faster, where N is the number of model replicas. An important caveat of this approach, is that in order to get a trained model that is equivalent to the same model trained without Data Parallelism, the user must scale either the learning rate or the desired batch size in function of the number of replicas. See <a rel="nofollow" class="external text" href="https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/13">this discussion</a> for more information. In the multiple-GPU case, each GPU hosts a replica of your model. Consequently, the model must be small enough to fit inside the memory of a single GPU. Refer to the <a class="mw-selflink-fragment" href="#Model_parallelism_with_multiple_GPUs">Model Parallelism</a> section for options to train very large models that do not fit inside a single GPU.
</p><p>There are several ways to perform Data Parallelism using PyTorch. This section features tutorials on three of them: using the <b>DistributedDataParallel</b> class, using the <b>PyTorch Lightning</b> package and using the <b>Horovod</b> package.
</p>
<h4><span class="mw-headline" id="Using_DistributedDataParallel">Using DistributedDataParallel</span></h4>
<p>The <b>DistributedDataParallel</b> class is the way <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel">recommended by PyTorch maintainers</a> to use multiple GPUs, whether they are all on a single node, or distributed across multiple nodes.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2+++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Asrun+-N+%24SLURM_NNODES+-n+%24SLURM_NNODES+bash+%3C%3C+EOF%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0AEOF%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0Aexport+MASTER_ADDR%3D%24%28hostname%29+%23Store+the+master+node%E2%80%99s+IP+address+in+the+MASTER_ADDR+environment+variable.%0A%0Aecho+%22r%24SLURM_NODEID+master%3A+%24MASTER_ADDR%22%0Aecho+%22r%24SLURM_NODEID+Launching+python+script%22%0A%0A%23+The+%24%28%28SLURM_NTASKS_PER_NODE+%2A+SLURM_JOB_NUM_NODES%29%29+variable+tells+the+script+how+many+processes+are+available+for+this+execution.+%E2%80%9Csrun%E2%80%9D+executes+the+script+%3Ctasks-per-node+%2A+nodes%3E+times%0A%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0A%0Asrun+python+pytorch-ddp-test.py+--init_method+tcp%3A%2F%2F%24MASTER_ADDR%3A3456+--world_size+%24%28%28SLURM_NTASKS_PER_NODE+%2A+SLURM_JOB_NUM_NODES%29%29++--batch_size+256" />
<input type="hidden" name="filename" value="pytorch-ddp-test.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resources”.</span>
<span class="c1">#SBATCH --tasks-per-node=2   # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
srun<span class="w"> </span>-N<span class="w"> </span><span class="nv">$SLURM_NNODES</span><span class="w"> </span>-n<span class="w"> </span><span class="nv">$SLURM_NNODES</span><span class="w"> </span>bash<span class="w"> </span><span class="s">&lt;&lt; EOF</span>
<span class="s">virtualenv --no-download $SLURM_TMPDIR/env</span>
<span class="s">source $SLURM_TMPDIR/env/bin/activate</span>
<span class="s">pip install torch torchvision --no-index</span>
<span class="s">EOF</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span><span class="w"> </span><span class="c1">#Store the master node’s IP address in the MASTER_ADDR environment variable.</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;r</span><span class="nv">$SLURM_NODEID</span><span class="s2"> master: </span><span class="nv">$MASTER_ADDR</span><span class="s2">&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;r</span><span class="nv">$SLURM_NODEID</span><span class="s2"> Launching python script&quot;</span>

<span class="c1"># The $((SLURM_NTASKS_PER_NODE * SLURM_JOB_NUM_NODES)) variable tells the script how many processes are available for this execution. “srun” executes the script &lt;tasks-per-node * nodes&gt; times</span>

<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate

srun<span class="w"> </span>python<span class="w"> </span>pytorch-ddp-test.py<span class="w"> </span>--init_method<span class="w"> </span>tcp://<span class="nv">$MASTER_ADDR</span>:3456<span class="w"> </span>--world_size<span class="w"> </span><span class="k">$((</span><span class="nv">SLURM_NTASKS_PER_NODE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">SLURM_JOB_NUM_NODES</span><span class="k">))</span><span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p>The Python script <code>pytorch-ddp-test.py</code> has the form
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0Aimport+time%0Aimport+datetime%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0Aimport+torch.backends.cudnn+as+cudnn%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+distributed+data+parallel+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D4%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0Aparser.add_argument%28%27--init_method%27%2C+default%3D%27tcp%3A%2F%2F127.0.0.1%3A3456%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--dist-backend%27%2C+default%3D%27nccl%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--world_size%27%2C+default%3D1%2C+type%3Dint%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--distributed%27%2C+action%3D%27store_true%27%2C+help%3D%27%27%29%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++ngpus_per_node+%3D+torch.cuda.device_count%28%29%0A%0A++++%22%22%22+This+next+line+is+the+key+to+getting+DistributedDataParallel+working+on+SLURM%3A%0A%09%09SLURM_NODEID+is+0+or+1+in+this+example%2C+SLURM_LOCALID+is+the+id+of+the+%0A+%09%09current+process+inside+a+node+and+is+also+0+or+1+in+this+example.%22%22%22%0A%0A++++local_rank+%3D+int%28os.environ.get%28%22SLURM_LOCALID%22%29%29+%0A++++rank+%3D+int%28os.environ.get%28%22SLURM_NODEID%22%29%29%2Angpus_per_node+%2B+local_rank%0A%0A++++current_device+%3D+local_rank%0A%0A++++torch.cuda.set_device%28current_device%29%0A%0A++++%22%22%22+this+block+initializes+a+process+group+and+initiate+communications%0A%09%09between+all+processes+running+on+all+nodes+%22%22%22%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Initializing+Process+Group...%27.format%28rank%29%29%0A++++%23init+the+process+group%0A++++dist.init_process_group%28backend%3Dargs.dist_backend%2C+init_method%3Dargs.init_method%2C+world_size%3Dargs.world_size%2C+rank%3Drank%29%0A++++print%28%22process+group+ready%21%22%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Making+model..%27.format%28rank%29%29%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++net.cuda%28%29%0A++++net+%3D+torch.nn.parallel.DistributedDataParallel%28net%2C+device_ids%3D%5Bcurrent_device%5D%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Preparing+data..%27.format%28rank%29%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%2C+momentum%3D0.9%2C+weight_decay%3D1e-4%29%0A%0A++++for+epoch+in+range%28args.max_epochs%29%3A%0A%0A++++++++train_sampler.set_epoch%28epoch%29%0A%0A++++++++train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+rank%29%0A%0Adef+train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+train_rank%29%3A%0A%0A++++train_loss+%3D+0%0A++++correct+%3D+0%0A++++total+%3D+0%0A%0A++++epoch_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++inputs+%3D+inputs.cuda%28%29%0A+++++++targets+%3D+targets.cuda%28%29%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++train_loss+%2B%3D+loss.item%28%29%0A+++++++_%2C+predicted+%3D+outputs.max%281%29%0A+++++++total+%2B%3D+targets.size%280%29%0A+++++++correct+%2B%3D+predicted.eq%28targets%29.sum%28%29.item%28%29%0A+++++++acc+%3D+100+%2A+correct+%2F+total%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++elapse_time+%3D+time.time%28%29+-+epoch_start%0A+++++++elapse_time+%3D+datetime.timedelta%28seconds%3Delapse_time%29%0A+++++++print%28%22From+Rank%3A+%7B%7D%2C+Training+time+%7B%7D%22.format%28train_rank%2C+elapse_time%29%29%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-ddp-test.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="k">as</span> <span class="nn">cudnn</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, distributed data parallel test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--init_method&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;tcp://127.0.0.1:3456&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--dist-backend&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--world_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--distributed&#39;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">ngpus_per_node</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; This next line is the key to getting DistributedDataParallel working on SLURM:</span>
<span class="sd">		SLURM_NODEID is 0 or 1 in this example, SLURM_LOCALID is the id of the </span>
<span class="sd"> 		current process inside a node and is also 0 or 1 in this example.&quot;&quot;&quot;</span>

    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_LOCALID&quot;</span><span class="p">))</span> 
    <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_NODEID&quot;</span><span class="p">))</span><span class="o">*</span><span class="n">ngpus_per_node</span> <span class="o">+</span> <span class="n">local_rank</span>

    <span class="n">current_device</span> <span class="o">=</span> <span class="n">local_rank</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; this block initializes a process group and initiate communications</span>
<span class="sd">		between all processes running on all nodes &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Initializing Process Group...&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
    <span class="c1">#init the process group</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;process group ready!&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Making model..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">net</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">current_device</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Preparing data..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>

        <span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_rank</span><span class="p">):</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">epoch_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
       <span class="n">total</span> <span class="o">+=</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
       <span class="n">correct</span> <span class="o">+=</span> <span class="n">predicted</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start</span>
       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">elapse_time</span><span class="p">)</span>
       <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From Rank: </span><span class="si">{}</span><span class="s2">, Training time </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_rank</span><span class="p">,</span> <span class="n">elapse_time</span><span class="p">))</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="Using_PyTorch_Lightning">Using PyTorch Lightning</span></h4>
<p><b>PyTorch Lightning</b> is a Python package that provides interfaces to PyTorch to make many common, but otherwise code-heavy tasks, more straightforward. This includes training on multiple GPUs. The following is the same tutorial from the section above, but using PyTorch Lightning instead of explicitly leveraging the DistributedDataParallel class:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test-pl.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+pytorch-ddp-test-pl.py++--batch_size+256" />
<input type="hidden" name="filename" value="pytorch-ddp-test-pl.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resources”.</span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>pytorch-ddp-test-pl.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test-pl.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+datetime%0A%0Aimport+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+pytorch-lightning+parallel+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D4%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+torch.optim.Adam%28self.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPUs+per+node.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3D%27ddp%27%2C+max_epochs+%3D+args.max_epochs%2C+enable_progress_bar%3DFalse%29+%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-ddp-test-pl.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">datetime</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, pytorch-lightning parallel test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPUs per node.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;ddp&#39;</span><span class="p">,</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">,</span> <span class="n">enable_progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="Using_Horovod">Using Horovod</span></h4>
<p><a rel="nofollow" class="external text" href="https://horovod.readthedocs.io/en/latest/summary_include.html">Horovod</a> is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. Its API allows you to retain the level of control over your training code that <code>DistributedDataParallel</code> provides, but makes writing your scripts easier by abstracting away the need to directly configure process groups and dealing with the cluster scheduler's environment variables. It also features distributed optimizers, which may increase performance in some cases. The following is the same example as above, re-implemented using Horovod:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch_horovod.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2+++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%0A%23SBATCH+--tasks-per-node%3D2+++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+horovod+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0Asrun+python+pytorch_horovod.py++--batch_size+256" />
<input type="hidden" name="filename" value="pytorch_horovod.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1            </span>
<span class="c1">#SBATCH --gres=gpu:2         # Request 2 GPU &quot;generic resources”.</span>

<span class="c1">#SBATCH --tasks-per-node=2   # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>

<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>horovod<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

srun<span class="w"> </span>python<span class="w"> </span>pytorch_horovod.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch_horovod.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0Aimport+time%0Aimport+datetime%0Aimport+numpy+as+np%0Aimport+horovod.torch+as+hvd%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+horovod+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++hvd.init%28%29%0A%0A++++print%28%22Starting...%22%29%0A%0A++++local_rank+%3D+hvd.local_rank%28%29%0A++++global_rank+%3D+hvd.rank%28%29%0A%0A++++torch.cuda.set_device%28local_rank%29%0A%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++net.cuda%28%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Preparing+data..%27.format%28global_rank%29%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%2C+num_replicas%3Dhvd.size%28%29%2Crank%3Dglobal_rank%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%2C+momentum%3D0.9%2C+weight_decay%3D1e-4%29%0A%0A++++optimizer+%3D+hvd.DistributedOptimizer%28optimizer%2C+named_parameters%3Dnet.named_parameters%28%29%29%0A%0A++++hvd.broadcast_parameters%28net.state_dict%28%29%2C+root_rank%3D0%29%0A%0A++++for+epoch+in+range%28args.max_epochs%29%3A%0A%0A++++++++train_sampler.set_epoch%28epoch%29%0A%0A++++++++train%28args%2Cepoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+global_rank%29%0A%0A%0Adef+train%28args%2Cepoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+train_rank%29%3A%0A%0A++++train_loss+%3D+0%0A++++correct+%3D+0%0A++++total+%3D+0%0A%0A++++epoch_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++inputs+%3D+inputs.cuda%28%29%0A+++++++targets+%3D+targets.cuda%28%29%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++train_loss+%2B%3D+loss.item%28%29%0A+++++++_%2C+predicted+%3D+outputs.max%281%29%0A+++++++total+%2B%3D+targets.size%280%29%0A+++++++correct+%2B%3D+predicted.eq%28targets%29.sum%28%29.item%28%29%0A+++++++acc+%3D+100+%2A+correct+%2F+total%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++elapse_time+%3D+time.time%28%29+-+epoch_start%0A+++++++elapse_time+%3D+datetime.timedelta%28seconds%3Delapse_time%29%0A+++++++print%28%22From+Rank%3A+%7B%7D%2C+Training+time+%7B%7D%22.format%28train_rank%2C+elapse_time%29%29%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch_horovod.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">horovod.torch</span> <span class="k">as</span> <span class="nn">hvd</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>


<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, horovod test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">local_rank</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>


    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">net</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Preparing data..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">global_rank</span><span class="p">))</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span><span class="n">rank</span><span class="o">=</span><span class="n">global_rank</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>


    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">named_parameters</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>

    <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_parameters</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>

        <span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">global_rank</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_rank</span><span class="p">):</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">epoch_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
       <span class="n">total</span> <span class="o">+=</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
       <span class="n">correct</span> <span class="o">+=</span> <span class="n">predicted</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start</span>
       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">elapse_time</span><span class="p">)</span>
       <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From Rank: </span><span class="si">{}</span><span class="s2">, Training time </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_rank</span><span class="p">,</span> <span class="n">elapse_time</span><span class="p">))</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span class="mw-headline" id="Model_parallelism_with_multiple_GPUs">Model parallelism with multiple GPUs</span></h3>
<p>In cases where a model is too large to fit inside a <a class="mw-selflink-fragment" href="#PyTorch_with_a_single_GPU">single GPU</a>, you can split it into multiple parts and load each one onto a separate GPU. In the example below, we revisit the code example from previous sections to illustrate how this works: we will split a Convolutional Neural Network in two parts - the convolutional/pooling layers and the densely connected feedforward layers. This job will request 2 GPUs and each of the two parts of the model will be loaded on its own GPU. We will also add code to perform <a rel="nofollow" class="external text" href="https://pytorch.org/docs/stable/pipeline.html?highlight=pipeline">pipeline parallelism</a> and minimize as much as possible the amount of time the second GPU sits idle waiting for the outputs of the first. To do this, we will create a separate <code>nn.Module</code> for each part of our model, create a sequence of modules by wrapping our model parts with <code>nn.Sequential</code>, then use <code>torch.distributed.pipeline.sync.Pipe</code> to break each input batch into chunks and feed them in parallel to all parts of our model.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-modelpar-pipelined-rpc.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--gres%3Dgpu%3A2+%23+request+2+GPUs%0A%23SBATCH+--tasks-per-node%3D1+%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A10%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0A%23+This+is+needed+to+initialize+pytorch%27s+RPC+module%2C+required+for+the+Pipe+class+which+we%27ll+use+for+Pipeline+Parallelism%0Aexport+MASTER_ADDR%3D%24%28hostname%29%0Aexport+MASTER_PORT%3D34567%0A+%0Aecho+%22starting+training...%22%0Atime+python+pytorch-modelpar-pipelined-rpc.py+--batch_size%3D512+--num_workers%3D0" />
<input type="hidden" name="filename" value="pytorch-modelpar-pipelined-rpc.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --gres=gpu:2 # request 2 GPUs</span>
<span class="c1">#SBATCH --tasks-per-node=1 </span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:10:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="c1"># This is needed to initialize pytorch&#39;s RPC module, required for the Pipe class which we&#39;ll use for Pipeline Parallelism</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">34567</span>
<span class="w"> </span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>
<span class="nb">time</span><span class="w"> </span>python<span class="w"> </span>pytorch-modelpar-pipelined-rpc.py<span class="w"> </span>--batch_size<span class="o">=</span><span class="m">512</span><span class="w"> </span>--num_workers<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-modelpar-pipelined-rpc.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+time%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.optim+as+optim%0Afrom+torch.distributed.pipeline.sync+import+Pipe%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+single+node+model+parallelism+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++%23+Convolutional+%2B+pooling+part+of+the+model%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++torch.distributed.rpc.init_rpc%28%27worker%27%2C+rank%3D0%2C+world_size%3D1%29+%23+initializing+RPC+is+required+by+Pipe+we+use+below%0A%0A++++part1+%3D+ConvPart%28%29.to%28%27cuda%3A0%27%29+%23+Load+part1+on+the+first+GPU%0A++++part2+%3D+MLPPart%28%29.to%28%27cuda%3A1%27%29+%23+Load+part2+on+the+second+GPU%0A%0A++++net+%3D+nn.Sequential%28part1%2Cpart2%29+%23+Pipe+requires+all+modules+be+wrapped+with+nn.Sequential%28%29%0A%0A++++net+%3D+Pipe%28net%2C+chunks%3D32%29+%23+Wrap+with+Pipe+to+perform+Pipeline+Parallelism%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.to%28%27cuda%3A1%27%29+%23+Load+the+loss+function+on+the+last+GPU%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++inputs+%3D+inputs.to%28%27cuda%3A0%27%29%0A+++++++targets+%3D+targets.to%28%27cuda%3A1%27%29%0A%0A+++++++%23+Models+wrapped+with+Pipe%28%29+return+a+RRef+object.+Since+the+example+is+single+node%2C+all+values+are+local+to+the+node+and+we+can+grab+them%0A+++++++outputs+%3D+net%28inputs%29.local_value%28%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A+++++++print%28f%22Loss%3A+%7Bloss.item%28%29%7D%22%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-modelpar-pipelined-rpc.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributed.pipeline.sync</span> <span class="kn">import</span> <span class="n">Pipe</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, single node model parallelism test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Convolutional + pooling part of the model</span>
    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">rpc</span><span class="o">.</span><span class="n">init_rpc</span><span class="p">(</span><span class="s1">&#39;worker&#39;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># initializing RPC is required by Pipe we use below</span>

    <span class="n">part1</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span> <span class="c1"># Load part1 on the first GPU</span>
    <span class="n">part2</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span> <span class="c1"># Load part2 on the second GPU</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">part1</span><span class="p">,</span><span class="n">part2</span><span class="p">)</span> <span class="c1"># Pipe requires all modules be wrapped with nn.Sequential()</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Pipe</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span> <span class="c1"># Wrap with Pipe to perform Pipeline Parallelism</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span> <span class="c1"># Load the loss function on the last GPU</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>

       <span class="c1"># Models wrapped with Pipe() return a RRef object. Since the example is single node, all values are local to the node and we can grab them</span>
       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">local_value</span><span class="p">()</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
       <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span class="mw-headline" id="Combining_model_and_data_parallelism">Combining model and data parallelism</span></h3>
<p>In cases where a model is too large to fit inside a Single GPU and, additionally, the goal is to train such a model using a very large training set, combining model parallelism with data parallelism becomes a viable option to achieve high performance. The idea is straightforward: you will split a large model into smaller parts, give each part its own GPU, perform pipeline parallelism on the inputs, then, additionally, you will create replicas of this whole process, which will be trained in parallel over separate subsets of the training set. As in the <a class="mw-selflink-fragment" href="#Data_Parallelism_with_Multiple_GPUs">example from the previous section</a>, gradients are computed independently within each replica, then an aggregation of these gradients is used to update all replicas synchronously or asynchronously, depending on the method used. The main difference here is that each model replica lives in more than one GPU. 
</p>
<h4><span class="mw-headline" id="Using_Torch_RPC_and_DDP">Using Torch RPC and DDP</span></h4>
<p>The following example is a reprise of the ones from previous sections. Here we combine Torch RPC and DistributedDataParallel to split a model in two parts, then train four replicas of the model distributed over two nodes in parallel. In other words, we will have 2 model replicas spanning 2 GPUs on each node. An <b>important caveat</b> of using Torch RPC is that currently it only supports splitting models inside a single node. For very large models that do not fit inside the combined memory space of all GPUs of a single compute node, see the next section on <a class="mw-selflink-fragment" href="#DeepSpeed">DeepSpeed</a>.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-model-data-par.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+2%0A%23SBATCH+--gres%3Dgpu%3A4+%23+Request+4+GPUs+per+node%0A%23SBATCH+--tasks-per-node%3D2+%23+Request+one+task+per+MODEL+per+node%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D16G++++++%0A%23SBATCH+--time%3D0%3A10%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+StdEnv%2F2020+gcc%2F11.3.0%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%2C+python%2F3.10.2+works+with+this+demo%0Amodule+load+cuda%2F11.8.0%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0Aexport+MAIN_NODE%3D%24%28hostname%29%0A%0Aecho+%22starting+training...%22%0A%0Asrun+python+pytorch-model-data-par.py+--init_method+tcp%3A%2F%2F%24MAIN_NODE%3A3456+--world_size+%24SLURM_NTASKS++--batch_size+512" />
<input type="hidden" name="filename" value="pytorch-model-data-par.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 2</span>
<span class="c1">#SBATCH --gres=gpu:4 # Request 4 GPUs per node</span>
<span class="c1">#SBATCH --tasks-per-node=2 # Request one task per MODEL per node</span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=16G      </span>
<span class="c1">#SBATCH --time=0:10:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>StdEnv/2020<span class="w"> </span>gcc/11.3.0
module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application, python/3.10.2 works with this demo</span>
module<span class="w"> </span>load<span class="w"> </span>cuda/11.8.0
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">MAIN_NODE</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>

srun<span class="w"> </span>python<span class="w"> </span>pytorch-model-data-par.py<span class="w"> </span>--init_method<span class="w"> </span>tcp://<span class="nv">$MAIN_NODE</span>:3456<span class="w"> </span>--world_size<span class="w"> </span><span class="nv">$SLURM_NTASKS</span><span class="w">  </span>--batch_size<span class="w"> </span><span class="m">512</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-model-data-par.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+time%0Aimport+os%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.optim+as+optim%0Afrom+torch.distributed.pipeline.sync+import+Pipe%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+distributed+data+%26+model+parallel+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D4%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0Aparser.add_argument%28%27--init_method%27%2C+default%3D%27tcp%3A%2F%2F127.0.0.1%3A3456%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--dist-backend%27%2C+default%3D%27mpi%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--world_size%27%2C+default%3D1%2C+type%3Dint%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--distributed%27%2C+action%3D%27store_true%27%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++%23+Convolutional+%2B+pooling+part+of+the+model%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++ngpus_per_node+%3D+torch.cuda.device_count%28%29%0A++++local_rank+%3D+int%28os.environ.get%28%22SLURM_LOCALID%22%29%29%0A++++rank+%3D+int%28os.environ.get%28%22SLURM_NODEID%22%29%29%2A%28ngpus_per_node%2F%2F2%29+%2B+local_rank++%23+Divide+ngpus_per_node+by+the+number+of+model+parts%0A%0A++++os.environ%5B%27MASTER_ADDR%27%5D+%3D+%27127.0.0.1%27+%23+Each+model+replica+will+run+its+own+RPC+server+to+run+pipeline+parallelism%0A++++os.environ%5B%27MASTER_PORT%27%5D+%3D+str%2834567+%2B+local_rank%29+%23+Make+sure+each+RPC+server+starts+on+a+different+port%0A++++torch.distributed.rpc.init_rpc%28%27worker%27%2C+rank%3D0%2C+world_size%3D1%29+%23+Different+replicas+won%27t+communicate+through+RPC%2C+but+through+DDP%0A%0A++++dist.init_process_group%28backend%3Dargs.dist_backend%2C+init_method%3Dargs.init_method%2C+world_size%3Dargs.world_size%2C+rank%3Drank%29+%23+Initialize+Data+Parallelism+communications%0A%0A++++part1+%3D+ConvPart%28%29.cuda%28local_rank%29+%23+First+part+of+the+model+goes+on+the+first+GPU+of+each+process%0A++++part2+%3D+MLPPart%28%29.cuda%28local_rank+%2B+1%29+%23+Second+part+goes+on+the+second+GPU+of+each+process%0A%0A++++net+%3D+nn.Sequential%28part1%2Cpart2%29%0A%0A++++net+%3D+Pipe%28net%2C+chunks%3D32%2C+checkpoint%3D%22never%22%29%0A%0A++++net+%3D+torch.nn.parallel.DistributedDataParallel%28net%29%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28local_rank+%2B+1%29+%23+Loss+function+goes+on+the+second+GPU+of+each+process%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A%0A++++for+epoch+in+range%28args.max_epochs%29%3A%0A%0A++++++++train_sampler.set_epoch%28epoch%29%0A%0A++++++++train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+rank%2C+local_rank%29%0A%0Adef+train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+train_rank%2C+model_rank%29%3A%0A%0A++++train_loss+%3D+0%0A++++correct+%3D+0%0A++++total+%3D+0%0A%0A++++epoch_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A++++++++start+%3D+time.time%28%29%0A%0A++++++++inputs+%3D+inputs.cuda%28model_rank%29%0A++++++++targets+%3D+targets.cuda%28model_rank+%2B+1%29%0A%0A++++++++outputs+%3D+net%28inputs%29.local_value%28%29%0A++++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A++++++++optimizer.zero_grad%28%29%0A++++++++loss.backward%28%29%0A++++++++optimizer.step%28%29%0A++++++++print%28f%22From+Rank+%7Btrain_rank%7D+-+Loss%3A+%7Bloss.item%28%29%7D%22%29%0A%0A++++++++batch_time+%3D+time.time%28%29+-+start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-model-data-par.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributed.pipeline.sync</span> <span class="kn">import</span> <span class="n">Pipe</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, distributed data &amp; model parallel test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--init_method&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;tcp://127.0.0.1:3456&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--dist-backend&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;mpi&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--world_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--distributed&#39;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Convolutional + pooling part of the model</span>
    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="n">ngpus_per_node</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_LOCALID&quot;</span><span class="p">))</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_NODEID&quot;</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">ngpus_per_node</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">local_rank</span>  <span class="c1"># Divide ngpus_per_node by the number of model parts</span>

    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span> <span class="c1"># Each model replica will run its own RPC server to run pipeline parallelism</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="mi">34567</span> <span class="o">+</span> <span class="n">local_rank</span><span class="p">)</span> <span class="c1"># Make sure each RPC server starts on a different port</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">rpc</span><span class="o">.</span><span class="n">init_rpc</span><span class="p">(</span><span class="s1">&#39;worker&#39;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Different replicas won&#39;t communicate through RPC, but through DDP</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span> <span class="c1"># Initialize Data Parallelism communications</span>

    <span class="n">part1</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span> <span class="c1"># First part of the model goes on the first GPU of each process</span>
    <span class="n">part2</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">local_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Second part goes on the second GPU of each process</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">part1</span><span class="p">,</span><span class="n">part2</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Pipe</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;never&quot;</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">local_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Loss function goes on the second GPU of each process</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>


    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>

        <span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">local_rank</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_rank</span><span class="p">,</span> <span class="n">model_rank</span><span class="p">):</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">epoch_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model_rank</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">local_value</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;From Rank </span><span class="si">{</span><span class="n">train_rank</span><span class="si">}</span><span class="s2"> - Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span class="mw-headline" id="DeepSpeed">DeepSpeed</span></h3>
<p><a rel="nofollow" class="external text" href="https://www.deepspeed.ai">DeepSpeed</a> is a deep learning training optimization library, providing the means to train massive billion parameter models at scale. Fully compatible with PyTorch, DeepSpeed features implementations of novel memory-efficient distributed training methods, based on the <a rel="nofollow" class="external text" href="https://arxiv.org/abs/1910.02054">Zero Redundancy Optimizer (ZeRO)</a> concept. Through the use of ZeRO, DeepSpeed enables distributed storage and computing of different elements of a training task - such as optimizer states, model weights, model gradients and model activations - across multiple devices, including GPU, CPU, local hard disk, and/or combinations of these devices. This "pooling" of resources, notably for storage, allows models with massive amounts of parameters to be trained efficiently, across multiple nodes, without explicitly handling Model, Pipeline or Data Parallelism in your code. The examples below show how to take advantage of DeepSpeed and its implementations of ZeRO variants through its PyTorch Lightning interface for ease of use.
</p>
<h4><span class="mw-headline" id="ZeRO_on_GPU">ZeRO on GPU</span></h4>
<p>In the following example, we use ZeRO Stage 3 to train a model using a "pool" of 4 GPUs. Stage 3 means all three of: optimizer states; model parameters; and model gradients will be split (sharded) between all 4 GPUs. This is more memory-efficient than pure Data Parallelism, where we would have a full replica of the model loaded on each GPU. Using DeepSpeed's optimizer <code>FusedAdam</code> instead of a native PyTorch one, performance is comparable with pure Data Parallelism. DeepSpeed's optimizers are JIT compiled at run-time and you must load the module <code>cuda/&lt;version&gt;</code> where <b>&lt;version&gt;</b> must match the version used to build the PyTorch install you are using.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D32G++++++%0A%23SBATCH+--time%3D0-00%3A20%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+cuda+%23+CUDA+must+be+loaded+if+using+a+DeepSpeed+optimizer%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+deepspeed+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+deepspeed-stage3.py++--batch_size+256" />
<input type="hidden" name="filename" value="deepspeed-stage3.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resources”.</span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=32G      </span>
<span class="c1">#SBATCH --time=0-00:20</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span>cuda<span class="w"> </span><span class="c1"># CUDA must be loaded if using a DeepSpeed optimizer</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>deepspeed<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>deepspeed-stage3.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Afrom+deepspeed.ops.adam+import+FusedAdam%0Afrom+pytorch_lightning.strategies+import+DeepSpeedStrategy%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models+deep+seed+stage+3+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D2%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv_part+%3D+ConvPart%28%29%0A++++++++++self.mlp_part+%3D+MLPPart%28%29%0A%0A+++++++def+configure_sharded_model%28self%29%3A%0A%0A++++++++++self.block+%3D+nn.Sequential%28self.conv_part%2C+self.mlp_part%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.block%28x%29%0A%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+FusedAdam%28self.parameters%28%29%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPU.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3D%22deepspeed_stage_3%22%2C+max_epochs+%3D+args.max_epochs%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="deepspeed-stage3.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">FusedAdam</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models deep seed stage 3 test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">FusedAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPU.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_3&quot;</span><span class="p">,</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="ZeRO_with_offload_to_CPU">ZeRO with offload to CPU</span></h4>
<p>In this example, we will again use ZeRO stage 3, but this time we enable offloading model parameters and optimizers states to the CPU. This means that the compute node's memory will be available to store these tensors while they are not required by any GPU computations, and additionally, optimizer steps will be computed on the CPU. For practical purposes, you can think of this as though your GPUs were gaining an extra 32GB of memory. This takes even more pressure off from GPU memory and would allow you to increase your batch size, for example, or increase the size of the model. Using DeepSpeed's optimizer <code>DeepSpeedCPUAdam</code> instead of a native PyTorch one, performance remains at par with pure Data Parallelism. DeepSpeed's optimizers are JIT compiled at run-time and you must load the module <code>cuda/&lt;version&gt;</code> where <b>&lt;version&gt;</b> must match the version used to build the PyTorch install you are using.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-cpu.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D32G++++++%0A%23SBATCH+--time%3D0-00%3A20%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+cuda+%23+CUDA+must+be+loaded+if+using+ZeRO+offloading+to+CPU+or+NVMe.+Version+must+be+the+same+used+to+compile+PyTorch.+%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+deepspeed+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+deepspeed-stage3-offload-cpu.py++--batch_size+256" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-cpu.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resources”.</span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=32G      </span>
<span class="c1">#SBATCH --time=0-00:20</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span>cuda<span class="w"> </span><span class="c1"># CUDA must be loaded if using ZeRO offloading to CPU or NVMe. Version must be the same used to compile PyTorch. </span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>deepspeed<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>deepspeed-stage3-offload-cpu.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-cpu.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Afrom+deepspeed.ops.adam+import+DeepSpeedCPUAdam%0Afrom+pytorch_lightning.strategies+import+DeepSpeedStrategy%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+deepspeed+offload+to+cpu+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D2%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv_part+%3D+ConvPart%28%29%0A++++++++++self.mlp_part+%3D+MLPPart%28%29%0A%0A+++++++def+configure_sharded_model%28self%29%3A%0A%0A++++++++++self.block+%3D+nn.Sequential%28self.conv_part%2C+self.mlp_part%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.block%28x%29%0A%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+DeepSpeedCPUAdam%28self.parameters%28%29%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPU.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3DDeepSpeedStrategy%28%0A++++++++stage%3D3%2C%0A++++++++offload_optimizer%3DTrue%2C%0A++++++++offload_parameters%3DTrue%2C%0A++++++++%29%2C+max_epochs+%3D+args.max_epochs%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-cpu.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">DeepSpeedCPUAdam</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, deepspeed offload to cpu test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">DeepSpeedCPUAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPU.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span>
        <span class="n">stage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">offload_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="ZeRO_with_offload_to_NVMe">ZeRO with offload to NVMe</span></h4>
<p>In this example, we use ZeRO stage 3 yet again, but this time we enable offloading model parameters and optimizers states to the local disk. This means that the compute node's local disk storage will be available to store these tensors while they are not required by any GPU computations. As before, optimizer steps will be computed on the CPU. Again, for practical purposes, you can think of this as extending GPU memory by however much storage is available on the local disk, though this time performance will significantly degrade. This approach works best (i.e., performance degradation is least noticeable) on NVMe-enabled drives, which have higher throughput and faster response times, but it can be used with any type of storage.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-nvme.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.+%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D32G++++++%0A%23SBATCH+--time%3D0-00%3A20%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+cuda+%23+CUDA+must+be+loaded+if+using+ZeRO+offloading+to+CPU+or+NVMe.+Version+must+be+the+same+used+to+compile+PyTorch.+%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+deepspeed+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+deepspeed-stage3-offload-nvme.py++--batch_size+256" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-nvme.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resources”. </span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=32G      </span>
<span class="c1">#SBATCH --time=0-00:20</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span>cuda<span class="w"> </span><span class="c1"># CUDA must be loaded if using ZeRO offloading to CPU or NVMe. Version must be the same used to compile PyTorch. </span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>deepspeed<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>deepspeed-stage3-offload-nvme.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-nvme.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0A%0Aimport+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Afrom+deepspeed.ops.adam+import+DeepSpeedCPUAdam%0Afrom+pytorch_lightning.strategies+import+DeepSpeedStrategy%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+deepspeed+offload+to+nvme+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D2%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv_part+%3D+ConvPart%28%29%0A++++++++++self.mlp_part+%3D+MLPPart%28%29%0A%0A+++++++def+configure_sharded_model%28self%29%3A%0A%0A++++++++++self.block+%3D+nn.Sequential%28self.conv_part%2C+self.mlp_part%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.block%28x%29%0A%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+DeepSpeedCPUAdam%28self.parameters%28%29%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPU.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++local_scratch+%3D+os.environ%5B%27SLURM_TMPDIR%27%5D+%23+Get+path+where+local+storage+is+mounted%0A%0A++++print%28f%27Offloading+to%3A+%7Blocal_scratch%7D%27%29%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3DDeepSpeedStrategy%28%0A++++++++stage%3D3%2C%0A++++++++offload_optimizer%3DTrue%2C%0A++++++++offload_parameters%3DTrue%2C%0A++++++++remote_device%3D%22nvme%22%2C%0A++++++++offload_params_device%3D%22nvme%22%2C%0A++++++++offload_optimizer_device%3D%22nvme%22%2C%0A++++++++nvme_path%3D%22local_scratch%22%2C%0A++++++++%29%2C+max_epochs+%3D+args.max_epochs%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-nvme.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">DeepSpeedCPUAdam</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, deepspeed offload to nvme test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">DeepSpeedCPUAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPU.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">local_scratch</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_TMPDIR&#39;</span><span class="p">]</span> <span class="c1"># Get path where local storage is mounted</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Offloading to: </span><span class="si">{</span><span class="n">local_scratch</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span>
        <span class="n">stage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">offload_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">remote_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">offload_params_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">offload_optimizer_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">nvme_path</span><span class="o">=</span><span class="s2">&quot;local_scratch&quot;</span><span class="p">,</span>
        <span class="p">),</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h1><span class="mw-headline" id="Creating_model_checkpoints">Creating model checkpoints</span></h1>
<p>Whether or not you expect your code to run for long time periods, it is a good habit to create Checkpoints during training. A checkpoint is a snapshot of your model at a given point during the training process (after a certain number of iterations or after a number of epochs) that is saved to disk and can be loaded at a later time. It is a handy way of breaking up jobs that are expected to run for a very long time, into multiple shorter jobs that may get allocated on the cluster more quickly. It is also a good way of avoiding losing progress in case of unexpected errors in your code or node failures.
</p>
<h2><span class="mw-headline" id="With_PyTorch_Lightning">With PyTorch Lightning</span></h2>
<p>To create a checkpoint when training with <code>pytorch-lightning</code>, we recommend using the callbacks parameter of the <code>Trainer()</code> class. The following example shows how to instruct PyTorch to create a checkpoint at the end of every training epoch.  Make sure the path where you want to create the checkpoint exists.
</p>
<pre>callbacks = [pl.callbacks.ModelCheckpoint(dirpath="./ckpt",every_n_epochs=1)]
trainer = pl.Trainer(callbacks=callbacks) 
trainer.fit(model)
</pre>
<p>This code snippet will also load a checkpoint from <code>./ckpt</code>, if there is one, and continue training from that point. For more information, please refer to the <a rel="nofollow" class="external text" href="https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.model_checkpoint.html">official PyTorch Lightning documentation</a>.
</p>
<h2><span class="mw-headline" id="With_custom_training_loops">With custom training loops</span></h2>
<p>Please refer to the <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">official PyTorch documentation</a> for examples on how to create and load checkpoints inside of a training loop.
</p>
<h2><span class="mw-headline" id="During_distributed_training">During distributed training</span></h2>
<p>Checkpointing can also be done while running a distributed training program. With PyTorch Lightning, no extra code is required other than using the checkpoint callback as described above. If you are using DistributedDataParallel or Horovod however, checkpointing should be done only by one process (one of the ranks) of your program, since all ranks will have the same state at the end of each iteration. The following example uses the first process (rank 0) to create a checkpoint:
</p>
<pre>if global_rank == 0:
       torch.save(ddp_model.state_dict(), "./checkpoint_path")
</pre>
<p>You must be careful when loading a checkpoint created in this manner. If a process tries to load a checkpoint that has not yet been saved by another, you may see errors or get wrong results. To avoid this, you can add a barrier to your code to make sure the process that will create the checkpoint finishes writing it to disk before other processes attempt to load it. Also note that <code>torch.load</code> will attempt to load tensors to the GPU that saved them originally (<code>cuda:0</code> in this case) by default. To avoid issues, pass <code>map_location</code> to <code>torch.load</code> to load tensors on the correct GPU for each rank.
</p>
<pre>torch.distributed.barrier()
map_location = f"cuda:{local_rank}"  
ddp_model.load_state_dict(
torch.load("./checkpoint_path", map_location=map_location))
</pre>
<p><br />
</p>
<h1><span class="mw-headline" id="Troubleshooting">Troubleshooting</span></h1>
<h2><span class="mw-headline" id="Memory_leak">Memory leak</span></h2>
<p>On AVX512 hardware (Béluga, Skylake or V100 nodes), older versions of Pytorch (less than v1.0.1) using older libraries (cuDNN &lt; v7.5 or MAGMA &lt; v2.5) may considerably leak memory resulting in an out-of-memory exception and death of your tasks. Please upgrade to the latest <code>torch</code> version.
</p>
<h2><span class="mw-headline" id="c10::Error">c10::Error</span></h2>
<p>There are cases where we get this kind of error:
</p>
<pre> terminate called after throwing an instance of 'c10::Error'
   what():  Given groups=1, weight of size [256, 1, 3, 3], expected input[16, 10, 16, 16] to have 1 channels, but got 10 channels instead
 Exception raised from check_shape_forward at /tmp/coulombc/pytorch_build_2021-11-09_14-57-01/avx2/python3.8/pytorch/aten/src/ATen/native/Convolution.cpp:496 (most recent call first):
 ...
</pre>
<p>A C++ exception is thrown instead of a Python exception. This might happen when programming in C++ with libtorch, but it is unexpected when programming in Python. We cannot see the Python traceback, which makes it difficult to pinpoint the cause of the error in our python script. On Graham, it has been observed that using PyTorch 1.9.1 (instead of PyTorch 1.10.x) helps: it allows to get the Python traceback.
</p>
<h2><span class="mw-headline" id="CUDA_error:_no_kernel_image_is_available_for_execution_on_the_device">CUDA error: no kernel image is available for execution on the device</span></h2>
<p>This exception means that the current torch installation does not support the compute architecture or the device (gpu) used.
Either update <tt>torch</tt> to a more recent version or request a different GPU compatible with the current version used.
</p>
<h1><span class="mw-headline" id="LibTorch">LibTorch</span></h1>
<p>LibTorch allows one to implement both C++ extensions to PyTorch and <b>pure C++ machine learning applications</b>. It contains "all headers, libraries and CMake configuration files required to depend on PyTorch", as described in the <a rel="nofollow" class="external text" href="https://pytorch.org/cppdocs/installing.html">documentation</a>.
</p>
<h3><span class="mw-headline" id="How_to_use_LibTorch">How to use LibTorch</span></h3>
<h4><span class="mw-headline" id="Setting_up_the_environment">Setting up the environment</span></h4>
<p>Load the modules required by Libtorch, then install PyTorch in a Python virtual environment:
</p>
<form id="tabs-inputform" class="tabs tabs-inputform" action="#"></form><div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-1-0" name="tabs-1" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-1-1" name="tabs-1" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-1-1" data-tabpos="1">StdEnv/2023</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-1-2" name="tabs-1" class="tabs-input tabs-input-2" /><label class="tabs-label" for="tabs-input-1-2" data-tabpos="2">StdEnv/2020</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<pre>module load StdEnv/2023 gcc cuda/12.2 cmake protobuf cudnn python/3.11 abseil  cusparselt  opencv/4.8.1
virtualenv --no-download --clear ~/ENV &amp;&amp; source ~/ENV/bin/activate 
pip install --no-index torch numpy 
</pre>
<p>Note that the versions for the abseil, cusparselt and opencv modules may need to be adjusted,
depending on the version of the torch package.  In order to find out which version of those
modules was used to compile the Python wheel for torch, use the following command:
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=ldd+%24VIRTUAL_ENV%2Flib%2Fpython3.11%2Fsite-packages%2Ftorch%2Flib%2Flibtorch_cuda.so+%7C+sed+-n+%27s%26%5E.%2A%2F%5C%28%5C%28opencv%5C%7Cabseil%5C%7Ccusparselt%5C%29%2F%5B%5E%2F%5D%2A%5C%29.%2A%26%5C1%26p%27+%7C+sort+-u" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>$<span class="w"> </span>ldd<span class="w"> </span><span class="nv">$VIRTUAL_ENV</span>/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so<span class="w"> </span><span class="p">|</span><span class="w"> </span>sed<span class="w"> </span>-n<span class="w"> </span><span class="s1">&#39;s&amp;^.*/\(\(opencv\|abseil\|cusparselt\)/[^/]*\).*&amp;\1&amp;p&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sort<span class="w"> </span>-u
abseil/20230125.3
cusparselt/0.5.0.1
opencv/4.8.1
</pre></div>
</div>
</div>
</div>
<div class="tabs-content tabs-content-2">
<pre>module load gcc cuda/11.4 cmake protobuf cudnn python/3.10
virtualenv --no-download --clear ~/ENV &amp;&amp; source ~/ENV/bin/activate 
pip install --no-index torch numpy 
</pre>
</div>
</div></div>
<h4><span class="mw-headline" id="Compiling_a_minimal_example">Compiling a minimal example</span></h4>
<p>Create the following two files:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> example.cpp</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23include+%3Ctorch%2Ftorch.h%3E%0A%23include+%3Ciostream%3E%0A%0Aint+main%28%29+%0A%7B%0A++++torch%3A%3ADevice+device%28torch%3A%3AkCPU%29%3B%0A++++if+%28torch%3A%3Acuda%3A%3Ais_available%28%29%29+%0A++++%7B%0A++++++++std%3A%3Acout+%3C%3C+%22CUDA+is+available%21+Using+GPU.%22+%3C%3C+std%3A%3Aendl%3B%0A++++++++device+%3D+torch%3A%3ADevice%28torch%3A%3AkCUDA%29%3B%0A++++%7D%0A%0A++++torch%3A%3ATensor+tensor+%3D+torch%3A%3Arand%28%7B2%2C+3%7D%29.to%28device%29%3B%0A++++std%3A%3Acout+%3C%3C+tensor+%3C%3C+std%3A%3Aendl%3B%0A%7D" />
<input type="hidden" name="filename" value="example.cpp" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-cpp mw-content-ltr" dir="ltr"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/torch.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span>
<span class="p">{</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCPU</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">is_available</span><span class="p">())</span><span class="w"> </span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;CUDA is available! Using GPU.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">        </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">}).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> CMakeLists.txt</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="cmake_minimum_required%28VERSION+3.0+FATAL_ERROR%29%0Aproject%28example%29%0A%0Afind_package%28Torch+REQUIRED%29%0A%0Aadd_executable%28example+example.cpp%29%0Atarget_link_libraries%28example+%22%24%7BTORCH_LIBRARIES%7D%22%29%0Aset_property%28TARGET+example+PROPERTY+CXX_STANDARD+14%29" />
<input type="hidden" name="filename" value="CMakeLists.txt" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-content-ltr" dir="ltr"><pre>cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
project(example)

find_package(Torch REQUIRED)

add_executable(example example.cpp)
target_link_libraries(example "${TORCH_LIBRARIES}")
set_property(TARGET example PROPERTY CXX_STANDARD 14)</pre></div>
</div>
<p><br />
</p><p>With the python virtualenv activated, configure the project and compile the program:
</p>
<div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-2-0" name="tabs-2" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-2-1" name="tabs-2" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-2-1" data-tabpos="1">StdEnv/2023</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-2-2" name="tabs-2" class="tabs-input tabs-input-2" /><label class="tabs-label" for="tabs-input-2-2" data-tabpos="2">StdEnv/2020</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<pre>cmake -B build -S . -DCMAKE_PREFIX_PATH=$VIRTUAL_ENV/lib/python3.11/site-packages \
                    -DCMAKE_EXE_LINKER_FLAGS=-Wl,-rpath=$VIRTUAL_ENV/lib/python3.11/site-packages/torch/lib,-L$EBROOTCUDA/extras/CUPTI/lib64 \
                    -DCMAKE_SKIP_RPATH=ON -DTORCH_CUDA_ARCH_LIST="6.0;7.0;7.5;8.0;9.0"
cmake --build build
</pre>
</div>
<div class="tabs-content tabs-content-2">
<pre>cmake -B build -S . -DCMAKE_PREFIX_PATH=$VIRTUAL_ENV/lib/python3.10/site-packages \
                    -DCMAKE_EXE_LINKER_FLAGS=-Wl,-rpath=$VIRTUAL_ENV/lib/python3.10/site-packages/torch/lib \
                    -DCMAKE_SKIP_RPATH=ON
cmake --build build 
</pre>
</div>
</div></div>
<p>Run the program:
</p>
<pre>build/example
</pre>
<p>To test an application with CUDA, request an <a href="/wiki/Running_jobs#Interactive_jobs" title="Running jobs">interactive job</a> with a <a href="/wiki/Using_GPUs_with_Slurm" title="Using GPUs with Slurm">GPU</a>.
</p>
<h1><span class="mw-headline" id="Resources">Resources</span></h1>
<p><a rel="nofollow" class="external free" href="https://pytorch.org/cppdocs/">https://pytorch.org/cppdocs/</a>
</p>
<!-- 
NewPP limit report
Cached time: 20250530235248
Cache expiry: 86400
Reduced expiry: false
Complications: [show‐toc]
CPU time usage: 0.239 seconds
Real time usage: 0.240 seconds
Preprocessor visited node count: 1759/1000000
Post‐expand include size: 70248/2097152 bytes
Template argument size: 100251/2097152 bytes
Highest expansion depth: 7/100
Expensive parser function count: 30/100
Unstrip recursion depth: 2/20
Unstrip post‐expand size: 310375/5000000 bytes
ExtLoops count: 0/100
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%   21.464      1 -total
 68.95%   14.800     26 Template:File
  6.41%    1.376      5 Template:Command
  1.50%    0.321      1 Template:Note
-->

<!-- Saved in parser cache with key ccwiki:pcache:idhash:4457-0!canonical and timestamp 20250530235248 and revision id 175635. Rendering was triggered because: page-view
 -->
</div>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://docs.alliancecan.ca/mediawiki/index.php?title=PyTorch&amp;oldid=175635">https://docs.alliancecan.ca/mediawiki/index.php?title=PyTorch&amp;oldid=175635</a>"</div></div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>: <ul><li><a href="/mediawiki/index.php?title=Category:Pages_with_syntax_highlighting_errors&amp;action=edit&amp;redlink=1" class="new" title="Category:Pages with syntax highlighting errors (page does not exist)">Pages with syntax highlighting errors</a></li><li><a href="/wiki/Category:Software" title="Category:Software">Software</a></li><li><a href="/wiki/Category:AI_and_Machine_Learning" title="Category:AI and Machine Learning">AI and Machine Learning</a></li></ul></div></div>
	</div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		
<nav id="p-personal" class="mw-portlet mw-portlet-personal vector-user-menu-legacy vector-menu" aria-labelledby="p-personal-label"  >
	<h3
		id="p-personal-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Personal tools</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-uls" class="mw-list-item active"><a class="uls-trigger" href="#"><span>English</span></a></li><li id="pt-login" class="mw-list-item"><a href="/mediawiki/index.php?title=Special:UserLogin&amp;returnto=PyTorch" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o"><span>Log in</span></a></li>
		</ul>
		
	</div>
</nav>

		<div id="left-navigation">
			
<nav id="p-namespaces" class="mw-portlet mw-portlet-namespaces vector-menu-tabs vector-menu-tabs-legacy vector-menu" aria-labelledby="p-namespaces-label"  >
	<h3
		id="p-namespaces-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Namespaces</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected mw-list-item"><a href="/wiki/PyTorch" title="View the content page [c]" accesskey="c"><span>Page</span></a></li><li id="ca-talk" class="new mw-list-item"><a href="/mediawiki/index.php?title=Talk:PyTorch&amp;action=edit&amp;redlink=1" rel="discussion" class="new" title="Discussion about the content page (page does not exist) [t]" accesskey="t"><span>Discussion</span></a></li>
		</ul>
		
	</div>
</nav>

			
<nav id="p-variants" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown vector-menu" aria-labelledby="p-variants-label"  >
	<input type="checkbox"
		id="p-variants-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-variants"
		class="vector-menu-checkbox"
		aria-labelledby="p-variants-label"
	>
	<label
		id="p-variants-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">English</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</nav>

		</div>
		<div id="right-navigation">
			
<nav id="p-views" class="mw-portlet mw-portlet-views vector-menu-tabs vector-menu-tabs-legacy vector-menu" aria-labelledby="p-views-label"  >
	<h3
		id="p-views-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Views</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected mw-list-item"><a href="/wiki/PyTorch"><span>Read</span></a></li><li id="ca-viewsource" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch&amp;action=edit" title="This page is protected.&#10;You can view its source [e]" accesskey="e"><span>View source</span></a></li><li id="ca-history" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</nav>

			
<nav id="p-cactions" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown vector-menu" aria-labelledby="p-cactions-label"  title="More options" >
	<input type="checkbox"
		id="p-cactions-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-cactions"
		class="vector-menu-checkbox"
		aria-labelledby="p-cactions-label"
	>
	<label
		id="p-cactions-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">More</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</nav>

			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<h3 >Search</h3>
	<form action="/mediawiki/index.php" id="searchform" class="vector-search-box-form">
		<div id="simpleSearch"
			class="vector-search-box-inner"
			 data-search-loc="header-navigation">
			<input class="vector-search-box-input"
				 type="search" name="search" placeholder="Search Alliance Doc" aria-label="Search Alliance Doc" autocapitalize="sentences" title="Search Alliance Doc [f]" accesskey="f" id="searchInput"
			>
			<input type="hidden" name="title" value="Special:Search">
			<input id="mw-searchButton"
				 class="searchButton mw-fallbackSearchButton" type="submit" name="fulltext" title="Search the pages for this text" value="Search">
			<input id="searchButton"
				 class="searchButton" type="submit" name="go" title="Go to a page with this exact name if it exists" value="Go">
		</div>
	</form>
</div>

		</div>
	</div>
	
<div id="mw-panel" class="vector-legacy-sidebar">
	<div id="p-logo" role="banner">
		<a class="mw-wiki-logo" href="/wiki/Technical_documentation"
			title="Visit the main page"></a>
	</div>
	
<nav id="p-navigation" class="mw-portlet mw-portlet-navigation vector-menu-portal portal vector-menu" aria-labelledby="p-navigation-label"  >
	<h3
		id="p-navigation-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Navigation</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-wiki-main-page" class="mw-list-item"><a href="/wiki/Technical_documentation"><span>Wiki Main Page</span></a></li>
		</ul>
		
	</div>
</nav>

	
<nav id="p-sidebar-support" class="mw-portlet mw-portlet-sidebar-support vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-support-label"  >
	<h3
		id="p-sidebar-support-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Support</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-getting-started" class="mw-list-item"><a href="/wiki/Getting_started"><span>Getting started</span></a></li><li id="n-sidebar-technical-support" class="mw-list-item"><a href="/wiki/Technical_support"><span>Getting help</span></a></li><li id="n-sidebar-running-jobs" class="mw-list-item"><a href="/wiki/Running_jobs"><span>Running jobs</span></a></li><li id="n-sidebar-known-issues" class="mw-list-item"><a href="/wiki/Known_issues"><span>Known issues</span></a></li><li id="n-sidebar-system-status" class="mw-list-item"><a href="http://status.computecanada.ca" rel="nofollow"><span>System status</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-resources" class="mw-portlet mw-portlet-sidebar-resources vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-resources-label"  >
	<h3
		id="p-sidebar-resources-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Resources</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-Béluga" class="mw-list-item"><a href="/wiki/B%C3%A9luga/en"><span>Béluga</span></a></li><li id="n-Cedar" class="mw-list-item"><a href="/wiki/Cedar"><span>Cedar</span></a></li><li id="n-Graham" class="mw-list-item"><a href="/wiki/Graham"><span>Graham</span></a></li><li id="n-Narval" class="mw-list-item"><a href="/wiki/Narval/en"><span>Narval</span></a></li><li id="n-Niagara" class="mw-list-item"><a href="/wiki/Niagara"><span>Niagara</span></a></li><li id="n-sidebar-cloud" class="mw-list-item"><a href="/wiki/CC-Cloud"><span>Cloud</span></a></li><li id="n-tamIA" class="mw-list-item"><a href="/wiki/TamIA/en"><span>tamIA</span></a></li><li id="n-Killarney" class="mw-list-item"><a href="/wiki/Killarney"><span>Killarney</span></a></li><li id="n-sidebar-quantum-computing" class="mw-list-item"><a href="/wiki/Services_d%27informatique_quantique/en"><span>Quantum computing</span></a></li><li id="n-sidebar-available-software" class="mw-list-item"><a href="/wiki/Available_software"><span>Available software</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-alliance" class="mw-portlet mw-portlet-sidebar-alliance vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-alliance-label"  >
	<h3
		id="p-sidebar-alliance-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">The Alliance</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-alliance-main-page" class="mw-list-item"><a href="https://alliancecan.ca/en" rel="nofollow"><span>Alliance main page</span></a></li><li id="n-sidebar-ccdb" class="mw-list-item"><a href="https://ccdb.computecanada.ca/security/login" rel="nofollow"><span>CCDB</span></a></li><li id="n-sidebar-getting-an-account" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/account-management/apply-account" rel="nofollow"><span>Getting An Account</span></a></li><li id="n-sidebar-acknowledging-alliance" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/research-portal/acknowledging-alliance" rel="nofollow"><span>Acknowledging the Alliance</span></a></li><li id="n-sidebar-aup" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/account-management/policies" rel="nofollow"><span>Acceptable Use Policy</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-authoring" class="mw-portlet mw-portlet-sidebar-authoring vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-authoring-label"  >
	<h3
		id="p-sidebar-authoring-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Authoring</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-guidelines" class="mw-list-item"><a href="/wiki/Authoring_guidelines"><span>Guidelines</span></a></li><li id="n-sidebar-mediawiki-help" class="mw-list-item"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/Help:Contents"><span>MediaWiki Help</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r"><span>Recent changes</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-tb" class="mw-portlet mw-portlet-tb vector-menu-portal portal vector-menu" aria-labelledby="p-tb-label"  >
	<h3
		id="p-tb-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Tools</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/PyTorch" title="A list of all wiki pages that link here [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/PyTorch" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-print" class="mw-list-item"><a href="javascript:print();" rel="alternate" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch&amp;oldid=175635" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch&amp;action=info" title="More information about this page"><span>Page information</span></a></li>
		</ul>
		
	</div>
</nav>

	
</div>

</div>

<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 10 April 2025, at 03:05.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="/wiki/CCWiki:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/CCWiki:About">About Alliance Doc</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/CCWiki:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-mobileview"><a href="https://docs.alliancecan.ca/mediawiki/index.php?title=PyTorch&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><img src="/mediawiki/resources/assets/poweredby_mediawiki.svg" alt="Powered by MediaWiki" width="88" height="31" loading="lazy"></a></li>
</ul>

</footer>

<script>(RLQ=window.RLQ||[]).push(function(){mw.log.warn("This page is using the deprecated ResourceLoader module \"codex-search-styles\".\n[1.43] Use a CodexModule with codexComponents to set your specific components used: https://www.mediawiki.org/wiki/Codex#Using_a_limited_subset_of_components");});</script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-TVBPRD78K4" async=""></script><script>
window.dataLayer = window.dataLayer || [];

function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-TVBPRD78K4', {});
</script>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":349,"wgPageParseReport":{"limitreport":{"cputime":"0.239","walltime":"0.240","ppvisitednodes":{"value":1759,"limit":1000000},"postexpandincludesize":{"value":70248,"limit":2097152},"templateargumentsize":{"value":100251,"limit":2097152},"expansiondepth":{"value":7,"limit":100},"expensivefunctioncount":{"value":30,"limit":100},"unstrip-depth":{"value":2,"limit":20},"unstrip-size":{"value":310375,"limit":5000000},"timingprofile":["100.00%   21.464      1 -total"," 68.95%   14.800     26 Template:File","  6.41%    1.376      5 Template:Command","  1.50%    0.321      1 Template:Note"]},"loops":{"limitreport-count-limited":{"value":0,"limit":100}},"cachereport":{"timestamp":"20250530235248","ttl":86400,"transientcontent":false}}});});</script>
</body>
</html>