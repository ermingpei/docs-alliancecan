<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>PyTorch - Alliance Doc</title>
<script>(function(){var className="client-js";var cookie=document.cookie.match(/(?:^|; )ccwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":[",\t."," \t,"],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","janvier","février","mars","avril","mai","juin","juillet","août","septembre","octobre","novembre","décembre"],"wgRequestId":"aDpEzI34nW-BQYizeTZIRwAAA1Y","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"PyTorch/fr","wgTitle":"PyTorch/fr","wgCurRevisionId":175858,"wgRevisionId":175858,"wgArticleId":4699,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":[
"Pages with syntax highlighting errors","Software","AI and Machine Learning"],"wgPageViewLanguage":"fr","wgPageContentLanguage":"fr","wgPageContentModel":"wikitext","wgRelevantPageName":"PyTorch/fr","wgRelevantArticleId":4699,"wgIsProbablyEditable":false,"wgRelevantPageIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgULSAcceptLanguageList":[],"wgMFDisplayWikibaseDescriptions":{"search":false,"watchlist":false,"tagline":false},"wgCiteReferencePreviewsActive":true,"wgTranslatePageTranslation":"translation","wgULSPosition":"personal","wgULSisCompactLinksEnabled":true,"wgVector2022LanguageInHeader":false,"wgULSisLanguageSelectorEmpty":false};RLSTATE={"site.styles":"ready","user.styles":"ready","user":"ready","user.options":"loading","ext.translate.tag.languages":"ready","ext.pygments":"ready","skins.vector.styles.legacy":"ready","ext.translate":"ready","codex-search-styles":"ready","ext.uls.pt":"ready"};RLPAGEMODULES=["ext.pygments.view","ext.tabs","site",
"mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.languageSelector","ext.translate.pagetranslation.uls","ext.uls.compactlinks","ext.uls.geoclient","ext.uls.interface","ext.moderation.notify","ext.moderation.notify.desktop"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/mediawiki/load.php?lang=en&amp;modules=codex-search-styles%7Cext.pygments%2Ctranslate%7Cext.translate.tag.languages%7Cext.uls.pt%7Cskins.vector.styles.legacy&amp;only=styles&amp;skin=vector">
<script async="" src="/mediawiki/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/mediawiki/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector">
<meta name="generator" content="MediaWiki 1.43.0">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta name="viewport" content="width=1120">
<link rel="icon" href="/mediawiki/resources/assets/Alliance_favicon.png">
<link rel="search" type="application/opensearchdescription+xml" href="/mediawiki/rest.php/v1/search" title="Alliance Doc (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://docs.alliancecan.ca/mediawiki/api.php?action=rsd">
<link rel="alternate" type="application/atom+xml" title="Alliance Doc Atom feed" href="/mediawiki/index.php?title=Special:RecentChanges&amp;feed=atom">
<style type="text/css" id="tabs-dynamic-styles">/*<![CDATA[*/
/* Dynamically generated tabs styles */
.tabs-input-1:checked ~ .tabs-container .tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-content-2,
.tabs-input-0:checked ~ .tabs-container .tabs-content-1 {display:inline-block;}
.tabs-input-1:checked ~ .tabs-container .tabs-inline.tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-inline.tabs-content-2,
.tabs-input-0:checked ~ .tabs-container .tabs-inline.tabs-content-1 {display:inline;}
.tabs-input-1:checked ~ .tabs-container .tabs-block.tabs-content-1,
.tabs-input-2:checked ~ .tabs-container .tabs-block.tabs-content-2,
.tabs-input-0:checked ~ .tabs-container .tabs-block.tabs-content-1 {display:block;}
/* The same styles, but with .checked instead of :checked, for browsers that rely on the JavaScript fallback */
.tabs-input-1.checked ~ .tabs-container .tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-content-2,
.tabs-input-0.checked ~ .tabs-container .tabs-content-1 {display:inline-block;}
.tabs-input-1.checked ~ .tabs-container .tabs-inline.tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-inline.tabs-content-2,
.tabs-input-0.checked ~ .tabs-container .tabs-inline.tabs-content-1 {display:inline;}
.tabs-input-1.checked ~ .tabs-container .tabs-block.tabs-content-1,
.tabs-input-2.checked ~ .tabs-container .tabs-block.tabs-content-2,
.tabs-input-0.checked ~ .tabs-container .tabs-block.tabs-content-1 {display:block;}
.tabs-dropdown .tabs-content,.tabs-dropdown .tabs-container,.tabs-dropdown li,.tabs-dropdown ul,.tabs-dropdown ol {background-color: white /* Malicious data in tabs-dropdown-bgcolor */}
/*]]>*/</style>
</head>
<body class="skin-vector-legacy mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-PyTorch_fr rootpage-PyTorch_fr skin-vector action-view"><div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice"></div>
	<div class="mw-indicators">
	<div id="mw-indicator-languageselector" class="mw-indicator"><span id="languageselector-box-1" class="languageselector " style=""><form name="languageselector-form-1" id="languageselector-form-1" method="get" action="/mediawiki/index.php" style="display:inline;"><input type="hidden" value="PyTorch/fr" name="title"><select name="setlang" id="languageselector-select-1" style=""><option value="aae">Arbërisht</option><option value="ab">аԥсшәа</option><option value="abs">bahasa ambon</option><option value="ace">Acèh</option><option value="acf">Kwéyòl Sent Lisi</option><option value="acm">عراقي</option><option value="ady">адыгабзэ</option><option value="ady-cyrl">адыгабзэ</option><option value="aeb">تونسي / Tûnsî</option><option value="aeb-arab">تونسي</option><option value="aeb-latn">Tûnsî</option><option value="af">Afrikaans</option><option value="aln">Gegë</option><option value="alt">алтай тил</option><option value="am">አማርኛ</option><option value="ami">Pangcah</option><option value="an">aragonés</option><option value="ang">Ænglisc</option><option value="ann">Obolo</option><option value="anp">अंगिका</option><option value="apc">شامي</option><option value="ar">العربية</option><option value="arc">ܐܪܡܝܐ</option><option value="arn">mapudungun</option><option value="arq">جازايرية</option><option value="ary">الدارجة</option><option value="arz">مصرى</option><option value="as">অসমীয়া</option><option value="ase">American sign language</option><option value="ast">asturianu</option><option value="atj">Atikamekw</option><option value="av">авар</option><option value="avk">Kotava</option><option value="awa">अवधी</option><option value="ay">Aymar aru</option><option value="az">azərbaycanca</option><option value="azb">تۆرکجه</option><option value="ba">башҡортса</option><option value="ban">Basa Bali</option><option value="ban-bali">ᬩᬲᬩᬮᬶ</option><option value="bar">Boarisch</option><option value="bbc">Batak Toba</option><option value="bbc-latn">Batak Toba</option><option value="bcc">جهلسری بلوچی</option><option value="bci">wawle</option><option value="bcl">Bikol Central</option><option value="bdr">Bajau Sama</option><option value="be">беларуская</option><option value="be-tarask">беларуская (тарашкевіца)</option><option value="bew">Betawi</option><option value="bg">български</option><option value="bgc">हरियाणवी</option><option value="bgn">روچ کپتین بلوچی</option><option value="bh">भोजपुरी</option><option value="bho">भोजपुरी</option><option value="bi">Bislama</option><option value="bjn">Banjar</option><option value="blk">ပအိုဝ်ႏဘာႏသာႏ</option><option value="bm">bamanankan</option><option value="bn">বাংলা</option><option value="bo">བོད་ཡིག</option><option value="bpy">বিষ্ণুপ্রিয়া মণিপুরী</option><option value="bqi">بختیاری</option><option value="br">brezhoneg</option><option value="brh">Bráhuí</option><option value="bs">bosanski</option><option value="btm">Batak Mandailing</option><option value="bto">Iriga Bicolano</option><option value="bug">Basa Ugi</option><option value="bxr">буряад</option><option value="ca">català</option><option value="cbk-zam">Chavacano de Zamboanga</option><option value="ccp">𑄌𑄋𑄴𑄟𑄳𑄦</option><option value="cdo">閩東語 / Mìng-dĕ̤ng-ngṳ̄</option><option value="ce">нохчийн</option><option value="ceb">Cebuano</option><option value="ch">Chamoru</option><option value="chn">chinuk wawa</option><option value="chr">ᏣᎳᎩ</option><option value="chy">Tsetsêhestâhese</option><option value="ckb">کوردی</option><option value="co">corsu</option><option value="cps">Capiceño</option><option value="cpx">莆仙語 / Pó-sing-gṳ̂</option><option value="cpx-hans">莆仙语（简体）</option><option value="cpx-hant">莆仙語（繁體）</option><option value="cr">Nēhiyawēwin / ᓀᐦᐃᔭᐍᐏᐣ</option><option value="crh">qırımtatarca</option><option value="crh-cyrl">къырымтатарджа (Кирилл)</option><option value="crh-latn">qırımtatarca (Latin)</option><option value="crh-ro">tatarşa</option><option value="cs">čeština</option><option value="csb">kaszëbsczi</option><option value="cu">словѣньскъ / ⰔⰎⰑⰂⰡⰐⰠⰔⰍⰟ</option><option value="cv">чӑвашла</option><option value="cy">Cymraeg</option><option value="da">dansk</option><option value="dag">dagbanli</option><option value="de">Deutsch</option><option value="de-at">Österreichisches Deutsch</option><option value="de-ch">Schweizer Hochdeutsch</option><option value="de-formal">Deutsch (Sie-Form)</option><option value="dga">Dagaare</option><option value="din">Thuɔŋjäŋ</option><option value="diq">Zazaki</option><option value="dsb">dolnoserbski</option><option value="dtp">Kadazandusun</option><option value="dty">डोटेली</option><option value="dua">Duálá</option><option value="dv">ދިވެހިބަސް</option><option value="dz">ཇོང་ཁ</option><option value="ee">eʋegbe</option><option value="efi">Efịk</option><option value="egl">emiliàn e rumagnòl</option><option value="el">Ελληνικά</option><option value="eml">emiliàn e rumagnòl</option><option value="en" selected="">English</option><option value="en-ca">Canadian English</option><option value="en-gb">British English</option><option value="eo">Esperanto</option><option value="es">español</option><option value="es-formal">español (formal)</option><option value="et">eesti</option><option value="eu">euskara</option><option value="ext">estremeñu</option><option value="fa">فارسی</option><option value="fat">mfantse</option><option value="ff">Fulfulde</option><option value="fi">suomi</option><option value="fit">meänkieli</option><option value="fj">Na Vosa Vakaviti</option><option value="fo">føroyskt</option><option value="fon">fɔ̀ngbè</option><option value="fr">français</option><option value="frc">français cadien</option><option value="frp">arpetan</option><option value="frr">Nordfriisk</option><option value="fur">furlan</option><option value="fy">Frysk</option><option value="ga">Gaeilge</option><option value="gaa">Ga</option><option value="gag">Gagauz</option><option value="gan">贛語</option><option value="gan-hans">赣语（简体）</option><option value="gan-hant">贛語（繁體）</option><option value="gcf">kréyòl Gwadloup</option><option value="gcr">kriyòl gwiyannen</option><option value="gd">Gàidhlig</option><option value="gl">galego</option><option value="gld">на̄ни</option><option value="glk">گیلکی</option><option value="gn">Avañe'ẽ</option><option value="gom">गोंयची कोंकणी / Gõychi Konknni</option><option value="gom-deva">गोंयची कोंकणी</option><option value="gom-latn">Gõychi Konknni</option><option value="gor">Bahasa Hulontalo</option><option value="got">𐌲𐌿𐍄𐌹𐍃𐌺</option><option value="gpe">Ghanaian Pidgin</option><option value="grc">Ἀρχαία ἑλληνικὴ</option><option value="gsw">Alemannisch</option><option value="gu">ગુજરાતી</option><option value="guc">wayuunaiki</option><option value="gur">farefare</option><option value="guw">gungbe</option><option value="gv">Gaelg</option><option value="ha">Hausa</option><option value="hak">客家語 / Hak-kâ-ngî</option><option value="haw">Hawaiʻi</option><option value="he">עברית</option><option value="hi">हिन्दी</option><option value="hif">Fiji Hindi</option><option value="hif-latn">Fiji Hindi</option><option value="hil">Ilonggo</option><option value="hno">ہندکو</option><option value="hr">hrvatski</option><option value="hrx">Hunsrik</option><option value="hsb">hornjoserbsce</option><option value="hsn">湘語</option><option value="ht">Kreyòl ayisyen</option><option value="hu">magyar</option><option value="hu-formal">magyar (formal)</option><option value="hy">հայերեն</option><option value="hyw">Արեւմտահայերէն</option><option value="ia">interlingua</option><option value="iba">Jaku Iban</option><option value="ibb">ibibio</option><option value="id">Bahasa Indonesia</option><option value="ie">Interlingue</option><option value="ig">Igbo</option><option value="igl">Igala</option><option value="ii">ꆇꉙ</option><option value="ik">Iñupiatun</option><option value="ike-cans">ᐃᓄᒃᑎᑐᑦ</option><option value="ike-latn">inuktitut</option><option value="ilo">Ilokano</option><option value="inh">гӀалгӀай</option><option value="io">Ido</option><option value="is">íslenska</option><option value="isv-cyrl">меджусловјанскы</option><option value="isv-latn">medžuslovjansky</option><option value="it">italiano</option><option value="iu">ᐃᓄᒃᑎᑐᑦ / inuktitut</option><option value="ja">日本語</option><option value="jam">Patois</option><option value="jbo">la .lojban.</option><option value="jut">jysk</option><option value="jv">Jawa</option><option value="ka">ქართული</option><option value="kaa">Qaraqalpaqsha</option><option value="kab">Taqbaylit</option><option value="kai">Karai-karai</option><option value="kbd">адыгэбзэ</option><option value="kbd-cyrl">адыгэбзэ</option><option value="kbp">Kabɩyɛ</option><option value="kcg">Tyap</option><option value="kea">kabuverdianu</option><option value="kg">Kongo</option><option value="kge">Kumoring</option><option value="khw">کھوار</option><option value="ki">Gĩkũyũ</option><option value="kiu">Kırmancki</option><option value="kjh">хакас</option><option value="kjp">ဖၠုံလိက်</option><option value="kk">қазақша</option><option value="kk-arab">قازاقشا (تٴوتە)</option><option value="kk-cn">قازاقشا (جۇنگو)</option><option value="kk-cyrl">қазақша (кирил)</option><option value="kk-kz">қазақша (Қазақстан)</option><option value="kk-latn">qazaqşa (latın)</option><option value="kk-tr">qazaqşa (Türkïya)</option><option value="kl">kalaallisut</option><option value="km">ភាសាខ្មែរ</option><option value="kn">ಕನ್ನಡ</option><option value="knc">Yerwa Kanuri</option><option value="ko">한국어</option><option value="ko-kp">조선말</option><option value="koi">перем коми</option><option value="kr">kanuri</option><option value="krc">къарачай-малкъар</option><option value="kri">Krio</option><option value="krj">Kinaray-a</option><option value="krl">karjal</option><option value="ks">कॉशुर / کٲشُر</option><option value="ks-arab">کٲشُر</option><option value="ks-deva">कॉशुर</option><option value="ksh">Ripoarisch</option><option value="ksw">စှီၤ</option><option value="ku">kurdî</option><option value="ku-arab">کوردی (عەرەبی)</option><option value="ku-latn">kurdî (latînî)</option><option value="kum">къумукъ</option><option value="kus">Kʋsaal</option><option value="kv">коми</option><option value="kw">kernowek</option><option value="ky">кыргызча</option><option value="la">Latina</option><option value="lad">Ladino</option><option value="lb">Lëtzebuergesch</option><option value="lbe">лакку</option><option value="lez">лезги</option><option value="lfn">Lingua Franca Nova</option><option value="lg">Luganda</option><option value="li">Limburgs</option><option value="lij">Ligure</option><option value="liv">Līvõ kēļ</option><option value="lki">لەکی</option><option value="lld">Ladin</option><option value="lmo">lombard</option><option value="ln">lingála</option><option value="lo">ລາວ</option><option value="loz">Silozi</option><option value="lrc">لۊری شومالی</option><option value="lt">lietuvių</option><option value="ltg">latgaļu</option><option value="lua">ciluba</option><option value="lus">Mizo ţawng</option><option value="luz">لئری دوٙمینی</option><option value="lv">latviešu</option><option value="lzh">文言</option><option value="lzz">Lazuri</option><option value="mad">Madhurâ</option><option value="mag">मगही</option><option value="mai">मैथिली</option><option value="map-bms">Basa Banyumasan</option><option value="mdf">мокшень</option><option value="mg">Malagasy</option><option value="mhr">олык марий</option><option value="mi">Māori</option><option value="min">Minangkabau</option><option value="mk">македонски</option><option value="ml">മലയാളം</option><option value="mn">монгол</option><option value="mnc">manju gisun</option><option value="mnc-latn">manju gisun</option><option value="mnc-mong">ᠮᠠᠨᠵᡠ ᡤᡳᠰᡠᠨ</option><option value="mni">ꯃꯤꯇꯩ ꯂꯣꯟ</option><option value="mnw">ဘာသာမန်</option><option value="mo">молдовеняскэ</option><option value="mos">moore</option><option value="mr">मराठी</option><option value="mrh">Mara</option><option value="mrj">кырык мары</option><option value="ms">Bahasa Melayu</option><option value="ms-arab">بهاس ملايو</option><option value="mt">Malti</option><option value="mui">Baso Palembang</option><option value="mwl">Mirandés</option><option value="my">မြန်မာဘာသာ</option><option value="myv">эрзянь</option><option value="mzn">مازِرونی</option><option value="na">Dorerin Naoero</option><option value="nah">Nāhuatl</option><option value="nan">閩南語 / Bân-lâm-gú</option><option value="nan-hant">閩南語（傳統漢字）</option><option value="nan-latn-pehoeji">Bân-lâm-gú (Pe̍h-ōe-jī)</option><option value="nan-latn-tailo">Bân-lâm-gú (Tâi-lô)</option><option value="nap">Napulitano</option><option value="nb">norsk bokmål</option><option value="nds">Plattdüütsch</option><option value="nds-nl">Nedersaksies</option><option value="ne">नेपाली</option><option value="new">नेपाल भाषा</option><option value="nia">Li Niha</option><option value="nit">కొలామి</option><option value="niu">Niuē</option><option value="nl">Nederlands</option><option value="nl-informal">Nederlands (informeel)</option><option value="nmz">nawdm</option><option value="nn">norsk nynorsk</option><option value="nod">ᨣᩤᩴᨾᩮᩬᩥᨦ</option><option value="nog">ногайша</option><option value="nov">Novial</option><option value="nqo">ߒߞߏ</option><option value="nr">isiNdebele seSewula</option><option value="nrm">Nouormand</option><option value="nso">Sesotho sa Leboa</option><option value="nup">Nupe</option><option value="nv">Diné bizaad</option><option value="ny">Chi-Chewa</option><option value="nyn">runyankore</option><option value="nyo">Orunyoro</option><option value="nys">Nyunga</option><option value="oc">occitan</option><option value="ojb">Ojibwemowin</option><option value="olo">livvinkarjala</option><option value="om">Oromoo</option><option value="or">ଓଡ଼ିଆ</option><option value="os">ирон</option><option value="pa">ਪੰਜਾਬੀ</option><option value="pag">Pangasinan</option><option value="pam">Kapampangan</option><option value="pap">Papiamentu</option><option value="pcd">Picard</option><option value="pcm">Naijá</option><option value="pdc">Deitsch</option><option value="pdt">Plautdietsch</option><option value="pfl">Pälzisch</option><option value="pi">पालि</option><option value="pih">Norfuk / Pitkern</option><option value="pl">polski</option><option value="pms">Piemontèis</option><option value="pnb">پنجابی</option><option value="pnt">Ποντιακά</option><option value="prg">prūsiskan</option><option value="ps">پښتو</option><option value="pt">português</option><option value="pt-br">português do Brasil</option><option value="pwn">pinayuanan</option><option value="qqq">Message documentation</option><option value="qu">Runa Simi</option><option value="qug">Runa shimi</option><option value="rgn">Rumagnôl</option><option value="rif">Tarifit</option><option value="rki">ရခိုင်</option><option value="rm">rumantsch</option><option value="rmc">romaňi čhib</option><option value="rmy">romani čhib</option><option value="rn">ikirundi</option><option value="ro">română</option><option value="roa-tara">tarandíne</option><option value="rsk">руски</option><option value="ru">русский</option><option value="rue">русиньскый</option><option value="rup">armãneashti</option><option value="ruq">Vlăheşte</option><option value="ruq-cyrl">Влахесте</option><option value="ruq-latn">Vlăheşte</option><option value="rut">мыхаӀбишды</option><option value="rw">Ikinyarwanda</option><option value="ryu">うちなーぐち</option><option value="sa">संस्कृतम्</option><option value="sah">саха тыла</option><option value="sat">ᱥᱟᱱᱛᱟᱲᱤ</option><option value="sc">sardu</option><option value="scn">sicilianu</option><option value="sco">Scots</option><option value="sd">سنڌي</option><option value="sdc">Sassaresu</option><option value="sdh">کوردی خوارگ</option><option value="se">davvisámegiella</option><option value="se-fi">davvisámegiella (Suoma bealde)</option><option value="se-no">davvisámegiella (Norgga bealde)</option><option value="se-se">davvisámegiella (Ruoŧa bealde)</option><option value="sei">Cmique Itom</option><option value="ses">Koyraboro Senni</option><option value="sg">Sängö</option><option value="sgs">žemaitėška</option><option value="sh">srpskohrvatski / српскохрватски</option><option value="sh-cyrl">српскохрватски (ћирилица)</option><option value="sh-latn">srpskohrvatski (latinica)</option><option value="shi">Taclḥit</option><option value="shn">ၽႃႇသႃႇတႆး </option><option value="shy">tacawit</option><option value="shy-latn">tacawit</option><option value="si">සිංහල</option><option value="sjd">кӣллт са̄мь кӣлл</option><option value="sje">bidumsámegiella</option><option value="sk">slovenčina</option><option value="skr">سرائیکی</option><option value="skr-arab">سرائیکی</option><option value="sl">slovenščina</option><option value="sli">Schläsch</option><option value="sm">Gagana Samoa</option><option value="sma">åarjelsaemien</option><option value="smn">anarâškielâ</option><option value="sms">nuõrttsääʹmǩiõll</option><option value="sn">chiShona</option><option value="so">Soomaaliga</option><option value="sq">shqip</option><option value="sr">српски / srpski</option><option value="sr-ec">српски (ћирилица)</option><option value="sr-el">srpski (latinica)</option><option value="srn">Sranantongo</option><option value="sro">sardu campidanesu</option><option value="ss">SiSwati</option><option value="st">Sesotho</option><option value="stq">Seeltersk</option><option value="sty">себертатар</option><option value="su">Sunda</option><option value="sv">svenska</option><option value="sw">Kiswahili</option><option value="syl">ꠍꠤꠟꠐꠤ</option><option value="szl">ślůnski</option><option value="szy">Sakizaya</option><option value="ta">தமிழ்</option><option value="tay">Tayal</option><option value="tcy">ತುಳು</option><option value="tdd">ᥖᥭᥰ ᥖᥬᥲ ᥑᥨᥒᥰ</option><option value="te">తెలుగు</option><option value="tet">tetun</option><option value="tg">тоҷикӣ</option><option value="tg-cyrl">тоҷикӣ</option><option value="tg-latn">tojikī</option><option value="th">ไทย</option><option value="ti">ትግርኛ</option><option value="tig">ትግሬ</option><option value="tk">Türkmençe</option><option value="tl">Tagalog</option><option value="tly">tolışi</option><option value="tn">Setswana</option><option value="to">lea faka-Tonga</option><option value="tok">toki pona</option><option value="tpi">Tok Pisin</option><option value="tr">Türkçe</option><option value="tru">Ṫuroyo</option><option value="trv">Seediq</option><option value="ts">Xitsonga</option><option value="tt">татарча / tatarça</option><option value="tt-cyrl">татарча</option><option value="tt-latn">tatarça</option><option value="ttj">Orutooro</option><option value="tum">chiTumbuka</option><option value="tw">Twi</option><option value="ty">reo tahiti</option><option value="tyv">тыва дыл</option><option value="tzm">ⵜⴰⵎⴰⵣⵉⵖⵜ</option><option value="udm">удмурт</option><option value="ug">ئۇيغۇرچە / Uyghurche</option><option value="ug-arab">ئۇيغۇرچە</option><option value="ug-latn">Uyghurche</option><option value="uk">українська</option><option value="ur">اردو</option><option value="uz">oʻzbekcha / ўзбекча</option><option value="ve">Tshivenda</option><option value="vec">vèneto</option><option value="vep">vepsän kel’</option><option value="vi">Tiếng Việt</option><option value="vls">West-Vlams</option><option value="vmf">Mainfränkisch</option><option value="vmw">emakhuwa</option><option value="vo">Volapük</option><option value="vot">Vaďďa</option><option value="vro">võro</option><option value="wa">walon</option><option value="wal">wolaytta</option><option value="war">Winaray</option><option value="wls">Fakaʻuvea</option><option value="wo">Wolof</option><option value="wuu">吴语</option><option value="wuu-hans">吴语（简体）</option><option value="wuu-hant">吳語（正體）</option><option value="xal">хальмг</option><option value="xh">isiXhosa</option><option value="xmf">მარგალური</option><option value="xsy">saisiyat</option><option value="yi">ייִדיש</option><option value="yo">Yorùbá</option><option value="yrl">Nhẽẽgatú</option><option value="yue">粵語</option><option value="yue-hans">粵语（简体）</option><option value="yue-hant">粵語（繁體）</option><option value="za">Vahcuengh</option><option value="zea">Zeêuws</option><option value="zgh">ⵜⴰⵎⴰⵣⵉⵖⵜ ⵜⴰⵏⴰⵡⴰⵢⵜ</option><option value="zh">中文</option><option value="zh-cn">中文（中国大陆）</option><option value="zh-hans">中文（简体）</option><option value="zh-hant">中文（繁體）</option><option value="zh-hk">中文（香港）</option><option value="zh-mo">中文（澳門）</option><option value="zh-my">中文（马来西亚）</option><option value="zh-sg">中文（新加坡）</option><option value="zh-tw">中文（臺灣）</option><option value="zu">isiZulu</option></select><input id="languageselector-commit-1" style="" type="submit" value="set"></form></span></div>
	</div>
	<h1 id="firstHeading" class="firstHeading mw-first-heading">PyTorch</h1>
	<div id="bodyContent" class="vector-body">
		<div id="siteSub" class="noprint">From Alliance Doc</div>
		<div id="contentSub"><div id="mw-content-subtitle"></div></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" class="mw-body-content"><div class="mw-pt-translate-header noprint" dir="ltr" lang="en">This page is a <span class="plainlinks"><a rel="nofollow" class="external text" href="https://docs.alliancecan.ca/mediawiki/index.php?title=Special:Translate&amp;group=page-PyTorch&amp;action=page&amp;filter=&amp;language=fr">translated version</a></span> of the page <a href="/wiki/PyTorch" title="PyTorch">PyTorch</a> and the translation is 100% complete.</div><div class="mw-content-ltr mw-parser-output" lang="fr" dir="ltr"><div class="mw-pt-languages noprint navigation-not-searchable" lang="en" dir="ltr"><div class="mw-pt-languages-label">Other languages:</div><ul class="mw-pt-languages-list"><li><a href="/wiki/PyTorch" class="mw-pt-languages-ui mw-pt-progress mw-pt-progress--complete" title="PyTorch (100% translated)" lang="en" dir="ltr">English</a></li>
<li><span class="mw-pt-languages-selected mw-pt-progress mw-pt-progress--complete" lang="fr" dir="ltr">français</span></li></ul></div>
<p><a rel="nofollow" class="external text" href="http://pytorch.org/">PyTorch</a> est un paquet Python qui offre deux fonctionnalités de haut niveau&#160;: 
</p>
<ul><li>le calcul tensoriel (semblable à celui effectué par NumPy) avec forte accélération de GPU,</li>
<li>des réseaux de neurones d’apprentissage profond dans un système de gradients conçu sur le modèle d’un magnétophone.</li></ul>
<p>Si vous voulez porter un programme PyTorch sur une de nos grappes, il serait bon de prendre connaissance <a href="/wiki/Tutoriel_Apprentissage_machine" title="Tutoriel Apprentissage machine"> de ce tutoriel</a>.
</p><p><span id="Disambiguation"></span>
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Clarification"><span class="tocnumber">1</span> <span class="toctext">Clarification</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Installation"><span class="tocnumber">2</span> <span class="toctext">Installation</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Wheels_récemment_ajoutés"><span class="tocnumber">2.1</span> <span class="toctext">Wheels récemment ajoutés</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Installation_du_wheel"><span class="tocnumber">2.2</span> <span class="toctext">Installation du wheel</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="#GPU_et_CPU"><span class="tocnumber">2.2.1</span> <span class="toctext">GPU et CPU</span></a></li>
<li class="toclevel-3 tocsection-6"><a href="#En_supplément"><span class="tocnumber">2.2.2</span> <span class="toctext">En supplément</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-7"><a href="#Soumettre_une_tâche"><span class="tocnumber">3</span> <span class="toctext">Soumettre une tâche</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#Haute_performance"><span class="tocnumber">4</span> <span class="toctext">Haute performance</span></a>
<ul>
<li class="toclevel-2 tocsection-9"><a href="#TF32_:_Performance_vs_précision"><span class="tocnumber">4.1</span> <span class="toctext">TF32&#160;: Performance vs précision</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Travailler_avec_plusieurs_CPU"><span class="tocnumber">4.2</span> <span class="toctext">Travailler avec plusieurs CPU</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Travailler_avec_un_seul_GPU"><span class="tocnumber">4.3</span> <span class="toctext">Travailler avec un seul GPU</span></a>
<ul>
<li class="toclevel-3 tocsection-12"><a href="#Parallélisme_des_données_avec_un_seul_GPU"><span class="tocnumber">4.3.1</span> <span class="toctext">Parallélisme des données avec un seul GPU</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-13"><a href="#Travailler_avec_plusieurs_GPU"><span class="tocnumber">4.4</span> <span class="toctext">Travailler avec plusieurs GPU</span></a>
<ul>
<li class="toclevel-3 tocsection-14"><a href="#Problème_avec_DistributedDataParallel_et_PyTorch_1.10"><span class="tocnumber">4.4.1</span> <span class="toctext">Problème avec DistributedDataParallel et PyTorch 1.10</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="#Paralléliser_les_données_avec_plusieurs_GPU"><span class="tocnumber">4.4.2</span> <span class="toctext">Paralléliser les données avec plusieurs GPU</span></a>
<ul>
<li class="toclevel-4 tocsection-16"><a href="#DistributedDataParallel"><span class="tocnumber">4.4.2.1</span> <span class="toctext">DistributedDataParallel</span></a></li>
<li class="toclevel-4 tocsection-17"><a href="#PyTorch_Lightning"><span class="tocnumber">4.4.2.2</span> <span class="toctext">PyTorch Lightning</span></a></li>
<li class="toclevel-4 tocsection-18"><a href="#Horovod"><span class="tocnumber">4.4.2.3</span> <span class="toctext">Horovod</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-19"><a href="#Paralléliser_un_modèle_avec_plusieurs_GPU"><span class="tocnumber">4.4.3</span> <span class="toctext">Paralléliser un modèle avec plusieurs GPU</span></a></li>
<li class="toclevel-3 tocsection-20"><a href="#Paralléliser_modèle_et_données_avec_plusieurs_GPU"><span class="tocnumber">4.4.4</span> <span class="toctext">Paralléliser modèle et données avec plusieurs GPU</span></a>
<ul>
<li class="toclevel-4 tocsection-21"><a href="#Utiliser_Torch_RPC_et_DDP"><span class="tocnumber">4.4.4.1</span> <span class="toctext">Utiliser Torch RPC et DDP</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-22"><a href="#DeepSpeed"><span class="tocnumber">4.4.5</span> <span class="toctext">DeepSpeed</span></a>
<ul>
<li class="toclevel-4 tocsection-23"><a href="#ZeRO_avec_GPU"><span class="tocnumber">4.4.5.1</span> <span class="toctext">ZeRO avec GPU</span></a></li>
<li class="toclevel-4 tocsection-24"><a href="#ZeRO_avec_CPU"><span class="tocnumber">4.4.5.2</span> <span class="toctext">ZeRO avec CPU</span></a></li>
<li class="toclevel-4 tocsection-25"><a href="#ZeRO_avec_utilisation_de_disques_NVMe"><span class="tocnumber">4.4.5.3</span> <span class="toctext">ZeRO avec utilisation de disques NVMe</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-26"><a href="#Créer_des_points_de_contrôle"><span class="tocnumber">5</span> <span class="toctext">Créer des points de contrôle</span></a>
<ul>
<li class="toclevel-2 tocsection-27"><a href="#Avec_PyTorch_Lightning"><span class="tocnumber">5.1</span> <span class="toctext">Avec PyTorch Lightning</span></a></li>
<li class="toclevel-2 tocsection-28"><a href="#Avec_des_boucles_d&#39;entraînement_personnalisées"><span class="tocnumber">5.2</span> <span class="toctext">Avec des boucles d'entraînement personnalisées</span></a></li>
<li class="toclevel-2 tocsection-29"><a href="#Pendant_l’entraînement_distribué"><span class="tocnumber">5.3</span> <span class="toctext">Pendant l’entraînement distribué</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-30"><a href="#Dépannage"><span class="tocnumber">6</span> <span class="toctext">Dépannage</span></a>
<ul>
<li class="toclevel-2 tocsection-31"><a href="#Fuites_de_mémoire"><span class="tocnumber">6.1</span> <span class="toctext">Fuites de mémoire</span></a></li>
<li class="toclevel-2 tocsection-32"><a href="#c10::Error"><span class="tocnumber">6.2</span> <span class="toctext">c10::Error</span></a></li>
<li class="toclevel-2 tocsection-33"><a href="#Erreur_CUDA_:_no_kernel_image_is_available_for_execution_on_the_device"><span class="tocnumber">6.3</span> <span class="toctext">Erreur CUDA&#160;: no kernel image is available for execution on the device</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-34"><a href="#LibTorch"><span class="tocnumber">7</span> <span class="toctext">LibTorch</span></a>
<ul>
<li class="toclevel-2 tocsection-35"><a href="#Utiliser_LibTorch"><span class="tocnumber">7.1</span> <span class="toctext">Utiliser LibTorch</span></a>
<ul>
<li class="toclevel-3 tocsection-36"><a href="#Configurer_l&#39;environnement"><span class="tocnumber">7.1.1</span> <span class="toctext">Configurer l'environnement</span></a></li>
<li class="toclevel-3 tocsection-37"><a href="#Compiler_un_exemple_simple"><span class="tocnumber">7.1.2</span> <span class="toctext">Compiler un exemple simple</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-38"><a href="#Ressources"><span class="tocnumber">8</span> <span class="toctext">Ressources</span></a></li>
</ul>
</div>

<h1><span class="mw-headline" id="Clarification">Clarification</span></h1>
<p>Il y a une certaine ressemblance entre PyTorch et <a href="/wiki/Torch/fr" title="Torch/fr">Torch</a>, mais pour des raisons pratiques vous pouvez considérer que ce sont des projets différents.
</p><p>Les développeurs PyTorch offrent aussi <a class="mw-selflink-fragment" href="#LibTorch">LibTorch</a> qui permet d'implémenter des extensions à PyTorch à l'aide de C++ et d'implémenter des applications d'apprentissage machine en C++ pur. Les modèles Python écrits avec PyTorch peuvent être convertis et utilisés en C++ avec <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/advanced/cpp_export.html">TorchScript</a>.
</p>
<h1><span class="mw-headline" id="Installation">Installation</span></h1>
<h2><span id="Wheels_r.C3.A9cemment_ajout.C3.A9s"></span><span class="mw-headline" id="Wheels_récemment_ajoutés">Wheels récemment ajoutés</span></h2>
<p>Pour connaître la dernière version de PyTorch, utilisez
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=avail_wheels+torch" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>avail_wheels<span class="w"> </span>torch
</pre></div>
</div>
</div>
<p>Pour plus d'information, voyez <a href="/wiki/Python/fr#Wheels_disponibles" title="Python/fr">Wheels disponibles</a>.
</p><p><span id="Installing_our_wheel"></span>
</p>
<h2><span class="mw-headline" id="Installation_du_wheel">Installation du wheel</span></h2>
<p>La meilleure option est d'installer avec <a rel="nofollow" class="external text" href="https://pythonwheels.com/">Python wheels</a> comme suit&#160;:
</p>
<dl><dd><dl><dd>1. <a href="/wiki/Utiliser_des_modules#Sous-commande_load" title="Utiliser des modules">Chargez un module</a>  Python avec <code>module load python</code>.</dd>
<dd>2. Créez et démarrez un  <a href="/wiki/Python/fr#Créer_et_utiliser_un_environnement_virtuel" title="Python/fr">environnement virtuel</a>.</dd>
<dd>3. Installez PyTorch dans l'environnement virtuel avec <code>pip install</code>.</dd></dl></dd></dl>
<h4><span class="mw-headline" id="GPU_et_CPU">GPU et CPU</span></h4>
<dl><dd><div></div></dd></dl>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=pip+install+--no-index+torch" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">(</span>venv<span class="o">)</span><span class="w"> </span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-index<span class="w"> </span>torch
</pre></div>
</div>

<table class="wikitable" width="600px">
<tbody><tr>
<td>
<p><span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Light-bulb.png" class="mw-file-description"><img src="/mediawiki/images/8/8b/Light-bulb.png" decoding="async" width="25" height="25" class="mw-file-element" /></a></span>With H100 gpus, torch 2.3 and higher is required.
</p>
</td>
</tr>
</tbody></table>
<p><br />
<b>Remarque&#160;: </b>PyTorch 1.10 cause des problèmes connus sur nos grappes (à l'exception de Narval). Si l'entraînement distribué produit des erreurs ou si vous obtenez une erreur qui inclut <code>c10::Error</code>, nous vous recommandons d'installer PyTorch 1.9.1 avec <code>pip install --no-index torch==1.9.1</code>.
</p>
<h4><span id="En_suppl.C3.A9ment"></span><span class="mw-headline" id="En_supplément">En supplément</span></h4>
<p>En plus de  <code>torch</code>, vous pouvez aussi installer <code>torchvision</code>, <code>torchtext</code> et <code>torchaudio</code>.
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=pip+install+--no-index+torch+torchvision+torchtext+torchaudio" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">(</span>venv<span class="o">)</span><span class="w"> </span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-index<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchtext<span class="w"> </span>torchaudio
</pre></div>
</div>
</div>
<p><span id="Job_submission"></span>
</p>
<h1><span id="Soumettre_une_t.C3.A2che"></span><span class="mw-headline" id="Soumettre_une_tâche">Soumettre une tâche</span></h1>
<p>Le script suivant est un exemple de soumission d'une tâche utilisant le wheel Python avec un environnement virtuel.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-test.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--gres%3Dgpu%3A1+++++++%23+Request+GPU+%22generic+resources%22%0A%23SBATCH+--cpus-per-task%3D6++%23+Cores+proportional+to+GPUs%3A+6+on+Cedar%2C+16+on+Graham.%0A%23SBATCH+--mem%3D32000M+++++++%23+Memory+proportional+to+GPUs%3A+32000+Cedar%2C+64000+Graham.%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python%2F%3Cselect+version%3E+%23+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+--no-index%0A%0Apython+pytorch-test.py" />
<input type="hidden" name="filename" value="pytorch-test.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --gres=gpu:1       # Request GPU &quot;generic resources&quot;</span>
<span class="c1">#SBATCH --cpus-per-task=6  # Cores proportional to GPUs: 6 on Cedar, 16 on Graham.</span>
<span class="c1">#SBATCH --mem=32000M       # Memory proportional to GPUs: 32000 Cedar, 64000 Graham.</span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python/&lt;<span class="k">select</span><span class="w"> </span>version&gt;<span class="w"> </span><span class="c1"># Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>--no-index

python<span class="w"> </span>pytorch-test.py
</pre></div>
</div>
<p><br />
</p><p>Le script Python <code>pytorch-ddp-test.py</code> a la forme suivante&#160;:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-test.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+torch%0Ax+%3D+torch.Tensor%285%2C+3%29%0Aprint%28x%29%0Ay+%3D+torch.rand%285%2C+3%29%0Aprint%28y%29%0A%23+let+us+run+the+following+only+if+CUDA+is+available%0Aif+torch.cuda.is_available%28%29%3A%0A++++x+%3D+x.cuda%28%29%0A++++y+%3D+y.cuda%28%29%0A++++print%28x+%2B+y%29" />
<input type="hidden" name="filename" value="pytorch-test.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="c1"># let us run the following only if CUDA is available</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p><br />
</p><p>Vous pouvez alors soumettre une tâche PyTorch avec
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=sbatch+pytorch-test.sh" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="o">[</span>name@server<span class="w"> </span>~<span class="o">]</span>$<span class="w"> </span>sbatch<span class="w"> </span>pytorch-test.sh
</pre></div>
</div>
</div>
<p><span id="High_performance_with_PyTorch"></span>
</p>
<h1><span class="mw-headline" id="Haute_performance">Haute performance</span></h1>
<p><span id="TF32:_Performance_vs_numerical_accuracy"></span>
</p>
<h2><span id="TF32_:_Performance_vs_pr.C3.A9cision"></span><span class="mw-headline" id="TF32_:_Performance_vs_précision">TF32&#160;: Performance vs précision</span></h2>
<p>Avec sa version 1.7.0, PyTorch a ajouté le support pour le <a rel="nofollow" class="external text" href="https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/">mode TensorFloat-32 (TF32) de Nvidia</a> et est seulement disponible pour les architectures GPU d'Ampere et de Nvidia. Avec ce mode qui est offert par défaut dans les versions 1.7.x à 1.11.x, les opérations tensorielles se font jusqu'à 20x plus rapidement que les opérations équivalentes en simple précision (FP32). Cependant, ce gain en performance peut engendrer une baisse dans la précision du résultat des opérations, ce qui pose problème avec les modèles d'apprentissage profond qui utilisent à l'occasion des matrices mal conditionnées ou qui effectuent de longues séquences d'opérations tensorielles. Suite aux commentaires de la communauté des utilisateurs, TF32 est <b>désactivé par défaut pour les multiplications matricielle et activé par défaut pour les convolutions</b> à partir de la version 1.12.0.
</p><p>En date d'octobre 2022, notre seule grappe qui offre des GPU Ampere est <a href="/wiki/Narval" title="Narval">Narval</a>. Quand vous utilisez PyTorch sur Narval,
</p>
<ul><li>Vous pourriez remarquer un fort ralentissement dans l'exécution sur GPU du même code avec <code>torch &lt; 1.12.0</code> et <code>torch &gt;= 1.12.0</code>.</li>
<li>Vous pourriez obtenir des résultats différents dans l'exécution sur GPU du même code avec <code>torch &lt; 1.12.0</code> et <code>torch &gt;= 1.12.0</code>.</li></ul>
<p>Pour activer ou désactiver TF32 pour <code>torch &gt;= 1.12.0</code>, donnez la valeur <code>True</code> ou <code>False</code> aux indicateurs suivants&#160;:
</p>
<pre>torch.backends.cuda.matmul.allow_tf32 = False # Enable/disable TF32 for matrix multiplications
torch.backends.cudnn.allow_tf32 = False # Enable/disable TF32 for convolutions
</pre>
<p>Pour plus d'information, consultez <a rel="nofollow" class="external text" href="https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere">ce paragraphe de la documentation PyTorch</a>.
</p><p><span id="PyTorch_with_multiple_CPUs"></span>
</p>
<h2><span class="mw-headline" id="Travailler_avec_plusieurs_CPU">Travailler avec plusieurs CPU</span></h2>
<p>Par défaut, PyTorch permet le parallélisme avec plusieurs CPU de deux façons&#160;:
</p>
<ul><li><b>intra-op</b>, par l’implémentation parallèle d’opérateurs souvent utilisés en apprentissage profond comme le produit matriciel ou le produit de convolution, en utilisant <a rel="nofollow" class="external text" href="https://www.openmp.org">OpenMP</a> directement ou avec des bibliothèques de bas niveau comme <a rel="nofollow" class="external text" href="https://en.wikipedia.org/wiki/Math_Kernel_Library">MKL</a> et <a rel="nofollow" class="external text" href="https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top/api-based-programming/intel-oneapi-deep-neural-network-library-onednn.html">OneDNN</a>. Quand du code PyTorch doit effectuer de telles opérations, elles utilisent automatiquement de multiples fils avec tous les cœurs CPU disponibles.</li>
<li><b>inter-op</b>, par la capacité d’exécuter différentes parties de code de manière concurrente. Ce mode de parallélisme nécessite habituellement que le programme soit conçu de manière à exécuter plusieurs parties en parallèle, par exemple en faisant usage du compilateur en temps réel <tt>torch.jit</tt> pour exécuter des tâches asynchrones dans un programme <a rel="nofollow" class="external text" href="https://pytorch.org/docs/stable/jit.html#built-in-functions-and-modules">TorchScript</a>.</li></ul>
<p>Pour les petits modèles, nous recommandons fortement <b>d’utiliser plusieurs CPU plutôt qu’un GPU</b>. L’entraînement sera certainement plus rapide avec un GPU (sauf dans les cas de très petits modèles), mais si le modèle et le jeu de données ne sont pas assez grands, la vitesse gagnée avec le GPU ne sera probablement pas très importante et la tâche n’utilisera qu’une petite part de la capacité de calcul. Ce n’est peut-être pas grave sur votre propre ordinateur, mais dans un environnement partagé comme sur nos grappes, vous bloqueriez une ressource qui pourrait servir à effectuer de calculs de grande échelle par un autre projet. De plus, l’utilisation d’un GPU contribuerait à la diminution de l’allocation de votre groupe et aurait une incidence sur la priorité accordée aux tâches de vos collègues.
</p><p>Dans le code suivant, il y a plusieurs occasions d’utiliser le parallélisme intra-op. En demandant plus de CPU et sans changer le code, on peut constater l’effet sur la performance.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-multi-cpu.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--tasks-per-node%3D1+%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+to+see+the+effect+on+performance%0A%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A05%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0Aecho+%22starting+training...%22%0A%0Atime+python+cifar10-cpu.py" />
<input type="hidden" name="filename" value="pytorch-multi-cpu.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --tasks-per-node=1 </span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... to see the effect on performance</span>

<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:05:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>

<span class="nb">time</span><span class="w"> </span>python<span class="w"> </span>cifar10-cpu.py
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> cifar10-cpu.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+numpy+as+np%0Aimport+time%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0Aimport+os%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+cpu+performance+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A++++torch.set_num_threads%28int%28os.environ%5B%27SLURM_CPUS_PER_TASK%27%5D%29%29%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++%23%23%23+This+next+line+will+attempt+to+download+the+CIFAR10+dataset+from+the+internet+if+you+don%27t+already+have+it+stored+in+.%2Fdata+%0A++++%23%23%23+Run+this+line+on+a+login+node+with+%22download%3DTrue%22+prior+to+submitting+your+job%2C+or+manually+download+the+data+from+%0A++++%23%23%23+https%3A%2F%2Fwww.cs.toronto.edu%2F%7Ekriz%2Fcifar-10-python.tar.gz+and+place+it+under+.%2Fdata%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="cifar10-cpu.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, cpu performance test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_CPUS_PER_TASK&#39;</span><span class="p">]))</span>
    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="c1">### This next line will attempt to download the CIFAR10 dataset from the internet if you don&#39;t already have it stored in ./data </span>
    <span class="c1">### Run this line on a login node with &quot;download=True&quot; prior to submitting your job, or manually download the data from </span>
    <span class="c1">### https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and place it under ./data</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p><p><span id="PyTorch_with_a_single_GPU"></span>
</p>
<h2><span class="mw-headline" id="Travailler_avec_un_seul_GPU">Travailler avec un seul GPU</span></h2>
<p>On entend souvent dire qu’il faut absolument entraîner un modèle avec un GPU s’il y en a un à notre disposition. Ceci est <i>presque toujours</i> vrai (l'entraînement de très petits modèles est souvent plus rapide avec un ou plusieurs CPU) sur un poste de travail local, mais ce n’est pas le cas sur nos grappes.
</p><p>Autrement dit, vous <b>ne devriez pas demander un GPU</b> si votre code ne peut pas faire un usage raisonnable de sa capacité de calcul.
</p><p>La performance avantageuse des GPU pour les tâches d’apprentissage profond provient de deux sources&#160;:
</p>
<ol><li>La capacité de paralléliser l’exécution de certaines opérations clés, par exemple le <a rel="nofollow" class="external text" href="https://fr.wikipedia.org/wiki/Multiplieur-accumulateur">multiplieur-accumulateur</a>, sur plusieurs milliers de cœurs de calcul, en comparaison du très petit nombre de cœurs disponibles avec la plupart des CPU.</li>
<li>Une bande passante de mémoire beaucoup plus grande que pour un CPU, ce qui permet aux GPU d’utiliser efficacement leur très grand nombre de cœurs pour traiter une plus grande quantité de données par cycle de calcul.</li></ol>
<p>Comme c’est le cas avec plusieurs CPU, PyTorch offre des implémentations parallèles  d’opérateurs souvent utilisés en apprentissage profond, comme le produit matriciel et le produit de convolution et utilise des bibliothèques spécialisées pour les GPU comme <a rel="nofollow" class="external text" href="https://developer.nvidia.com/cudnn">CUDNN</a> ou <a rel="nofollow" class="external text" href="https://github.com/ROCmSoftwarePlatform/MIOpen">MIOpen</a>, selon la plateforme matérielle. Ceci signifie que pour qu’il vaille la peine d’utiliser un GPU pour une tâche d’apprentissage, elle doit être composée d’éléments qui peuvent être élargis à une application massive du parallélisme de par le nombre d’opérations pouvant être parallélisées, de par la quantité des données à traiter ou idéalement de par les deux. Un exemple concret serait un grand modèle qui a un grand nombre d’unités et de couches ou qui a beaucoup de données en entrée, et idéalement qui présente ces deux caractéristiques.
</p><p>Dans l’exemple ci-dessous, nous adaptons le code de la section précédente pour utiliser un GPU et nous examinons la performance. Nous observons que deux paramètres jouent un rôle important&#160;: <code>batch_size</code> et <code>num_workers</code>. Le premier paramètre améliore la performance en augmentant la taille des entrées à chaque itération et en utilisant mieux la capacité du GPU. Dans le cas du second paramètre, la performance est améliorée en facilitant  le mouvement des données entre la mémoire de l’hôte (le CPU) et la mémoire du GPU, ce qui réduit la durée d’inactivité du GPU en attente de données à traiter.
</p><p>Nous pouvons tirer deux conclusions&#160;:
</p>
<ol><li>Augmenter la valeur de <code>batch_size</code> au maximum qu’il est possible pour la mémoire du GPU optimise la performance.</li>
<li>Utiliser un <code>DataLoader</code> avec autant de workers que <code>cpus-per-task</code> facilite l’apport de données au GPU.</li></ol>
<p>Bien entendu, le paramètre <code>batch_size</code> a aussi un impact sur la performance d’un modèle dans une tâche(c.à.d. l’exactitude, l’erreur, etc.) et il existe différentes écoles de pensée sur l’utilisation de grands lots. Nous n’abordons pas le sujet ici, mais si vous croyez qu’un petit lot conviendrait mieux à votre application, allez à la section <a class="mw-selflink-fragment" href="#Travailler_avec_un_seul_GPU">Travailler avec un seul GPU</a>  pour savoir comment maximiser l’utilisation du GPU avec de petites entrées de données.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-single-gpu.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--gres%3Dgpu%3A1+%23+request+a+GPU%0A%23SBATCH+--tasks-per-node%3D1+%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A05%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0Aecho+%22starting+training...%22%0Atime+python+cifar10-gpu.py+--batch_size%3D512+--num_workers%3D0" />
<input type="hidden" name="filename" value="pytorch-single-gpu.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --gres=gpu:1 # request a GPU</span>
<span class="c1">#SBATCH --tasks-per-node=1 </span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:05:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>
<span class="nb">time</span><span class="w"> </span>python<span class="w"> </span>cifar10-gpu.py<span class="w"> </span>--batch_size<span class="o">=</span><span class="m">512</span><span class="w"> </span>--num_workers<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> cifar10-gpu.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+numpy+as+np%0Aimport+time%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+single+gpu+performance+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29.cuda%28%29+%23+Load+model+on+the+GPU%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29+%23+Load+the+loss+function+on+the+GPU%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A+++++++%0A+++++++inputs+%3D+inputs.cuda%28%29+%0A+++++++targets+%3D+targets.cuda%28%29%0A%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="cifar10-gpu.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, single gpu performance test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="c1"># Load model on the GPU</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="c1"># Load the loss function on the GPU</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
       
       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> 
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p><p><span id="Data_parallelism_with_a_single_GPU"></span>
</p>
<h3><span id="Parall.C3.A9lisme_des_donn.C3.A9es_avec_un_seul_GPU"></span><span class="mw-headline" id="Parallélisme_des_données_avec_un_seul_GPU">Parallélisme des données avec un seul GPU</span></h3>
<p>Il <b>n’est pas conseillé d’utiliser un GPU</b> avec un modèle de taille relativement petite qui n’utilise pas une grande part de la mémoire du GPU et une part raisonnable de sa capacité de calcul; utilisez plutôt <a class="mw-selflink-fragment" href="#Travailler_avec_plusieurs_CPU">un ou plusieurs CPU</a>. Par contre, profiter du parallélisme du GPU devient une bonne option si vous avez un tel modèle avec un très grand jeu de données et que vous voulez effectuer l’entraînement avec des lots de petite taille. 
</p><p>Dans ce contexte, la parallélisation des données réfère à des méthodes pour entraîner en parallèle plusieurs copies d’un modèle où chaque copie reçoit un morceau des données d’entraînement à chaque itération. À la fin d’une itération, les gradients sont agrégés et les paramètres de chaque copie sont mis à jour de façon synchrone ou asynchrone, dépendant de la méthode. Cette approche peut augmenter la vitesse d’exécution de façon importante avec une itération qui se fait environ N fois plus rapidement avec un grand jeu de données, N étant le nombre de copies du modèle. Pour utiliser cette approche, <b>un avertissement s’impose</b>&#160;:  pour que le modèle entraîné soit équivalent au même modèle entraîné sans parallélisme, vous devez adapter le taux d’apprentissage ou la taille du lot désirée en fonction du nombre de copies. Pour plus d’information, voyez <a rel="nofollow" class="external text" href="https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/13">ces échanges</a>.  
</p><p>PyTorch offre des implémentations de méthodes de parallélisme des données, la classe <code>DistributedDataParallel</code> étant celle <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel">recommandée par les développeurs de PyTorch</a> pour donner la meilleure performance. Conçue pour le <a class="mw-selflink-fragment" href="#Travailler_avec_plusieurs_GPU">travail avec plusieurs GPU</a>, elle peut aussi être employée avec un seul GPU.
</p><p>Dans l’exemple ci-dessous, nous adaptons le code pour un seul GPU pour utiliser le parallélisme des données. La tâche est relativement petite; la taille du lot est de 512 images, le modèle occupe environ 1Go de la mémoire du GPU et l’entraînement n’utilise qu’environ 6&#160;% de sa capacité de calcul. Ce modèle <b>ne devrait pas</b> être entraîné sur un GPU sur nos grappes. Cependant, en parallélisant les données, un GPU V100 avec 16Go de mémoire peut contenir 14 ou 15 copies du modèle et augmenter l'utilisation de la ressource en plus d’obtenir une bonne augmentation de vitesse. Nous utilisons <a rel="nofollow" class="external text" href="https://docs.nvidia.com/deploy/mps/index.html">Multi-Process Service (MPS) de NVIDIA</a> avec <a href="/wiki/MPI/fr" title="MPI/fr">MPI</a> pour placer plusieurs copies du modèle sur un GPU de façon efficace.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-gpu-mps.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--gres%3Dgpu%3A1+%23+request+a+GPU%0A%23SBATCH+--tasks-per-node%3D8+%23+This+is+the+number+of+model+replicas+we+will+place+on+the+GPU.+Change+this+to+10%2C12%2C14%2C...+to+see+the+effect+on+performance++%0A%23SBATCH+--cpus-per-task%3D1+%23+increase+this+parameter+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A05%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0A%23+Activate+Nvidia+MPS%3A%0Aexport+CUDA_MPS_PIPE_DIRECTORY%3D%2Ftmp%2Fnvidia-mps%0Aexport+CUDA_MPS_LOG_DIRECTORY%3D%2Ftmp%2Fnvidia-log%0Anvidia-cuda-mps-control+-d%0A%0Aecho+%22starting+training...%22%0Atime+srun+--cpus-per-task%3D%24SLURM_CPUS_PER_TASK+python+cifar10-gpu-mps.py+--batch_size%3D512+--num_workers%3D0" />
<input type="hidden" name="filename" value="pytorch-gpu-mps.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --gres=gpu:1 # request a GPU</span>
<span class="c1">#SBATCH --tasks-per-node=8 # This is the number of model replicas we will place on the GPU. Change this to 10,12,14,... to see the effect on performance  </span>
<span class="c1">#SBATCH --cpus-per-task=1 # increase this parameter and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:05:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="c1"># Activate Nvidia MPS:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_MPS_PIPE_DIRECTORY</span><span class="o">=</span>/tmp/nvidia-mps
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_MPS_LOG_DIRECTORY</span><span class="o">=</span>/tmp/nvidia-log
nvidia-cuda-mps-control<span class="w"> </span>-d

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>
<span class="nb">time</span><span class="w"> </span>srun<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span><span class="w"> </span>python<span class="w"> </span>cifar10-gpu-mps.py<span class="w"> </span>--batch_size<span class="o">=</span><span class="m">512</span><span class="w"> </span>--num_workers<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> cifar10-gpu-mps.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0Aimport+time%0Aimport+datetime%0Aimport+numpy+as+np%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+distributed+data+parallel+maps+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--init_method%27%2C+default%3D%27tcp%3A%2F%2F127.0.0.1%3A3456%27%2C+type%3Dstr%2C+help%3D%27%27%29%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++rank+%3D+os.environ.get%28%22SLURM_LOCALID%22%29%0A%0A++++current_device+%3D+0%0A++++torch.cuda.set_device%28current_device%29%0A%0A++++%22%22%22+this+block+initializes+a+process+group+and+initiate+communications%0A++++++++++++++++between+all+processes+that+will+run+a+model+replica+%22%22%22%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Initializing+Process+Group...%27.format%28rank%29%29%0A%0A++++dist.init_process_group%28backend%3D%22mpi%22%2C+init_method%3Dargs.init_method%29+%23+Use+backend%3D%22mpi%22+or+%22gloo%22.+NCCL+does+not+work+on+a+single+GPU+due+to+a+hard-coded+multi-GPU+topology+check.%0A++++print%28%22process+group+ready%21%22%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Making+model..%27.format%28rank%29%29%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++net.cuda%28%29%0A++++net+%3D+torch.nn.parallel.DistributedDataParallel%28net%2C+device_ids%3D%5Bcurrent_device%5D%29+%23+Wrap+the+model+with+DistributedDataParallel%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Preparing+data..%27.format%28rank%29%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27%7E%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A+++++++%0A+++++++inputs+%3D+inputs.cuda%28%29+%0A+++++++targets+%3D+targets.cuda%28%29%0A%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="cifar10-gpu-mps.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, distributed data parallel maps test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--init_method&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;tcp://127.0.0.1:3456&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_LOCALID&quot;</span><span class="p">)</span>

    <span class="n">current_device</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; this block initializes a process group and initiate communications</span>
<span class="sd">                between all processes that will run a model replica &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Initializing Process Group...&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;mpi&quot;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">)</span> <span class="c1"># Use backend=&quot;mpi&quot; or &quot;gloo&quot;. NCCL does not work on a single GPU due to a hard-coded multi-GPU topology check.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;process group ready!&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Making model..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">net</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">current_device</span><span class="p">])</span> <span class="c1"># Wrap the model with DistributedDataParallel</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Preparing data..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;~/data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
       
       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> 
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p><p><span id="PyTorch_with_multiple_GPUs"></span>
</p>
<h2><span class="mw-headline" id="Travailler_avec_plusieurs_GPU">Travailler avec plusieurs GPU</span></h2>
<p><span id="Issue_with_DistributedDataParallel_and_PyTorch_1.10"></span>
</p>
<h3><span id="Probl.C3.A8me_avec_DistributedDataParallel_et_PyTorch_1.10"></span><span class="mw-headline" id="Problème_avec_DistributedDataParallel_et_PyTorch_1.10">Problème avec DistributedDataParallel et PyTorch 1.10</span></h3>
<p>Avec notre wheel PyTorch 1.10 <code>torch-1.10.0+computecanada</code>, le code pour travailler avec plusieurs GPU et qui utilise 
<a class="mw-selflink-fragment" href="#DistributedDataParallel">DistributedDataParallel</a> pourrait échouer de façon imprévisible si le backend est défini comme étant <code>'nccl'</code> ou <code>'gloo'</code>. Nous vous recommandons d'utiliser la version la plus récente de PyTorch plutôt que la version 1.10 sur toutes les grappes d'usage général.
</p>
<h3><span id="Parall.C3.A9liser_les_donn.C3.A9es_avec_plusieurs_GPU"></span><span class="mw-headline" id="Paralléliser_les_données_avec_plusieurs_GPU">Paralléliser les données avec plusieurs GPU</span></h3>
<p>Dans ce contexte, la parallélisation des données réfère à des méthodes pour entraîner en parallèle plusieurs copies d’un modèle où chaque copie reçoit une portion des données d’entraînement à chaque itération. À la fin d’une itération, les gradients sont agrégés et les paramètres de chaque copie sont mis à jour de façon synchrone ou asynchrone, dépendant de la méthode. Cette approche peut augmenter la vitesse d’exécution de façon importante avec une itération qui se fait environ N fois plus rapidement avec un grand jeu de données, N étant le nombre de copies du modèle. Pour utiliser cette approche, un avertissement s’impose&#160;:  pour que le modèle entraîné soit équivalent au même modèle entraîné sans parallélisme, vous devez adapter le taux d’apprentissage ou la taille du lot désirée en fonction du nombre de copies. Pour plus d’information, voyez <a rel="nofollow" class="external text" href="https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/13">ces échanges</a>.
Quand plusieurs GPU sont utilisés, chacun reçoit une copie du modèle; il doit donc être assez petit pour être contenu dans la mémoire d’un GPU. Pour entraîner un modèle qui dépasse la quantité de mémoire d’un GPU, voyez la section <a class="mw-selflink-fragment" href="#Paralléliser_un_modèle_avec_plusieurs_GPU">Paralléliser un modèle avec plusieurs GPU</a>.
</p><p>Il existe plusieurs manières de paralléliser les données avec PyTorch. Nous présentons ici des tutoriels avec la classe <b>DistributedDataParallel</b>, avec le paquet  <b>PyTorch Lightning</b> et avec le paquet <b>Horovod</b>.
</p><p><span id="Using_DistributedDataParallel"></span>
</p>
<h4><span class="mw-headline" id="DistributedDataParallel">DistributedDataParallel</span></h4>
<p>Avec plusieurs GPU, la classe <b>DistributedDataParallel</b> est  <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel">recommandée par les développeurs PyTorch</a>, que ce soit avec un nœud unique ou avec plusieurs nœuds. Dans le cas qui suit, plusieurs GPU sont distribués sur deux nœuds.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2+++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Asrun+-N+%24SLURM_NNODES+-n+%24SLURM_NNODES+bash+%3C%3C+EOF%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0AEOF%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0Aexport+MASTER_ADDR%3D%24%28hostname%29+%23Store+the+master+node%E2%80%99s+IP+address+in+the+MASTER_ADDR+environment+variable.%0A%0Aecho+%22r%24SLURM_NODEID+master%3A+%24MASTER_ADDR%22%0Aecho+%22r%24SLURM_NODEID+Launching+python+script%22%0A%0A%23+The+%24%28%28SLURM_NTASKS_PER_NODE+%2A+SLURM_JOB_NUM_NODES%29%29+variable+tells+the+script+how+many+processes+are+available+for+this+execution.+%E2%80%9Csrun%E2%80%9D+executes+the+script+%3Ctasks-per-node+%2A+nodes%3E+times%0A%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0A%0Asrun+python+pytorch-ddp-test.py+--init_method+tcp%3A%2F%2F%24MASTER_ADDR%3A3456+--world_size+%24%28%28SLURM_NTASKS_PER_NODE+%2A+SLURM_JOB_NUM_NODES%29%29++--batch_size+256" />
<input type="hidden" name="filename" value="pytorch-ddp-test.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resources”.</span>
<span class="c1">#SBATCH --tasks-per-node=2   # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
srun<span class="w"> </span>-N<span class="w"> </span><span class="nv">$SLURM_NNODES</span><span class="w"> </span>-n<span class="w"> </span><span class="nv">$SLURM_NNODES</span><span class="w"> </span>bash<span class="w"> </span><span class="s">&lt;&lt; EOF</span>
<span class="s">virtualenv --no-download $SLURM_TMPDIR/env</span>
<span class="s">source $SLURM_TMPDIR/env/bin/activate</span>
<span class="s">pip install torch torchvision --no-index</span>
<span class="s">EOF</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span><span class="w"> </span><span class="c1">#Store the master node’s IP address in the MASTER_ADDR environment variable.</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;r</span><span class="nv">$SLURM_NODEID</span><span class="s2"> master: </span><span class="nv">$MASTER_ADDR</span><span class="s2">&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;r</span><span class="nv">$SLURM_NODEID</span><span class="s2"> Launching python script&quot;</span>

<span class="c1"># The $((SLURM_NTASKS_PER_NODE * SLURM_JOB_NUM_NODES)) variable tells the script how many processes are available for this execution. “srun” executes the script &lt;tasks-per-node * nodes&gt; times</span>

<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate

srun<span class="w"> </span>python<span class="w"> </span>pytorch-ddp-test.py<span class="w"> </span>--init_method<span class="w"> </span>tcp://<span class="nv">$MASTER_ADDR</span>:3456<span class="w"> </span>--world_size<span class="w"> </span><span class="k">$((</span><span class="nv">SLURM_NTASKS_PER_NODE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">SLURM_JOB_NUM_NODES</span><span class="k">))</span><span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p>Le script Python <code>pytorch-ddp-test.py</code> a la forme suivante&#160;:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0Aimport+time%0Aimport+datetime%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0Aimport+torch.backends.cudnn+as+cudnn%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+distributed+data+parallel+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D4%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0Aparser.add_argument%28%27--init_method%27%2C+default%3D%27tcp%3A%2F%2F127.0.0.1%3A3456%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--dist-backend%27%2C+default%3D%27nccl%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--world_size%27%2C+default%3D1%2C+type%3Dint%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--distributed%27%2C+action%3D%27store_true%27%2C+help%3D%27%27%29%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++ngpus_per_node+%3D+torch.cuda.device_count%28%29%0A%0A++++%22%22%22+This+next+line+is+the+key+to+getting+DistributedDataParallel+working+on+SLURM%3A%0A%09%09SLURM_NODEID+is+0+or+1+in+this+example%2C+SLURM_LOCALID+is+the+id+of+the+%0A+%09%09current+process+inside+a+node+and+is+also+0+or+1+in+this+example.%22%22%22%0A%0A++++local_rank+%3D+int%28os.environ.get%28%22SLURM_LOCALID%22%29%29+%0A++++rank+%3D+int%28os.environ.get%28%22SLURM_NODEID%22%29%29%2Angpus_per_node+%2B+local_rank%0A%0A++++current_device+%3D+local_rank%0A%0A++++torch.cuda.set_device%28current_device%29%0A%0A++++%22%22%22+this+block+initializes+a+process+group+and+initiate+communications%0A%09%09between+all+processes+running+on+all+nodes+%22%22%22%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Initializing+Process+Group...%27.format%28rank%29%29%0A++++%23init+the+process+group%0A++++dist.init_process_group%28backend%3Dargs.dist_backend%2C+init_method%3Dargs.init_method%2C+world_size%3Dargs.world_size%2C+rank%3Drank%29%0A++++print%28%22process+group+ready%21%22%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Making+model..%27.format%28rank%29%29%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++net.cuda%28%29%0A++++net+%3D+torch.nn.parallel.DistributedDataParallel%28net%2C+device_ids%3D%5Bcurrent_device%5D%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Preparing+data..%27.format%28rank%29%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%2C+momentum%3D0.9%2C+weight_decay%3D1e-4%29%0A%0A++++for+epoch+in+range%28args.max_epochs%29%3A%0A%0A++++++++train_sampler.set_epoch%28epoch%29%0A%0A++++++++train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+rank%29%0A%0Adef+train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+train_rank%29%3A%0A%0A++++train_loss+%3D+0%0A++++correct+%3D+0%0A++++total+%3D+0%0A%0A++++epoch_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++inputs+%3D+inputs.cuda%28%29%0A+++++++targets+%3D+targets.cuda%28%29%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++train_loss+%2B%3D+loss.item%28%29%0A+++++++_%2C+predicted+%3D+outputs.max%281%29%0A+++++++total+%2B%3D+targets.size%280%29%0A+++++++correct+%2B%3D+predicted.eq%28targets%29.sum%28%29.item%28%29%0A+++++++acc+%3D+100+%2A+correct+%2F+total%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++elapse_time+%3D+time.time%28%29+-+epoch_start%0A+++++++elapse_time+%3D+datetime.timedelta%28seconds%3Delapse_time%29%0A+++++++print%28%22From+Rank%3A+%7B%7D%2C+Training+time+%7B%7D%22.format%28train_rank%2C+elapse_time%29%29%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-ddp-test.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="k">as</span> <span class="nn">cudnn</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, distributed data parallel test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--init_method&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;tcp://127.0.0.1:3456&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--dist-backend&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--world_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--distributed&#39;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">ngpus_per_node</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; This next line is the key to getting DistributedDataParallel working on SLURM:</span>
<span class="sd">		SLURM_NODEID is 0 or 1 in this example, SLURM_LOCALID is the id of the </span>
<span class="sd"> 		current process inside a node and is also 0 or 1 in this example.&quot;&quot;&quot;</span>

    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_LOCALID&quot;</span><span class="p">))</span> 
    <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_NODEID&quot;</span><span class="p">))</span><span class="o">*</span><span class="n">ngpus_per_node</span> <span class="o">+</span> <span class="n">local_rank</span>

    <span class="n">current_device</span> <span class="o">=</span> <span class="n">local_rank</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; this block initializes a process group and initiate communications</span>
<span class="sd">		between all processes running on all nodes &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Initializing Process Group...&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
    <span class="c1">#init the process group</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;process group ready!&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Making model..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">net</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">current_device</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Preparing data..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>

        <span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_rank</span><span class="p">):</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">epoch_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
       <span class="n">total</span> <span class="o">+=</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
       <span class="n">correct</span> <span class="o">+=</span> <span class="n">predicted</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start</span>
       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">elapse_time</span><span class="p">)</span>
       <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From Rank: </span><span class="si">{}</span><span class="s2">, Training time </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_rank</span><span class="p">,</span> <span class="n">elapse_time</span><span class="p">))</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="PyTorch_Lightning">PyTorch Lightning</span></h4>
<p>Ce paquet fournit des interfaces à PyTorch afin de simplifier plusieurs tâches communes exigeant beaucoup de code; ceci inclut les tâches d'entraînement de modèles avec plusieurs GPU. Dans le tutoriel suivant pour PyTorch Lightning, nous reprenons le même exemple que ci-dessus, mais sans avoir explicitement recours à la classe DistributedDataParallel.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test-pl.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+pytorch-ddp-test-pl.py++--batch_size+256" />
<input type="hidden" name="filename" value="pytorch-ddp-test-pl.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resources”.</span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>pytorch-ddp-test-pl.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-ddp-test-pl.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+datetime%0A%0Aimport+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+pytorch-lightning+parallel+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D4%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+torch.optim.Adam%28self.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPUs+per+node.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3D%27ddp%27%2C+max_epochs+%3D+args.max_epochs%2C+enable_progress_bar%3DFalse%29+%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-ddp-test-pl.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">datetime</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, pytorch-lightning parallel test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPUs per node.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;ddp&#39;</span><span class="p">,</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">,</span> <span class="n">enable_progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="Horovod">Horovod</span></h4>
<p><a rel="nofollow" class="external text" href="https://horovod.readthedocs.io/en/latest/summary_include.html">Horovod</a> est une plateforme d'entraînement distribué pour l'apprentissage profond, compatible avec TensorFlow, Keras, PyTorch et Apache MXNet. Son API vous permet d'avoir le même niveau de contrôle sur votre code d'entraînement qu'avec <code>DistributedDataParallel</code>, mais simplifie l'écriture de vos scripts en éliminant le besoin de configurer directement les groupes de processus et en prenant en charge les variables d'environnement de l'ordonnanceur. Horovod offre aussi des optimiseurs distribués qui dans certains cas améliorent la performance. L'exemple suivant est le même que le précédent, cette fois avec Horovod.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch_horovod.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2+++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%0A%23SBATCH+--tasks-per-node%3D2+++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0-03%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+horovod+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0Asrun+python+pytorch_horovod.py++--batch_size+256" />
<input type="hidden" name="filename" value="pytorch_horovod.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1            </span>
<span class="c1">#SBATCH --gres=gpu:2         # Request 2 GPU &quot;generic resources”.</span>

<span class="c1">#SBATCH --tasks-per-node=2   # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>

<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0-03:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>horovod<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

srun<span class="w"> </span>python<span class="w"> </span>pytorch_horovod.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch_horovod.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0Aimport+time%0Aimport+datetime%0Aimport+numpy+as+np%0Aimport+horovod.torch+as+hvd%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.nn.functional+as+F%0Aimport+torch.optim+as+optim%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+horovod+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++hvd.init%28%29%0A%0A++++print%28%22Starting...%22%29%0A%0A++++local_rank+%3D+hvd.local_rank%28%29%0A++++global_rank+%3D+hvd.rank%28%29%0A%0A++++torch.cuda.set_device%28local_rank%29%0A%0A%0A++++class+Net%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28F.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A++++++++++x+%3D+F.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+F.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A++++++++++return+x%0A%0A++++net+%3D+Net%28%29%0A%0A++++net.cuda%28%29%0A%0A++++print%28%27From+Rank%3A+%7B%7D%2C+%3D%3D%3E+Preparing+data..%27.format%28global_rank%29%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%2C+num_replicas%3Dhvd.size%28%29%2Crank%3Dglobal_rank%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28%29%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%2C+momentum%3D0.9%2C+weight_decay%3D1e-4%29%0A%0A++++optimizer+%3D+hvd.DistributedOptimizer%28optimizer%2C+named_parameters%3Dnet.named_parameters%28%29%29%0A%0A++++hvd.broadcast_parameters%28net.state_dict%28%29%2C+root_rank%3D0%29%0A%0A++++for+epoch+in+range%28args.max_epochs%29%3A%0A%0A++++++++train_sampler.set_epoch%28epoch%29%0A%0A++++++++train%28args%2Cepoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+global_rank%29%0A%0A%0Adef+train%28args%2Cepoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+train_rank%29%3A%0A%0A++++train_loss+%3D+0%0A++++correct+%3D+0%0A++++total+%3D+0%0A%0A++++epoch_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++inputs+%3D+inputs.cuda%28%29%0A+++++++targets+%3D+targets.cuda%28%29%0A+++++++outputs+%3D+net%28inputs%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A%0A+++++++train_loss+%2B%3D+loss.item%28%29%0A+++++++_%2C+predicted+%3D+outputs.max%281%29%0A+++++++total+%2B%3D+targets.size%280%29%0A+++++++correct+%2B%3D+predicted.eq%28targets%29.sum%28%29.item%28%29%0A+++++++acc+%3D+100+%2A+correct+%2F+total%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++elapse_time+%3D+time.time%28%29+-+epoch_start%0A+++++++elapse_time+%3D+datetime.timedelta%28seconds%3Delapse_time%29%0A+++++++print%28%22From+Rank%3A+%7B%7D%2C+Training+time+%7B%7D%22.format%28train_rank%2C+elapse_time%29%29%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch_horovod.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">horovod.torch</span> <span class="k">as</span> <span class="nn">hvd</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>


<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, horovod test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">local_rank</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>


    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

    <span class="n">net</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From Rank: </span><span class="si">{}</span><span class="s1">, ==&gt; Preparing data..&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">global_rank</span><span class="p">))</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span><span class="n">rank</span><span class="o">=</span><span class="n">global_rank</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>


    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">named_parameters</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>

    <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_parameters</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>

        <span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">global_rank</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_rank</span><span class="p">):</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">epoch_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

       <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
       <span class="n">total</span> <span class="o">+=</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
       <span class="n">correct</span> <span class="o">+=</span> <span class="n">predicted</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
       <span class="n">acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start</span>
       <span class="n">elapse_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">elapse_time</span><span class="p">)</span>
       <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From Rank: </span><span class="si">{}</span><span class="s2">, Training time </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_rank</span><span class="p">,</span> <span class="n">elapse_time</span><span class="p">))</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span id="Parall.C3.A9liser_un_mod.C3.A8le_avec_plusieurs_GPU"></span><span class="mw-headline" id="Paralléliser_un_modèle_avec_plusieurs_GPU">Paralléliser un modèle avec plusieurs GPU</span></h3>
<p>Quand un modèle est trop grand pour être contenu dans <a class="mw-selflink-fragment" href="#Travailler_avec_un_seul_GPU">un seul GPU</a>, vous pouvez le diviser en portions et charger chacune sur des GPU distincts. Dans l’exemple suivant, nous reprenons le code des sections précédentes pour illustrer le fonctionnement. Nous divisons un réseau de neurones convolutifs en deux&#160;: d’une part les couches convolutionnelles/de regroupement et d'autre part les couches acycliques entièrement connectées. La tâche demande deux GPU, chacun d’eux étant chargé avec une des parts.  De plus, nous ajoutons du code pour <a rel="nofollow" class="external text" href="https://pytorch.org/docs/stable/pipeline.html?highlight=pipeline">paralléliser les pipelines</a> et ainsi réduire au maximum le temps pendant lequel le deuxième GPU est inactif dans l’attente des résultats du premier. Pour ce faire, nous créons un <code>nn.Module</code> distinct pour chacune des portions du modèle, puis créons une séquence de modules en enveloppant les portions avec <code>nn.Sequential</code>, et ensuite utilisons <code>torch.distributed.pipeline.sync.Pipe</code> pour morceler chacun des lots en entrée et les passer en parallèle aux deux portions du modèle.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-modelpar-pipelined-rpc.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1%0A%23SBATCH+--gres%3Dgpu%3A2+%23+request+2+GPUs%0A%23SBATCH+--tasks-per-node%3D1+%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D8G++++++%0A%23SBATCH+--time%3D0%3A10%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0A%23+This+is+needed+to+initialize+pytorch%27s+RPC+module%2C+required+for+the+Pipe+class+which+we%27ll+use+for+Pipeline+Parallelism%0Aexport+MASTER_ADDR%3D%24%28hostname%29%0Aexport+MASTER_PORT%3D34567%0A+%0Aecho+%22starting+training...%22%0Atime+python+pytorch-modelpar-pipelined-rpc.py+--batch_size%3D512+--num_workers%3D0" />
<input type="hidden" name="filename" value="pytorch-modelpar-pipelined-rpc.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1</span>
<span class="c1">#SBATCH --gres=gpu:2 # request 2 GPUs</span>
<span class="c1">#SBATCH --tasks-per-node=1 </span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=8G      </span>
<span class="c1">#SBATCH --time=0:10:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="c1"># This is needed to initialize pytorch&#39;s RPC module, required for the Pipe class which we&#39;ll use for Pipeline Parallelism</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">34567</span>
<span class="w"> </span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>
<span class="nb">time</span><span class="w"> </span>python<span class="w"> </span>pytorch-modelpar-pipelined-rpc.py<span class="w"> </span>--batch_size<span class="o">=</span><span class="m">512</span><span class="w"> </span>--num_workers<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-modelpar-pipelined-rpc.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+time%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.optim+as+optim%0Afrom+torch.distributed.pipeline.sync+import+Pipe%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+single+node+model+parallelism+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D512%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++%23+Convolutional+%2B+pooling+part+of+the+model%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++torch.distributed.rpc.init_rpc%28%27worker%27%2C+rank%3D0%2C+world_size%3D1%29+%23+initializing+RPC+is+required+by+Pipe+we+use+below%0A%0A++++part1+%3D+ConvPart%28%29.to%28%27cuda%3A0%27%29+%23+Load+part1+on+the+first+GPU%0A++++part2+%3D+MLPPart%28%29.to%28%27cuda%3A1%27%29+%23+Load+part2+on+the+second+GPU%0A%0A++++net+%3D+nn.Sequential%28part1%2Cpart2%29+%23+Pipe+requires+all+modules+be+wrapped+with+nn.Sequential%28%29%0A%0A++++net+%3D+Pipe%28net%2C+chunks%3D32%29+%23+Wrap+with+Pipe+to+perform+Pipeline+Parallelism%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.to%28%27cuda%3A1%27%29+%23+Load+the+loss+function+on+the+last+GPU%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++perf+%3D+%5B%5D%0A%0A++++total_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A+++++++start+%3D+time.time%28%29%0A%0A+++++++inputs+%3D+inputs.to%28%27cuda%3A0%27%29%0A+++++++targets+%3D+targets.to%28%27cuda%3A1%27%29%0A%0A+++++++%23+Models+wrapped+with+Pipe%28%29+return+a+RRef+object.+Since+the+example+is+single+node%2C+all+values+are+local+to+the+node+and+we+can+grab+them%0A+++++++outputs+%3D+net%28inputs%29.local_value%28%29%0A+++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A+++++++optimizer.zero_grad%28%29%0A+++++++loss.backward%28%29%0A+++++++optimizer.step%28%29%0A+++++++print%28f%22Loss%3A+%7Bloss.item%28%29%7D%22%29%0A%0A+++++++batch_time+%3D+time.time%28%29+-+start%0A%0A+++++++images_per_sec+%3D+args.batch_size%2Fbatch_time%0A%0A+++++++perf.append%28images_per_sec%29%0A%0A++++total_time+%3D+time.time%28%29+-+total_start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-modelpar-pipelined-rpc.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributed.pipeline.sync</span> <span class="kn">import</span> <span class="n">Pipe</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, single node model parallelism test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Convolutional + pooling part of the model</span>
    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">rpc</span><span class="o">.</span><span class="n">init_rpc</span><span class="p">(</span><span class="s1">&#39;worker&#39;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># initializing RPC is required by Pipe we use below</span>

    <span class="n">part1</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span> <span class="c1"># Load part1 on the first GPU</span>
    <span class="n">part2</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span> <span class="c1"># Load part2 on the second GPU</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">part1</span><span class="p">,</span><span class="n">part2</span><span class="p">)</span> <span class="c1"># Pipe requires all modules be wrapped with nn.Sequential()</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Pipe</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span> <span class="c1"># Wrap with Pipe to perform Pipeline Parallelism</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span> <span class="c1"># Load the loss function on the last GPU</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">perf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">total_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

       <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

       <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
       <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>

       <span class="c1"># Models wrapped with Pipe() return a RRef object. Since the example is single node, all values are local to the node and we can grab them</span>
       <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">local_value</span><span class="p">()</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

       <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
       <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

       <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

       <span class="n">images_per_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="o">/</span><span class="n">batch_time</span>

       <span class="n">perf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images_per_sec</span><span class="p">)</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">total_start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span id="Parall.C3.A9liser_mod.C3.A8le_et_donn.C3.A9es_avec_plusieurs_GPU"></span><span class="mw-headline" id="Paralléliser_modèle_et_données_avec_plusieurs_GPU">Paralléliser modèle et données avec plusieurs GPU</span></h3>
<p>Quand un modèle est trop grand pour être contenu dans un seul GPU et que son entraînement doit se faire avec un très grand ensemble de données, le fait de combiner le parallélisme du modèle et celui des données permet d’obtenir une bonne performance. Le principe est simple&#160;: le modèle est divisé en portions chacune attribuée à un GPU; le parallélisme des pipelines est fait avec les résultats; puis des copies du processus sont faites et les copies du modèle sont entraînées en parallèle avec des sous-ensembles distincts de l’ensemble de données d’entraînement. 
Comme décrit <a class="mw-selflink-fragment" href="#Paralléliser_les_données_avec_plusieurs_GPU">ci-dessus</a>, les gradients sont calculés indépendamment dans chacune des copies et agrégés pour modifier toutes les copies de façon synchrone ou asynchrone, dépendant de la méthode. La différence principale ici est que chaque copie du modèle se trouve sur plus d’un GPU. 
</p><p><span id="Using_Torch_RPC_and_DDP"></span>
</p>
<h4><span class="mw-headline" id="Utiliser_Torch_RPC_et_DDP">Utiliser Torch RPC et DDP</span></h4>
<p>Toujours avec le même exemple, nous combinons maintenant Torch RPC et DistributedDataParallel pour séparer le modèle en deux portions et entraîner quatre copies du modèle distribuées en parallèle sur deux nœuds. Autrement dit, nous avons deux copies sur deux GPU de chaque nœud. Cependant, un avertissement s’impose&#160;: à ce jour, Torch RPC prend en charge la division d’un modèle dans un seul nœud. Pour entraîner un modèle qui dépasse la quantité de mémoire de tous les GPU dans un nœud de calcul, voyez <a class="mw-selflink-fragment" href="#DeepSpeed">la section DeepSpeed</a>.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-model-data-par.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+2%0A%23SBATCH+--gres%3Dgpu%3A4+%23+Request+4+GPUs+per+node%0A%23SBATCH+--tasks-per-node%3D2+%23+Request+one+task+per+MODEL+per+node%0A%23SBATCH+--cpus-per-task%3D1+%23+change+this+parameter+to+2%2C4%2C6%2C...+and+increase+%22--num_workers%22+accordingly+to+see+the+effect+on+performance%0A%23SBATCH+--mem%3D16G++++++%0A%23SBATCH+--time%3D0%3A10%3A00%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+StdEnv%2F2020+gcc%2F11.3.0%0Amodule+load+python+%23+Using+Default+Python+version+-+Make+sure+to+choose+a+version+that+suits+your+application%2C+python%2F3.10.2+works+with+this+demo%0Amodule+load+cuda%2F11.8.0%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torch+torchvision+--no-index%0A%0Aexport+MAIN_NODE%3D%24%28hostname%29%0A%0Aecho+%22starting+training...%22%0A%0Asrun+python+pytorch-model-data-par.py+--init_method+tcp%3A%2F%2F%24MAIN_NODE%3A3456+--world_size+%24SLURM_NTASKS++--batch_size+512" />
<input type="hidden" name="filename" value="pytorch-model-data-par.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 2</span>
<span class="c1">#SBATCH --gres=gpu:4 # Request 4 GPUs per node</span>
<span class="c1">#SBATCH --tasks-per-node=2 # Request one task per MODEL per node</span>
<span class="c1">#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... and increase &quot;--num_workers&quot; accordingly to see the effect on performance</span>
<span class="c1">#SBATCH --mem=16G      </span>
<span class="c1">#SBATCH --time=0:10:00</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>StdEnv/2020<span class="w"> </span>gcc/11.3.0
module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span><span class="c1"># Using Default Python version - Make sure to choose a version that suits your application, python/3.10.2 works with this demo</span>
module<span class="w"> </span>load<span class="w"> </span>cuda/11.8.0
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">MAIN_NODE</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;starting training...&quot;</span>

srun<span class="w"> </span>python<span class="w"> </span>pytorch-model-data-par.py<span class="w"> </span>--init_method<span class="w"> </span>tcp://<span class="nv">$MAIN_NODE</span>:3456<span class="w"> </span>--world_size<span class="w"> </span><span class="nv">$SLURM_NTASKS</span><span class="w">  </span>--batch_size<span class="w"> </span><span class="m">512</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> pytorch-model-data-par.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+time%0Aimport+os%0A%0Aimport+torch%0Aimport+torch.nn+as+nn%0Aimport+torch.optim+as+optim%0Afrom+torch.distributed.pipeline.sync+import+Pipe%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Aimport+torch.distributed+as+dist%0Aimport+torch.utils.data.distributed%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+distributed+data+%26+model+parallel+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D4%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0Aparser.add_argument%28%27--init_method%27%2C+default%3D%27tcp%3A%2F%2F127.0.0.1%3A3456%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--dist-backend%27%2C+default%3D%27mpi%27%2C+type%3Dstr%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--world_size%27%2C+default%3D1%2C+type%3Dint%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--distributed%27%2C+action%3D%27store_true%27%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++%23+Convolutional+%2B+pooling+part+of+the+model%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++ngpus_per_node+%3D+torch.cuda.device_count%28%29%0A++++local_rank+%3D+int%28os.environ.get%28%22SLURM_LOCALID%22%29%29%0A++++rank+%3D+int%28os.environ.get%28%22SLURM_NODEID%22%29%29%2A%28ngpus_per_node%2F%2F2%29+%2B+local_rank++%23+Divide+ngpus_per_node+by+the+number+of+model+parts%0A%0A++++os.environ%5B%27MASTER_ADDR%27%5D+%3D+%27127.0.0.1%27+%23+Each+model+replica+will+run+its+own+RPC+server+to+run+pipeline+parallelism%0A++++os.environ%5B%27MASTER_PORT%27%5D+%3D+str%2834567+%2B+local_rank%29+%23+Make+sure+each+RPC+server+starts+on+a+different+port%0A++++torch.distributed.rpc.init_rpc%28%27worker%27%2C+rank%3D0%2C+world_size%3D1%29+%23+Different+replicas+won%27t+communicate+through+RPC%2C+but+through+DDP%0A%0A++++dist.init_process_group%28backend%3Dargs.dist_backend%2C+init_method%3Dargs.init_method%2C+world_size%3Dargs.world_size%2C+rank%3Drank%29+%23+Initialize+Data+Parallelism+communications%0A%0A++++part1+%3D+ConvPart%28%29.cuda%28local_rank%29+%23+First+part+of+the+model+goes+on+the+first+GPU+of+each+process%0A++++part2+%3D+MLPPart%28%29.cuda%28local_rank+%2B+1%29+%23+Second+part+goes+on+the+second+GPU+of+each+process%0A%0A++++net+%3D+nn.Sequential%28part1%2Cpart2%29%0A%0A++++net+%3D+Pipe%28net%2C+chunks%3D32%2C+checkpoint%3D%22never%22%29%0A%0A++++net+%3D+torch.nn.parallel.DistributedDataParallel%28net%29%0A%0A++++criterion+%3D+nn.CrossEntropyLoss%28%29.cuda%28local_rank+%2B+1%29+%23+Loss+function+goes+on+the+second+GPU+of+each+process%0A++++optimizer+%3D+optim.SGD%28net.parameters%28%29%2C+lr%3Dargs.lr%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_sampler+%3D+torch.utils.data.distributed.DistributedSampler%28dataset_train%29%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+shuffle%3D%28train_sampler+is+None%29%2C+num_workers%3Dargs.num_workers%2C+sampler%3Dtrain_sampler%29%0A%0A%0A++++for+epoch+in+range%28args.max_epochs%29%3A%0A%0A++++++++train_sampler.set_epoch%28epoch%29%0A%0A++++++++train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+rank%2C+local_rank%29%0A%0Adef+train%28epoch%2C+net%2C+criterion%2C+optimizer%2C+train_loader%2C+train_rank%2C+model_rank%29%3A%0A%0A++++train_loss+%3D+0%0A++++correct+%3D+0%0A++++total+%3D+0%0A%0A++++epoch_start+%3D+time.time%28%29%0A%0A++++for+batch_idx%2C+%28inputs%2C+targets%29+in+enumerate%28train_loader%29%3A%0A%0A++++++++start+%3D+time.time%28%29%0A%0A++++++++inputs+%3D+inputs.cuda%28model_rank%29%0A++++++++targets+%3D+targets.cuda%28model_rank+%2B+1%29%0A%0A++++++++outputs+%3D+net%28inputs%29.local_value%28%29%0A++++++++loss+%3D+criterion%28outputs%2C+targets%29%0A%0A++++++++optimizer.zero_grad%28%29%0A++++++++loss.backward%28%29%0A++++++++optimizer.step%28%29%0A++++++++print%28f%22From+Rank+%7Btrain_rank%7D+-+Loss%3A+%7Bloss.item%28%29%7D%22%29%0A%0A++++++++batch_time+%3D+time.time%28%29+-+start%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="pytorch-model-data-par.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributed.pipeline.sync</span> <span class="kn">import</span> <span class="n">Pipe</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, distributed data &amp; model parallel test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--init_method&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;tcp://127.0.0.1:3456&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--dist-backend&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;mpi&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--world_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--distributed&#39;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Convolutional + pooling part of the model</span>
    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="n">ngpus_per_node</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_LOCALID&quot;</span><span class="p">))</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLURM_NODEID&quot;</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">ngpus_per_node</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">local_rank</span>  <span class="c1"># Divide ngpus_per_node by the number of model parts</span>

    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span> <span class="c1"># Each model replica will run its own RPC server to run pipeline parallelism</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="mi">34567</span> <span class="o">+</span> <span class="n">local_rank</span><span class="p">)</span> <span class="c1"># Make sure each RPC server starts on a different port</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">rpc</span><span class="o">.</span><span class="n">init_rpc</span><span class="p">(</span><span class="s1">&#39;worker&#39;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Different replicas won&#39;t communicate through RPC, but through DDP</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span> <span class="c1"># Initialize Data Parallelism communications</span>

    <span class="n">part1</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span> <span class="c1"># First part of the model goes on the first GPU of each process</span>
    <span class="n">part2</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">local_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Second part goes on the second GPU of each process</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">part1</span><span class="p">,</span><span class="n">part2</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Pipe</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;never&quot;</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">local_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Loss function goes on the second GPU of each process</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>


    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>

        <span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">local_rank</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_rank</span><span class="p">,</span> <span class="n">model_rank</span><span class="p">):</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">epoch_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model_rank</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">local_value</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;From Rank </span><span class="si">{</span><span class="n">train_rank</span><span class="si">}</span><span class="s2"> - Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h3><span class="mw-headline" id="DeepSpeed">DeepSpeed</span></h3>
<p>DeepSpeed est une bibliothèque qui permet d'optimiser l’entraînement de l’apprentissage de modèles ayant des milliards de paramètres à l’échelle. Parfaitement compatible avec PyTorch, DeepSpeed offre des implémentations de nouvelles méthodes d’entraînement distribué qui font un usage efficace de la mémoire en appliquant le concept de <a rel="nofollow" class="external text" href="https://arxiv.org/abs/1910.02054">Zero Redundancy Optimizer (ZeRO)</a>. Avec ZeRO, DeepSpeed peut distribuer le stockage et le traitement  des divers éléments d’une tâche d’entraînement (états de l’optimiseur, poids du modèle, gradients et activations) sur plusieurs dispositifs comme les GPU, les CPU, les disques durs locaux et/ou les combinaisons de ces dispositifs. Cette mise en commun des ressources, surtout les ressources de stockage, permet l’entraînement efficace sur plusieurs nœuds de modèles ayant d’énormes quantités de paramètres sans avoir besoin d’écrire le code pour paralléliser le modèle, les pipelines ou les données. Les exemples qui suivent montrent comment profiter de DeepSpeed et des différentes implémentations de ZeRO en utilisant l’interface simple de  PyTorch Lightning.
</p><p><span id="ZeRO_on_GPU"></span>
</p>
<h4><span class="mw-headline" id="ZeRO_avec_GPU">ZeRO avec GPU</span></h4>
<p>Dans l’exemple ci-dessous, nous utilisons ZeRO stage 3 pour entraîner un modèle qui utilise un groupe de 4 GPU. Le stage 3  répartit sur les 4 GPU les trois caractéristiques, soit les états de l’optimiseur, les paramètres du modèle et les gradients du modèle. Ceci est plus efficace que le parallélisme pur des données où une copie complète du modèle est chargée sur chacun des GPU. Avec l’optimiseur DeepSpeed <code>FusedAdam</code> plutôt qu’un optimiseur natif de PyTorch, la performance se compare au parallélisme pur des données. Les optimiseurs DeepSpeed sont compilés en temps réel à l’exécution et vous devez charger un module <code>cuda/&lt;version&gt;</code> où <i>version</i> correspond à la version utilisée pour construire le paquet PyTorch que vous utilisez.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D32G++++++%0A%23SBATCH+--time%3D0-00%3A20%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+cuda+%23+CUDA+must+be+loaded+if+using+a+DeepSpeed+optimizer%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+deepspeed+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+deepspeed-stage3.py++--batch_size+256" />
<input type="hidden" name="filename" value="deepspeed-stage3.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resources”.</span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=32G      </span>
<span class="c1">#SBATCH --time=0-00:20</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span>cuda<span class="w"> </span><span class="c1"># CUDA must be loaded if using a DeepSpeed optimizer</span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>deepspeed<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>deepspeed-stage3.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Afrom+deepspeed.ops.adam+import+FusedAdam%0Afrom+pytorch_lightning.strategies+import+DeepSpeedStrategy%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models+deep+seed+stage+3+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D2%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv_part+%3D+ConvPart%28%29%0A++++++++++self.mlp_part+%3D+MLPPart%28%29%0A%0A+++++++def+configure_sharded_model%28self%29%3A%0A%0A++++++++++self.block+%3D+nn.Sequential%28self.conv_part%2C+self.mlp_part%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.block%28x%29%0A%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+FusedAdam%28self.parameters%28%29%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPU.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3D%22deepspeed_stage_3%22%2C+max_epochs+%3D+args.max_epochs%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="deepspeed-stage3.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">FusedAdam</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models deep seed stage 3 test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">FusedAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPU.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_3&quot;</span><span class="p">,</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p><p><span id="ZeRO_with_offload_to_CPU"></span>
</p>
<h4><span class="mw-headline" id="ZeRO_avec_CPU">ZeRO avec CPU</span></h4>
<p>Dans cet autre exemple, nous utilisons aussi ZeRO stage 3, mais cette fois-ci nous utilisons le CPU pour les états de l’optimiseur et les paramètres du modèle.  Ceci signifie que la mémoire du nœud de calcul sera disponible pour stocker ces tenseurs quand ils ne sont pas requis par les calculs effectués par le GPU et de plus, les pas de l’optimiseur seront calculés sur le CPU. Pour des raisons pratiques, ce serait comme ajouter 32Go de mémoire additionnelle à la mémoire du GPU. Comme la mémoire du GPU est moins sollicitée, vous pouvez par exemple augmenter la taille de vos lots ou de votre modèle. Avec l’optimiseur DeepSpeed <code>DeepSpeedCPUAdam</code> plutôt qu’un optimiseur natif de PyTorch, la performance se compare au parallélisme pur des données. Les optimiseurs DeepSpeed sont compilés en temps réel à l’exécution et vous devez charger un module <code>cuda/&lt;version&gt;</code> où <i>version</i> correspond à la version utilisée pour construire le paquet PyTorch que vous utilisez.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-cpu.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D32G++++++%0A%23SBATCH+--time%3D0-00%3A20%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+cuda+%23+CUDA+must+be+loaded+if+using+ZeRO+offloading+to+CPU+or+NVMe.+Version+must+be+the+same+used+to+compile+PyTorch.+%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+deepspeed+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+deepspeed-stage3-offload-cpu.py++--batch_size+256" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-cpu.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resources”.</span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=32G      </span>
<span class="c1">#SBATCH --time=0-00:20</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span>cuda<span class="w"> </span><span class="c1"># CUDA must be loaded if using ZeRO offloading to CPU or NVMe. Version must be the same used to compile PyTorch. </span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>deepspeed<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>deepspeed-stage3-offload-cpu.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-cpu.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Afrom+deepspeed.ops.adam+import+DeepSpeedCPUAdam%0Afrom+pytorch_lightning.strategies+import+DeepSpeedStrategy%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+deepspeed+offload+to+cpu+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D2%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv_part+%3D+ConvPart%28%29%0A++++++++++self.mlp_part+%3D+MLPPart%28%29%0A%0A+++++++def+configure_sharded_model%28self%29%3A%0A%0A++++++++++self.block+%3D+nn.Sequential%28self.conv_part%2C+self.mlp_part%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.block%28x%29%0A%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+DeepSpeedCPUAdam%28self.parameters%28%29%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPU.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3DDeepSpeedStrategy%28%0A++++++++stage%3D3%2C%0A++++++++offload_optimizer%3DTrue%2C%0A++++++++offload_parameters%3DTrue%2C%0A++++++++%29%2C+max_epochs+%3D+args.max_epochs%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-cpu.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">DeepSpeedCPUAdam</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, deepspeed offload to cpu test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">DeepSpeedCPUAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPU.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span>
        <span class="n">stage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">offload_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h4><span class="mw-headline" id="ZeRO_avec_utilisation_de_disques_NVMe">ZeRO avec utilisation de disques NVMe</span></h4>
<p>ZeRO stage 3 nous sert encore, cette fois-ci en utilisant le CPU pour les états de l’optimiseur et les paramètres du modèle. Ceci signifie que l’espace de stockage local du nœud de calcul sera disponible pour stocker ces tenseurs quand ils ne sont pas requis par les calculs effectués par le GPU. Ici encore,  les pas de l’optimiseur seront calculés sur le CPU. De même, pour des raisons pratiques, ce serait comme ajouter à la mémoire du GPU autant d’espace de stockage que sur le disque local, mais par contre nous aurons une forte perte de performance. Cette approche peut être utilisée avec tous les types de stockage, mais elle est à privilégier avec les disques NVMe qui sont plus rapides et ont des temps de réponse plus courts, ce qui compense pour la baisse de performance.
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-nvme.sh</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23%21%2Fbin%2Fbash%0A%23SBATCH+--nodes+1+++++++++++++%0A%23SBATCH+--gres%3Dgpu%3A2++++++++++%23+Request+2+GPU+%22generic+resources%E2%80%9D.+%0A%23SBATCH+--tasks-per-node%3D2++++%23+Request+1+process+per+GPU.+You+will+get+1+CPU+per+process+by+default.+Request+more+CPUs+with+the+%22cpus-per-task%22+parameter+to+enable+multiple+data-loader+workers+to+load+data+in+parallel.%0A%23SBATCH+--mem%3D32G++++++%0A%23SBATCH+--time%3D0-00%3A20%0A%23SBATCH+--output%3D%25N-%25j.out%0A%23SBATCH+--account%3D%3Cyour+account%3E%0A%0Amodule+load+python+cuda+%23+CUDA+must+be+loaded+if+using+ZeRO+offloading+to+CPU+or+NVMe.+Version+must+be+the+same+used+to+compile+PyTorch.+%0Avirtualenv+--no-download+%24SLURM_TMPDIR%2Fenv%0Asource+%24SLURM_TMPDIR%2Fenv%2Fbin%2Factivate%0Apip+install+torchvision+pytorch-lightning+deepspeed+--no-index%0A%0Aexport+TORCH_NCCL_ASYNC_HANDLING%3D1%0A%0A%23+PyTorch+Lightning+will+query+the+environment+to+figure+out+if+it+is+running+inside+a+SLURM+batch+job%0A%23+If+it+is%2C+it+expects+the+user+to+have+requested+one+task+per+GPU.%0A%23+If+you+do+not+ask+for+1+task+per+GPU%2C+and+you+do+not+run+your+script+with+%22srun%22%2C+your+job+will+fail%21%0A%0Asrun+python+deepspeed-stage3-offload-nvme.py++--batch_size+256" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-nvme.sh" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes 1             </span>
<span class="c1">#SBATCH --gres=gpu:2          # Request 2 GPU &quot;generic resources”. </span>
<span class="c1">#SBATCH --tasks-per-node=2    # Request 1 process per GPU. You will get 1 CPU per process by default. Request more CPUs with the &quot;cpus-per-task&quot; parameter to enable multiple data-loader workers to load data in parallel.</span>
<span class="c1">#SBATCH --mem=32G      </span>
<span class="c1">#SBATCH --time=0-00:20</span>
<span class="c1">#SBATCH --output=%N-%j.out</span>
<span class="c1">#SBATCH --account=&lt;your account&gt;</span>

module<span class="w"> </span>load<span class="w"> </span>python<span class="w"> </span>cuda<span class="w"> </span><span class="c1"># CUDA must be loaded if using ZeRO offloading to CPU or NVMe. Version must be the same used to compile PyTorch. </span>
virtualenv<span class="w"> </span>--no-download<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env
<span class="nb">source</span><span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/env/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>torchvision<span class="w"> </span>pytorch-lightning<span class="w"> </span>deepspeed<span class="w"> </span>--no-index

<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_NCCL_ASYNC_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># PyTorch Lightning will query the environment to figure out if it is running inside a SLURM batch job</span>
<span class="c1"># If it is, it expects the user to have requested one task per GPU.</span>
<span class="c1"># If you do not ask for 1 task per GPU, and you do not run your script with &quot;srun&quot;, your job will fail!</span>

srun<span class="w"> </span>python<span class="w"> </span>deepspeed-stage3-offload-nvme.py<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> deepspeed-stage3-offload-nvme.py</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="import+os%0A%0Aimport+torch%0Afrom+torch+import+nn%0Aimport+torch.nn.functional+as+F%0A%0Aimport+pytorch_lightning+as+pl%0A%0Aimport+torchvision%0Aimport+torchvision.transforms+as+transforms%0Afrom+torchvision.datasets+import+CIFAR10%0Afrom+torch.utils.data+import+DataLoader%0A%0Afrom+deepspeed.ops.adam+import+DeepSpeedCPUAdam%0Afrom+pytorch_lightning.strategies+import+DeepSpeedStrategy%0A%0Aimport+argparse%0A%0Aparser+%3D+argparse.ArgumentParser%28description%3D%27cifar10+classification+models%2C+deepspeed+offload+to+nvme+test%27%29%0Aparser.add_argument%28%27--lr%27%2C+default%3D0.1%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--max_epochs%27%2C+type%3Dint%2C+default%3D2%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--batch_size%27%2C+type%3Dint%2C+default%3D768%2C+help%3D%27%27%29%0Aparser.add_argument%28%27--num_workers%27%2C+type%3Dint%2C+default%3D0%2C+help%3D%27%27%29%0A%0A%0Adef+main%28%29%3A%0A++++print%28%22Starting...%22%29%0A%0A++++args+%3D+parser.parse_args%28%29%0A%0A++++class+ConvPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28ConvPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv1+%3D+nn.Conv2d%283%2C+6%2C+5%29%0A++++++++++self.pool+%3D+nn.MaxPool2d%282%2C+2%29%0A++++++++++self.conv2+%3D+nn.Conv2d%286%2C+16%2C+5%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv1%28x%29%29%29%0A++++++++++x+%3D+self.pool%28self.relu%28self.conv2%28x%29%29%29%0A++++++++++x+%3D+x.view%28-1%2C+16+%2A+5+%2A+5%29%0A%0A++++++++++return+x%0A%0A++++%23+Dense+feedforward+part+of+the+model%0A++++class+MLPPart%28nn.Module%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28MLPPart%2C+self%29.__init__%28%29%0A%0A++++++++++self.fc1+%3D+nn.Linear%2816+%2A+5+%2A+5%2C+120%29%0A++++++++++self.fc2+%3D+nn.Linear%28120%2C+84%29%0A++++++++++self.fc3+%3D+nn.Linear%2884%2C+10%29%0A++++++++++self.relu+%3D+nn.ReLU%28%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.relu%28self.fc1%28x%29%29%0A++++++++++x+%3D+self.relu%28self.fc2%28x%29%29%0A++++++++++x+%3D+self.fc3%28x%29%0A%0A++++++++++return+x%0A%0A++++class+Net%28pl.LightningModule%29%3A%0A%0A+++++++def+__init__%28self%29%3A%0A++++++++++super%28Net%2C+self%29.__init__%28%29%0A%0A++++++++++self.conv_part+%3D+ConvPart%28%29%0A++++++++++self.mlp_part+%3D+MLPPart%28%29%0A%0A+++++++def+configure_sharded_model%28self%29%3A%0A%0A++++++++++self.block+%3D+nn.Sequential%28self.conv_part%2C+self.mlp_part%29%0A%0A+++++++def+forward%28self%2C+x%29%3A%0A++++++++++x+%3D+self.block%28x%29%0A%0A++++++++++return+x%0A%0A+++++++def+training_step%28self%2C+batch%2C+batch_idx%29%3A%0A++++++++++x%2C+y+%3D+batch%0A++++++++++y_hat+%3D+self%28x%29%0A++++++++++loss+%3D+F.cross_entropy%28y_hat%2C+y%29%0A++++++++++return+loss%0A%0A+++++++def+configure_optimizers%28self%29%3A%0A++++++++++return+DeepSpeedCPUAdam%28self.parameters%28%29%29%0A%0A++++net+%3D+Net%28%29%0A%0A++++%22%22%22+Here+we+initialize+a+Trainer%28%29+explicitly+with+1+node+and+2+GPU.%0A++++++++To+make+this+script+more+generic%2C+you+can+use+torch.cuda.device_count%28%29+to+set+the+number+of+GPUs%0A++++++++and+you+can+use+int%28os.environ.get%28%22SLURM_JOB_NUM_NODES%22%29%29+to+set+the+number+of+nodes.+%0A++++++++We+also+set+progress_bar_refresh_rate%3D0+to+avoid+writing+a+progress+bar+to+the+logs%2C+%0A++++++++which+can+cause+issues+due+to+updating+logs+too+frequently.%22%22%22%0A%0A++++local_scratch+%3D+os.environ%5B%27SLURM_TMPDIR%27%5D+%23+Get+path+where+local+storage+is+mounted%0A%0A++++print%28f%27Offloading+to%3A+%7Blocal_scratch%7D%27%29%0A%0A++++trainer+%3D+pl.Trainer%28accelerator%3D%22gpu%22%2C+devices%3D2%2C+num_nodes%3D1%2C+strategy%3DDeepSpeedStrategy%28%0A++++++++stage%3D3%2C%0A++++++++offload_optimizer%3DTrue%2C%0A++++++++offload_parameters%3DTrue%2C%0A++++++++remote_device%3D%22nvme%22%2C%0A++++++++offload_params_device%3D%22nvme%22%2C%0A++++++++offload_optimizer_device%3D%22nvme%22%2C%0A++++++++nvme_path%3D%22local_scratch%22%2C%0A++++++++%29%2C+max_epochs+%3D+args.max_epochs%29%0A%0A++++transform_train+%3D+transforms.Compose%28%5Btransforms.ToTensor%28%29%2Ctransforms.Normalize%28%280.5%2C+0.5%2C+0.5%29%2C+%280.5%2C+0.5%2C+0.5%29%29%5D%29%0A%0A++++dataset_train+%3D+CIFAR10%28root%3D%27.%2Fdata%27%2C+train%3DTrue%2C+download%3DFalse%2C+transform%3Dtransform_train%29%0A%0A++++train_loader+%3D+DataLoader%28dataset_train%2C+batch_size%3Dargs.batch_size%2C+num_workers%3Dargs.num_workers%29%0A%0A++++trainer.fit%28net%2Ctrain_loader%29%0A%0A%0Aif+__name__%3D%3D%27__main__%27%3A%0A+++main%28%29" />
<input type="hidden" name="filename" value="deepspeed-stage3-offload-nvme.py" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">DeepSpeedCPUAdam</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;cifar10 classification models, deepspeed offload to nvme test&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_workers&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting...&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">class</span> <span class="nc">ConvPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">ConvPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># Dense feedforward part of the model</span>
    <span class="k">class</span> <span class="nc">MLPPart</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">MLPPart</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

    <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span> <span class="o">=</span> <span class="n">ConvPart</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span> <span class="o">=</span> <span class="n">MLPPart</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_part</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_part</span><span class="p">)</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="k">return</span> <span class="n">x</span>

       <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
          <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">loss</span>

       <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">DeepSpeedCPUAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot; Here we initialize a Trainer() explicitly with 1 node and 2 GPU.</span>
<span class="sd">        To make this script more generic, you can use torch.cuda.device_count() to set the number of GPUs</span>
<span class="sd">        and you can use int(os.environ.get(&quot;SLURM_JOB_NUM_NODES&quot;)) to set the number of nodes. </span>
<span class="sd">        We also set progress_bar_refresh_rate=0 to avoid writing a progress bar to the logs, </span>
<span class="sd">        which can cause issues due to updating logs too frequently.&quot;&quot;&quot;</span>

    <span class="n">local_scratch</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_TMPDIR&#39;</span><span class="p">]</span> <span class="c1"># Get path where local storage is mounted</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Offloading to: </span><span class="si">{</span><span class="n">local_scratch</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span>
        <span class="n">stage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">offload_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">remote_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">offload_params_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">offload_optimizer_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">nvme_path</span><span class="o">=</span><span class="s2">&quot;local_scratch&quot;</span><span class="p">,</span>
        <span class="p">),</span> <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">)</span>

    <span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_loader</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p><br />
</p>
<h1><span id="Cr.C3.A9er_des_points_de_contr.C3.B4le"></span><span class="mw-headline" id="Créer_des_points_de_contrôle">Créer des points de contrôle</span></h1>
<p>Peu importe si vous pensez que la durée d'exécution de votre code sera longue ou non, il est bon de prendre l'habitude de créer des points de contrôle pendant l'entraînement. Un point de contrôle est un portrait de votre modèle à un moment précis du processus d'entraînement (après un certain nombre d'itérations ou un certain nombre d'époques) que vous pouvez sauvegarder sur disque et utiliser plus tard. C'est un moyen pratique de diviser les tâches qui devraient être de longue durée en de multiples petites tâches auxquelles l'ordonnanceur peut allouer des ressources plus rapidement. C'est aussi une bonne façon de ne pas perdre le progrès réalisé au cas où des erreurs de code inattendues surviendraient ou que les nœuds ne soient pas disponibles pour quelconque raison.
</p><p><span id="With_PyTorch_Lightning"></span>
</p>
<h2><span class="mw-headline" id="Avec_PyTorch_Lightning">Avec PyTorch Lightning</span></h2>
<p>Nous recommandons d'utiliser le paramètre de rappels (<i>callbacks parameter</i>) de la classe <code>Trainer()</code>. Dans l'exemple suivant, on demande à PyTorch de créer un point de contrôle à la fin de chacune des époques d'entraînement. Vérifiez que le chemin où créer le point de contrôle existe.
</p>
<pre>callbacks = [pl.callbacks.ModelCheckpoint(dirpath="./ckpt",every_n_epochs=1)]
trainer = pl.Trainer(callbacks=callbacks) 
trainer.fit(model)
</pre>
<p>Ce bout de code chargera un point de contrôle de <code>./ckpt</code> (s'il en existe) et poursuivra l'entraînement à partir de ce point. Pour plus d'information, consultez la <a rel="nofollow" class="external text" href="https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.model_checkpoint.html">documentation PyTorch Lightning</a>.
</p><p><span id="With_custom_training_loops"></span>
</p>
<h2><span id="Avec_des_boucles_d.27entra.C3.AEnement_personnalis.C3.A9es"></span><span class="mw-headline" id="Avec_des_boucles_d'entraînement_personnalisées">Avec des boucles d'entraînement personnalisées</span></h2>
<p>Pour des exemples, consultez <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">la documentation PyTorch</a>.
</p><p><span id="During_distributed_training"></span>
</p>
<h2><span id="Pendant_l.E2.80.99entra.C3.AEnement_distribu.C3.A9"></span><span class="mw-headline" id="Pendant_l’entraînement_distribué">Pendant l’entraînement distribué</span></h2>
<p>Les points de contrôle peuvent être utilisés pendant l’exécution d’un programme d’entraînement distribué. Avec PyTorch Lightning, aucun code supplémentaire n’est requis, autre que d’insérer le paramètre de rappels (<i>callbacks parameter</i>) mentionné ci-dessus. Cependant, si vous utilisez  DistributedDataParallel ou Horovod, les points de contrôle devront être créés par un seul processus (<i>rank</i>) de votre programme puisque tous les processus auront le même état après chaque itération. Dans cet exemple, le premier processus (<i>rank 0</i>) crée un point de contrôle.
</p>
<pre>if global_rank == 0:
       torch.save(ddp_model.state_dict(), "./checkpoint_path")
</pre>
<p>Faites attention aux points de contrôle ainsi créés. Si un processus tente de charger un point de contrôle qui n’a pas encore été sauvegardé par un autre, des erreurs peuvent survenir ou de mauvais résultats peuvent être produits. Pour éviter ceci, vous pouvez ajouter une barrière à votre code pour faire en sorte que le processus qui crée le point de contrôle a terminé son écriture sur le disque avant que d’autres processus tentent de le charger. Remarquez aussi que <code>torch.load</code> essaiera par défaut de charger les tenseurs sur le GPU sur lequel ils étaient initialement sauvegardés, dans notre cas <code>cuda:0</code>. Pour éviter les problèmes, passez <code>map_location</code> à <code>torch.load</code> pour charger les tenseurs sur le GPU identifié par chaque processus.
</p>
<pre>torch.distributed.barrier()
map_location = f"cuda:{local_rank}"  
ddp_model.load_state_dict(
torch.load("./checkpoint_path", map_location=map_location))
</pre>
<p><br />
<span id="Troubleshooting"></span>
</p>
<h1><span id="D.C3.A9pannage"></span><span class="mw-headline" id="Dépannage">Dépannage</span></h1>
<h2><span id="Fuites_de_m.C3.A9moire"></span><span class="mw-headline" id="Fuites_de_mémoire">Fuites de mémoire</span></h2>
<p>Sur le matériel AVX512 (nœuds V100, Skylake ou Béluga), les versions PyTorch  antérieures à v1.0.1 qui utilisent des bibliothèques moins récentes (cuDNN &lt; v7.5 ou MAGMA &lt; v2.5) peuvent avoir des fuites de mémoire importantes et créer des exceptions de mémoire insuffisante et terminer vos tâches. Pour contrer ceci, utilisez la plus récente version de <code>torch</code>.
</p>
<h2><span class="mw-headline" id="c10::Error">c10::Error</span></h2>
<p>Dans certains cas, vous pouvez obtenir un erreur comme
</p>
<pre> terminate called after throwing an instance of 'c10::Error'
   what():  Given groups=1, weight of size [256, 1, 3, 3], expected input[16, 10, 16, 16] to have 1 channels, but got 10 channels instead
 Exception raised from check_shape_forward at /tmp/coulombc/pytorch_build_2021-11-09_14-57-01/avx2/python3.8/pytorch/aten/src/ATen/native/Convolution.cpp:496 (most recent call first):
 ...
</pre>
<p>Une exception C++ est émise plutôt qu'une exception Python. Ceci peut se produire quand vous programmez en C++ avec libtorch, mais ne devrait pas se produire quand vous programmez en Python. Il n'est pas possible de suivre la trace des appels (<i>traceback</i>)du programme Python, ce qui ne permet pas d'identifier facilement la cause de l'erreur dans le script Python. Nous avons constaté que le fait d'utiliser PyTorch 1.9.1 plutôt que 1.10.x permet le <i>traceback</i> du programme Python.
</p><p><span id="CUDA_error:_no_kernel_image_is_available_for_execution_on_the_device"></span>
</p>
<h2><span class="mw-headline" id="Erreur_CUDA_:_no_kernel_image_is_available_for_execution_on_the_device">Erreur CUDA&#160;: no kernel image is available for execution on the device</span></h2>
<p>Cette exception signifie que l'installation courante de Torch ne prend pas en charge l'architecture de calcul ou le GPU utilisé.
Vous pouvez installer une version plus récente de <tt>torch</tt> ou demander un GPU compatible avec la version que vous utilisez.
</p>
<h1><span class="mw-headline" id="LibTorch">LibTorch</span></h1>
<p>LibTorch permet d'implémenter à PyTorch des extensions C++ et des <b>applications d'apprentissage machine en C++ pur</b>.  La distribution LibTorch possède les en-têtes, bibliothèques et fichiers de configuration CMake nécessaires pour travailler avec PyTorch, tel que décrit dans la <a rel="nofollow" class="external text" href="https://pytorch.org/cppdocs/installing.html">documentation</a>.
</p><p><span id="How_to_use_LibTorch"></span>
</p>
<h3><span class="mw-headline" id="Utiliser_LibTorch">Utiliser LibTorch</span></h3>
<p><span id="Setting_up_the_environment"></span>
</p>
<h4><span id="Configurer_l.27environnement"></span><span class="mw-headline" id="Configurer_l'environnement">Configurer l'environnement</span></h4>
<p>Chargez les modules requis par LibTorch, puis installez PyTorch dans un environnement virtuel Python.
</p>
<form id="tabs-inputform" class="tabs tabs-inputform" action="#"></form><div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-1-0" name="tabs-1" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-1-1" name="tabs-1" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-1-1" data-tabpos="1">StdEnv/2023</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-1-2" name="tabs-1" class="tabs-input tabs-input-2" /><label class="tabs-label" for="tabs-input-1-2" data-tabpos="2">StdEnv/2020</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<pre>module load StdEnv/2023 gcc cuda/12.2 cmake protobuf cudnn python/3.11 abseil  cusparselt  opencv/4.8.1
virtualenv --no-download --clear ~/ENV &amp;&amp; source ~/ENV/bin/activate 
pip install --no-index torch numpy 
</pre>
<p>Vous devrez peut-être ajuster les versions des modules abseil, cusparselt et opencv, dépendant du paquet torch que vous utilisez. Pour savoir quelle version d'un module a été utilisée pour compiler le wheel Python, lancez la commande
</p>
<div>
<div style="float:right; margin-left:8px">
<p><span typeof="mw:File"><a href="https://explainshell.com/explain?cmd=ldd+%24VIRTUAL_ENV%2Flib%2Fpython3.11%2Fsite-packages%2Ftorch%2Flib%2Flibtorch_cuda.so+%7C+sed+-n+%27s%26%5E.%2A%2F%5C%28%5C%28opencv%5C%7Cabseil%5C%7Ccusparselt%5C%29%2F%5B%5E%2F%5D%2A%5C%29.%2A%26%5C1%26p%27+%7C+sort+-u" rel="nofollow"><img src="/mediawiki/images/thumb/3/30/Question.png/40px-Question.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="/mediawiki/images/thumb/3/30/Question.png/60px-Question.png 1.5x, /mediawiki/images/thumb/3/30/Question.png/80px-Question.png 2x" /></a></span>
</p>
</div>
<div class="command">
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>$<span class="w"> </span>ldd<span class="w"> </span><span class="nv">$VIRTUAL_ENV</span>/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so<span class="w"> </span><span class="p">|</span><span class="w"> </span>sed<span class="w"> </span>-n<span class="w"> </span><span class="s1">&#39;s&amp;^.*/\(\(opencv\|abseil\|cusparselt\)/[^/]*\).*&amp;\1&amp;p&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sort<span class="w"> </span>-u
abseil/20230125.3
cusparselt/0.5.0.1
opencv/4.8.1
</pre></div>
</div>
</div>
</div>
<div class="tabs-content tabs-content-2">
<pre>module load gcc cuda/11.4 cmake protobuf cudnn python/3.10
virtualenv --no-download --clear ~/ENV &amp;&amp; source ~/ENV/bin/activate 
pip install --no-index torch numpy 
</pre>
</div>
</div></div>
<p><span id="Compiling_a_minimal_example"></span>
</p>
<h4><span class="mw-headline" id="Compiler_un_exemple_simple">Compiler un exemple simple</span></h4>
<p>Créez les deux fichiers suivants&#160;:
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> example.cpp</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="%23include+%3Ctorch%2Ftorch.h%3E%0A%23include+%3Ciostream%3E%0A%0Aint+main%28%29+%0A%7B%0A++++torch%3A%3ADevice+device%28torch%3A%3AkCPU%29%3B%0A++++if+%28torch%3A%3Acuda%3A%3Ais_available%28%29%29+%0A++++%7B%0A++++++++std%3A%3Acout+%3C%3C+%22CUDA+is+available%21+Using+GPU.%22+%3C%3C+std%3A%3Aendl%3B%0A++++++++device+%3D+torch%3A%3ADevice%28torch%3A%3AkCUDA%29%3B%0A++++%7D%0A%0A++++torch%3A%3ATensor+tensor+%3D+torch%3A%3Arand%28%7B2%2C+3%7D%29.to%28device%29%3B%0A++++std%3A%3Acout+%3C%3C+tensor+%3C%3C+std%3A%3Aendl%3B%0A%7D" />
<input type="hidden" name="filename" value="example.cpp" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-highlight-lang-cpp mw-content-ltr" dir="ltr"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/torch.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span>
<span class="p">{</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCPU</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">is_available</span><span class="p">())</span><span class="w"> </span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;CUDA is available! Using GPU.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">        </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">}).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><br />
</p><p><br />
</p>
<div class="code-file">
<div class="filename"><b>File&#160;:</b> CMakeLists.txt</div>
<div class="download_form">
<p class="mw-empty-elt"></p><form action="/mediawiki/resources/assets/download.php" method="post">
<input type="hidden" name="text" value="cmake_minimum_required%28VERSION+3.0+FATAL_ERROR%29%0Aproject%28example%29%0A%0Afind_package%28Torch+REQUIRED%29%0A%0Aadd_executable%28example+example.cpp%29%0Atarget_link_libraries%28example+%22%24%7BTORCH_LIBRARIES%7D%22%29%0Aset_property%28TARGET+example+PROPERTY+CXX_STANDARD+14%29" />
<input type="hidden" name="filename" value="CMakeLists.txt" />
<input type="submit" value="" name="submit" class="download_submit" />
</form>
<p class="mw-empty-elt"></p>
</div>
<div class="mw-highlight mw-content-ltr" dir="ltr"><pre>cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
project(example)

find_package(Torch REQUIRED)

add_executable(example example.cpp)
target_link_libraries(example "${TORCH_LIBRARIES}")
set_property(TARGET example PROPERTY CXX_STANDARD 14)</pre></div>
</div>
<p><br />
</p><p>Activez l'environnement virtuel Python, configurez le projet et compilez le programme.
</p>
<div class="tabs tabs-tabbox"><input type="radio" form="tabs-inputform" id="tabs-input-2-0" name="tabs-2" class="tabs-input tabs-input-0" checked="" /><input type="radio" form="tabs-inputform" id="tabs-input-2-1" name="tabs-2" class="tabs-input tabs-input-1" /><label class="tabs-label" for="tabs-input-2-1" data-tabpos="1">StdEnv/2023</label><wbr /><input type="radio" form="tabs-inputform" id="tabs-input-2-2" name="tabs-2" class="tabs-input tabs-input-2" /><label class="tabs-label" for="tabs-input-2-2" data-tabpos="2">StdEnv/2020</label><wbr /><div class="tabs-container" style="">
<div class="tabs-content tabs-content-1">
<pre>cmake -B build -S . -DCMAKE_PREFIX_PATH=$VIRTUAL_ENV/lib/python3.11/site-packages \
                    -DCMAKE_EXE_LINKER_FLAGS=-Wl,-rpath=$VIRTUAL_ENV/lib/python3.11/site-packages/torch/lib,-L$EBROOTCUDA/extras/CUPTI/lib64 \
                    -DCMAKE_SKIP_RPATH=ON -DTORCH_CUDA_ARCH_LIST="6.0;7.0;7.5;8.0;9.0"
cmake --build build
</pre>
</div>
<div class="tabs-content tabs-content-2">
<pre>cmake -B build -S . -DCMAKE_PREFIX_PATH=$VIRTUAL_ENV/lib/python3.10/site-packages \
                    -DCMAKE_EXE_LINKER_FLAGS=-Wl,-rpath=$VIRTUAL_ENV/lib/python3.10/site-packages/torch/lib \
                    -DCMAKE_SKIP_RPATH=ON
cmake --build build 
</pre>
</div>
</div></div>
<p>Lancez le programme avec
</p>
<pre>build/example
</pre>
<p>Pour tester une application avec CUDA, demandez une <a href="/wiki/Running_jobs/fr#Tâches_interactives" title="Running jobs/fr">tâche interactive</a> avec <a href="/wiki/Using_GPUs_with_Slurm/fr" title="Using GPUs with Slurm/fr">GPU</a>.
</p><p><span id="Resources"></span>
</p>
<h1><span class="mw-headline" id="Ressources">Ressources</span></h1>
<p><a rel="nofollow" class="external free" href="https://pytorch.org/cppdocs/">https://pytorch.org/cppdocs/</a>
</p>
<!-- 
NewPP limit report
Cached time: 20250530235248
Cache expiry: 86400
Reduced expiry: false
Complications: [show‐toc]
CPU time usage: 0.319 seconds
Real time usage: 3.841 seconds
Preprocessor visited node count: 1652/1000000
Post‐expand include size: 70248/2097152 bytes
Template argument size: 100251/2097152 bytes
Highest expansion depth: 7/100
Expensive parser function count: 30/100
Unstrip recursion depth: 2/20
Unstrip post‐expand size: 310342/5000000 bytes
ExtLoops count: 0/100
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 3635.117      1 -total
 85.31% 3100.989     26 Template:File
 14.32%  520.401      5 Template:Command
  0.02%    0.565      1 Template:Note
-->

<!-- Saved in parser cache with key ccwiki:pcache:idhash:4699-0!canonical and timestamp 20250530235248 and revision id 175858. Rendering was triggered because: page-view
 -->
</div>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://docs.alliancecan.ca/mediawiki/index.php?title=PyTorch/fr&amp;oldid=175858">https://docs.alliancecan.ca/mediawiki/index.php?title=PyTorch/fr&amp;oldid=175858</a>"</div></div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>: <ul><li><a href="/mediawiki/index.php?title=Category:Pages_with_syntax_highlighting_errors&amp;action=edit&amp;redlink=1" class="new" title="Category:Pages with syntax highlighting errors (page does not exist)">Pages with syntax highlighting errors</a></li><li><a href="/wiki/Category:Software" title="Category:Software">Software</a></li><li><a href="/wiki/Category:AI_and_Machine_Learning" title="Category:AI and Machine Learning">AI and Machine Learning</a></li></ul></div></div>
	</div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		
<nav id="p-personal" class="mw-portlet mw-portlet-personal vector-user-menu-legacy vector-menu" aria-labelledby="p-personal-label"  >
	<h3
		id="p-personal-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Personal tools</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-uls" class="mw-list-item active"><a class="uls-trigger" href="#"><span>English</span></a></li><li id="pt-login" class="mw-list-item"><a href="/mediawiki/index.php?title=Special:UserLogin&amp;returnto=PyTorch%2Ffr" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o"><span>Log in</span></a></li>
		</ul>
		
	</div>
</nav>

		<div id="left-navigation">
			
<nav id="p-namespaces" class="mw-portlet mw-portlet-namespaces vector-menu-tabs vector-menu-tabs-legacy vector-menu" aria-labelledby="p-namespaces-label"  >
	<h3
		id="p-namespaces-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Namespaces</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected mw-list-item"><a href="/wiki/PyTorch/fr" title="View the content page [c]" accesskey="c"><span>Page</span></a></li><li id="ca-talk" class="new mw-list-item"><a href="/mediawiki/index.php?title=Talk:PyTorch/fr&amp;action=edit&amp;redlink=1" rel="discussion" class="new" title="Discussion about the content page (page does not exist) [t]" accesskey="t"><span>Discussion</span></a></li>
		</ul>
		
	</div>
</nav>

			
<nav id="p-variants" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown vector-menu" aria-labelledby="p-variants-label"  >
	<input type="checkbox"
		id="p-variants-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-variants"
		class="vector-menu-checkbox"
		aria-labelledby="p-variants-label"
	>
	<label
		id="p-variants-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">français</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</nav>

		</div>
		<div id="right-navigation">
			
<nav id="p-views" class="mw-portlet mw-portlet-views vector-menu-tabs vector-menu-tabs-legacy vector-menu" aria-labelledby="p-views-label"  >
	<h3
		id="p-views-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Views</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected mw-list-item"><a href="/wiki/PyTorch/fr"><span>Read</span></a></li><li id="ca-viewsource" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch/fr&amp;action=edit" title="This page is protected.&#10;You can view its source [e]" accesskey="e"><span>View source</span></a></li><li id="ca-history" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch/fr&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</nav>

			
<nav id="p-cactions" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown vector-menu" aria-labelledby="p-cactions-label"  title="More options" >
	<input type="checkbox"
		id="p-cactions-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-cactions"
		class="vector-menu-checkbox"
		aria-labelledby="p-cactions-label"
	>
	<label
		id="p-cactions-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">More</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</nav>

			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<h3 >Search</h3>
	<form action="/mediawiki/index.php" id="searchform" class="vector-search-box-form">
		<div id="simpleSearch"
			class="vector-search-box-inner"
			 data-search-loc="header-navigation">
			<input class="vector-search-box-input"
				 type="search" name="search" placeholder="Search Alliance Doc" aria-label="Search Alliance Doc" autocapitalize="sentences" title="Search Alliance Doc [f]" accesskey="f" id="searchInput"
			>
			<input type="hidden" name="title" value="Special:Search">
			<input id="mw-searchButton"
				 class="searchButton mw-fallbackSearchButton" type="submit" name="fulltext" title="Search the pages for this text" value="Search">
			<input id="searchButton"
				 class="searchButton" type="submit" name="go" title="Go to a page with this exact name if it exists" value="Go">
		</div>
	</form>
</div>

		</div>
	</div>
	
<div id="mw-panel" class="vector-legacy-sidebar">
	<div id="p-logo" role="banner">
		<a class="mw-wiki-logo" href="/wiki/Technical_documentation"
			title="Visit the main page"></a>
	</div>
	
<nav id="p-navigation" class="mw-portlet mw-portlet-navigation vector-menu-portal portal vector-menu" aria-labelledby="p-navigation-label"  >
	<h3
		id="p-navigation-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Navigation</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-wiki-main-page" class="mw-list-item"><a href="/wiki/Technical_documentation"><span>Wiki Main Page</span></a></li>
		</ul>
		
	</div>
</nav>

	
<nav id="p-sidebar-support" class="mw-portlet mw-portlet-sidebar-support vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-support-label"  >
	<h3
		id="p-sidebar-support-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Support</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-getting-started" class="mw-list-item"><a href="/wiki/Getting_started"><span>Getting started</span></a></li><li id="n-sidebar-technical-support" class="mw-list-item"><a href="/wiki/Technical_support"><span>Getting help</span></a></li><li id="n-sidebar-running-jobs" class="mw-list-item"><a href="/wiki/Running_jobs"><span>Running jobs</span></a></li><li id="n-sidebar-known-issues" class="mw-list-item"><a href="/wiki/Known_issues"><span>Known issues</span></a></li><li id="n-sidebar-system-status" class="mw-list-item"><a href="http://status.computecanada.ca" rel="nofollow"><span>System status</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-resources" class="mw-portlet mw-portlet-sidebar-resources vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-resources-label"  >
	<h3
		id="p-sidebar-resources-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Resources</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-Béluga" class="mw-list-item"><a href="/wiki/B%C3%A9luga/en"><span>Béluga</span></a></li><li id="n-Cedar" class="mw-list-item"><a href="/wiki/Cedar"><span>Cedar</span></a></li><li id="n-Graham" class="mw-list-item"><a href="/wiki/Graham"><span>Graham</span></a></li><li id="n-Narval" class="mw-list-item"><a href="/wiki/Narval/en"><span>Narval</span></a></li><li id="n-Niagara" class="mw-list-item"><a href="/wiki/Niagara"><span>Niagara</span></a></li><li id="n-sidebar-cloud" class="mw-list-item"><a href="/wiki/CC-Cloud"><span>Cloud</span></a></li><li id="n-tamIA" class="mw-list-item"><a href="/wiki/TamIA/en"><span>tamIA</span></a></li><li id="n-Killarney" class="mw-list-item"><a href="/wiki/Killarney"><span>Killarney</span></a></li><li id="n-sidebar-quantum-computing" class="mw-list-item"><a href="/wiki/Services_d%27informatique_quantique/en"><span>Quantum computing</span></a></li><li id="n-sidebar-available-software" class="mw-list-item"><a href="/wiki/Available_software"><span>Available software</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-alliance" class="mw-portlet mw-portlet-sidebar-alliance vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-alliance-label"  >
	<h3
		id="p-sidebar-alliance-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">The Alliance</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-alliance-main-page" class="mw-list-item"><a href="https://alliancecan.ca/en" rel="nofollow"><span>Alliance main page</span></a></li><li id="n-sidebar-ccdb" class="mw-list-item"><a href="https://ccdb.computecanada.ca/security/login" rel="nofollow"><span>CCDB</span></a></li><li id="n-sidebar-getting-an-account" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/account-management/apply-account" rel="nofollow"><span>Getting An Account</span></a></li><li id="n-sidebar-acknowledging-alliance" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/research-portal/acknowledging-alliance" rel="nofollow"><span>Acknowledging the Alliance</span></a></li><li id="n-sidebar-aup" class="mw-list-item"><a href="https://alliancecan.ca/en/services/advanced-research-computing/account-management/policies" rel="nofollow"><span>Acceptable Use Policy</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-sidebar-authoring" class="mw-portlet mw-portlet-sidebar-authoring vector-menu-portal portal vector-menu" aria-labelledby="p-sidebar-authoring-label"  >
	<h3
		id="p-sidebar-authoring-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Authoring</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-sidebar-guidelines" class="mw-list-item"><a href="/wiki/Authoring_guidelines"><span>Guidelines</span></a></li><li id="n-sidebar-mediawiki-help" class="mw-list-item"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/Help:Contents"><span>MediaWiki Help</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r"><span>Recent changes</span></a></li>
		</ul>
		
	</div>
</nav>

<nav id="p-tb" class="mw-portlet mw-portlet-tb vector-menu-portal portal vector-menu" aria-labelledby="p-tb-label"  >
	<h3
		id="p-tb-label"
		
		class="vector-menu-heading "
	>
		<span class="vector-menu-heading-label">Tools</span>
	</h3>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/PyTorch/fr" title="A list of all wiki pages that link here [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/PyTorch/fr" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-print" class="mw-list-item"><a href="javascript:print();" rel="alternate" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch/fr&amp;oldid=175858" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/mediawiki/index.php?title=PyTorch/fr&amp;action=info" title="More information about this page"><span>Page information</span></a></li>
		</ul>
		
	</div>
</nav>

	
</div>

</div>

<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 11 April 2025, at 09:27.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="/wiki/CCWiki:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/CCWiki:About">About Alliance Doc</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/CCWiki:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-mobileview"><a href="https://docs.alliancecan.ca/mediawiki/index.php?title=PyTorch/fr&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><img src="/mediawiki/resources/assets/poweredby_mediawiki.svg" alt="Powered by MediaWiki" width="88" height="31" loading="lazy"></a></li>
</ul>

</footer>

<script>(RLQ=window.RLQ||[]).push(function(){mw.log.warn("This page is using the deprecated ResourceLoader module \"codex-search-styles\".\n[1.43] Use a CodexModule with codexComponents to set your specific components used: https://www.mediawiki.org/wiki/Codex#Using_a_limited_subset_of_components");});</script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-TVBPRD78K4" async=""></script><script>
window.dataLayer = window.dataLayer || [];

function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-TVBPRD78K4', {});
</script>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":3969,"wgPageParseReport":{"limitreport":{"cputime":"0.319","walltime":"3.841","ppvisitednodes":{"value":1652,"limit":1000000},"postexpandincludesize":{"value":70248,"limit":2097152},"templateargumentsize":{"value":100251,"limit":2097152},"expansiondepth":{"value":7,"limit":100},"expensivefunctioncount":{"value":30,"limit":100},"unstrip-depth":{"value":2,"limit":20},"unstrip-size":{"value":310342,"limit":5000000},"timingprofile":["100.00% 3635.117      1 -total"," 85.31% 3100.989     26 Template:File"," 14.32%  520.401      5 Template:Command","  0.02%    0.565      1 Template:Note"]},"loops":{"limitreport-count-limited":{"value":0,"limit":100}},"cachereport":{"timestamp":"20250530235248","ttl":86400,"transientcontent":false}}});});</script>
</body>
</html>